<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>wgsl-analyzer</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->

        <!-- MathJax -->
        <script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">wgsl-analyzer</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>
                        <a href="https://github.com/wgsl-analyzer/wgsl-analyzer/tree/master/docs/book" title="Git repository" aria-label="Git repository">
                            <i id="git-repository-button" class="fa fa-github"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="wgsl-analyzer"><a class="header" href="#wgsl-analyzer">wgsl-analyzer</a></h1>
<p>At its core, <code>wgsl-analyzer</code> is a <strong>library</strong> for semantic analysis of WGSL and WESL code as it changes over time.
This manual focuses on a specific usage of the library - running it as part of a server that implements the <a href="https://microsoft.github.io/language-server-protocol">Language Server Protocol</a> (LSP).
The LSP allows various code editors, such as VS Code, Emacs, or Vim to implement semantic features such as completion or goto definition by talking to an external language server process.</p>
<p>To improve this document, send a pull request: <a href="https://github.com/wgsl-analyzer/wgsl-analyzer/blob/master/docs/book/README.md">https://github.com/wgsl-analyzer/wgsl-analyzer</a>.</p>
<p>The manual is written in markdown and includes some extra files which are generated from the source code.
Run <code>cargo test</code> and <code>cargo xtask codegen</code> to create these.</p>
<p>If you have a question about using <code>wgsl-analyzer</code>, please read the documentation.
If your question is not addressed, then ask it in the <a href="https://discord.gg/3QUGyyz984">"discord"</a>.
Ideally, the documentation should address all usage questions.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="installation"><a class="header" href="#installation">Installation</a></h1>
<p>To use <code>wgsl-analyzer</code>, you need a <code>wgsl-analyzer</code> binary and a text editor that supports <a href="https://microsoft.github.io/language-server-protocol">LSP</a>.</p>
<p>If you are <a href="./vs_code.html">using VS Code</a>, the extension bundles a copy of the <code>wgsl-analyzer</code> binary.
For other editors, you will need to <a href="./wgsl-analyzer_binary.html">install the binary</a> and <a href="./other_editors.html">configure your editor</a>.</p>
<h2 id="crates"><a class="header" href="#crates">Crates</a></h2>
<p>There is a package named <code>wa_ap_wgsl-analyzer</code> available on <a href="https://crates.io/crates/wa_ap_wgsl-analyzer">crates.io</a> for people who want to use <code>wgsl-analyzer</code> programmatically.</p>
<p>For more details, see <a href="https://github.com/wgsl-analyzer/wgsl-analyzer/blob/main/.github/workflows/autopublish.yaml">the publish workflow</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="vs-code"><a class="header" href="#vs-code">VS Code</a></h1>
<p>This is the best supported editor at the moment.
The <code>wgsl-analyzer</code> plugin for VS Code is maintained <a href="https://github.com/wgsl-analyzer/wgsl-analyzer/tree/master/editors/code">in-tree</a>.</p>
<p>You can install the latest release of the plugin from <a href="https://marketplace.visualstudio.com/items?itemName=wgsl-analyzer.wgsl-analyzer">the marketplace</a>.</p>
<p>The server binary is stored in the extension install directory, which starts with <code>wgsl-analyzer.wgsl-analyzer-</code> and is located in:</p>
<ul>
<li>Linux: <code>~/.vscode/extensions</code></li>
<li>Linux (Remote, such as WSL): <code>~/.vscode-server/extensions</code></li>
<li>macOS: <code>~/.vscode/extensions</code></li>
<li>Windows: <code>%USERPROFILE%\.vscode\extensions</code></li>
</ul>
<p>As an exception, on NixOS, the extension makes a copy of the server and stores it in <code>~/.config/Code/User/globalStorage/wgsl-analyzer.wgsl-analyzer</code>.</p>
<p>Note that we only support the two most recent versions of VS Code.</p>
<h2 id="updates"><a class="header" href="#updates">Updates</a></h2>
<p>The extension will be updated automatically as new versions become available.
It will ask your permission to download the matching language server version binary if needed.</p>
<h3 id="nightly"><a class="header" href="#nightly">Nightly</a></h3>
<p>We ship nightly releases for VS Code. To help us out by testing the newest code, you can enable pre-release versions in the Code extension page.</p>
<h2 id="manual-installation"><a class="header" href="#manual-installation">Manual installation</a></h2>
<p>Alternatively, download a VSIX corresponding to your platform from the <a href="https://github.com/wgsl-analyzer/wgsl-analyzer/releases">releases</a> page.</p>
<p>Install the extension with the <code>Extensions: Install from VSIX</code> command within VS Code, or from the command line via:</p>
<pre><code class="language-bash">code --install-extension /path/to/wgsl-analyzer.vsix
</code></pre>
<p>If you are running an unsupported platform, you can install <code>wgsl-analyzer-no-server.vsix</code> and compile or obtain a server binary.
Copy the server anywhere, then add the path to your <code>settings.json</code>.</p>
<p>For example:</p>
<pre><code class="language-json">{ "wgsl-analyzer.server.path": "~/.local/bin/wgsl-analyzer-linux" }
</code></pre>
<h2 id="building-from-source"><a class="header" href="#building-from-source">Building From Source</a></h2>
<p>Both the server and the Code plugin can be installed from source:</p>
<pre><code class="language-bash">git clone https://github.com/wgsl-analyzer/wgsl-analyzer.git &amp;&amp; cd wgsl-analyzer
cargo xtask install
</code></pre>
<p>You will need <a href="https://doc.rust-lang.org/cargo/getting-started/installation.html">Cargo</a>, <a href="https://nodejs.org/">Node.js</a> (matching a supported version of VS Code) and <a href="https://www.npmjs.com/get-npm">npm</a> for this.</p>
<p>Note that installing via <code>xtask install</code> does not work for VS Code Remote.
Instead, you will need to install the <code>.vsix</code> manually.</p>
<p>If you are not using Code, you can compile and install only the LSP
server:</p>
<pre><code class="language-bash">cargo xtask install --server
</code></pre>
<p>Make sure that <code>.cargo/bin</code> is in <code>$PATH</code> and precedes paths where <code>wgsl-analyzer</code> may also be installed.</p>
<h2 id="vs-code-or-vscodium-in-flatpak"><a class="header" href="#vs-code-or-vscodium-in-flatpak">VS Code or VSCodium in Flatpak</a></h2>
<p>Setting up <code>wgsl-analyzer</code> with a Flatpak version of Code is not trivial because of the Flatpak sandbox. This prevents access to files you might want to import.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="wgsl-analyzer-binary"><a class="header" href="#wgsl-analyzer-binary">wgsl-analyzer Binary</a></h1>
<p>Text editors require the <code>wgsl-analyzer</code> binary to be in <code>$PATH</code>.
You can download pre-built binaries from the <a href="https://github.com/wgsl-analyzer/wgsl-analyzer/releases">releases</a> page.
You will need to uncompress and rename the binary for your platform.</p>
<p>For example,  on Mac OS:</p>
<ol>
<li>extract <code>wgsl-analyzer-aarch64-apple-darwin.gz</code> to <code>wgsl-analyzer</code></li>
<li>make it executable</li>
<li>move it into a directory in your <code>$PATH</code></li>
</ol>
<p>On Linux, to install the <code>wgsl-analyzer</code> binary into <code>~/.local/bin</code>, these commands should work:</p>
<pre><code class="language-bash">mkdir -p ~/.local/bin
curl -L https://github.com/wgsl-analyzer/wgsl-analyzer/releases/latest/download/wgsl-analyzer-x86_64-unknown-linux-gnu.gz | gunzip -c - &gt; ~/.local/bin/wgsl-analyzer
chmod +x ~/.local/bin/wgsl-analyzer
</code></pre>
<p>Make sure that <code>~/.local/bin</code> is listed in the <code>$PATH</code> variable and use the appropriate URL if you are not on a <code>x86-64</code> system.</p>
<p>You do not have to use <code>~/.local/bin</code>, any other path like <code>~/.cargo/bin</code> or <code>/usr/local/bin</code> will work just as well.</p>
<p>Alternatively, you can install it from source using the command below.
You will need the latest stable version of the Rust toolchain.</p>
<pre><code class="language-bash">git clone https://github.com/wgsl-analyzer/wgsl-analyzer.git &amp;&amp; cd wgsl-analyzer
cargo xtask install --server
</code></pre>
<p>If your editor cannot find the binary even though the binary is on your <code>$PATH</code>, the likely explanation is that it does not see the same <code>$PATH</code> as the shell.
On Unix, running the editor from a shell or changing the <code>.desktop</code> file to set the environment should help.</p>
<h2 id="arch-linux"><a class="header" href="#arch-linux">Arch Linux</a></h2>
<p>The <code>wgsl-analyzer</code> binary can be installed from the repos or AUR (Arch User Repository):</p>
<ul>
<li>
<p><a href="https://www.archlinux.org/packages/extra/x86_64/wgsl-analyzer"><code>wgsl-analyzer</code></a> (built from latest tagged source)</p>
</li>
<li>
<p><a href="https://aur.archlinux.org/packages/wgsl-analyzer-git"><code>wgsl-analyzer-git</code></a> (latest Git version)</p>
</li>
</ul>
<p>Install it with <code>pacman</code>, for example:</p>
<pre><code class="language-bash">pacman -S wgsl-analyzer
</code></pre>
<h2 id="gentoo-linux"><a class="header" href="#gentoo-linux">Gentoo Linux</a></h2>
<!-- TODO make this real -->
<h2 id="macos"><a class="header" href="#macos">macOS</a></h2>
<!-- TODO publish to brew -->
<p>The <code>wgsl-analyzer</code> binary can be installed via <a href="https://brew.sh">Homebrew</a>.</p>
<pre><code class="language-zsh">brew install wgsl-analyzer
</code></pre>
<h2 id="windows"><a class="header" href="#windows">Windows</a></h2>
<!-- TODO publish to winget -->
<!-- TODO publish to choco -->
<p>The <code>wgsl-analyzer</code> binary can be installed via <a href="https://github.com/microsoft/winget-cli">WinGet</a> or <a href="https://github.com/chocolatey/choco">Chocolatey</a>.</p>
<pre><code class="language-powershell">winget install wgsl-analyzer
</code></pre>
<pre><code class="language-powershell">choco install wgsl-analyzer
</code></pre>
<div style="break-before: page; page-break-before: always;"></div><h1 id="other-editors"><a class="header" href="#other-editors">Other Editors</a></h1>
<p><code>wgsl-analyzer</code> works with any editor that supports the <a href="https://microsoft.github.io/language-server-protocol">Language Server Protocol</a>.</p>
<p>This page assumes that you have already <a href="./wgsl-analyzer_binary.html">installed the <code>wgsl-analyzer</code> binary</a>.</p>
<h2 id="emacs-using-lsp-mode"><a class="header" href="#emacs-using-lsp-mode">Emacs (using lsp-mode)</a></h2>
<ul>
<li>Assumes you are using <code>wgsl-mode</code>: <a href="https://github.com/acowley/wgsl-mode">https://github.com/acowley/wgsl-mode</a></li>
</ul>
<ol>
<li>
<p>Install the language server</p>
<pre><code class="language-bash">cargo install --git https://github.com/wgsl-analyzer/wgsl-analyzer wgsl-analyzer
</code></pre>
</li>
<li>
<p>Add the following to your init.el</p>
<pre><code class="language-emacs-lisp">(with-eval-after-load 'lsp-mode
(add-to-list 'lsp-language-id-configuration '(wgsl-mode . "wgsl"))
(lsp-register-client (make-lsp-client :new-connection (lsp-stdio-connection "wgsl-analyzer")
                                      :activation-fn (lsp-activate-on "wgsl")
                                      :server-id 'wgsl-analyzer)))
</code></pre>
</li>
</ol>
<h3 id="eglot"><a class="header" href="#eglot"><a href="https://github.com/joaotavora/eglot">Eglot</a></a></h3>
<p><a href="https://github.com/joaotavora/eglot">Eglot</a> is the more minimalistic and lightweight LSP client for Emacs, integrates well with existing Emacs functionality and is built into Emacs starting from release 29.</p>
<p>After installing <a href="https://github.com/joaotavora/eglot">Eglot</a>, e.g. via <code>M-x package-install</code> (not needed from Emacs 29), you can enable it via the <code>M-x eglot</code> command or load it automatically in <code>wgsl-mode</code> via</p>
<pre><code class="language-emacs-lisp">(add-hook 'wgsl-mode-hook 'eglot-ensure)
</code></pre>
<p>For more detailed instructions and options see the <a href="https://joaotavora.github.io/eglot">Eglot manual</a> (also available from Emacs via <code>M-x info</code>) and the <a href="https://github.com/joaotavora/eglot/blob/master/README.md">Eglot readme</a>.</p>
<p>Eglot does not support the <code>wgsl-analyzer</code> extensions to the language-server protocol and does not aim to do so in the future.
The <a href="https://github.com/nemethf/eglot-x#wgsl-analyzer-extensions">eglot-x</a> package adds experimental support for those LSP extensions.</p>
<h3 id="lsp-mode"><a class="header" href="#lsp-mode">LSP Mode</a></h3>
<p>LSP-mode is the original LSP-client for emacs.
Compared to Eglot it has a larger codebase and supports more features, like LSP protocol extensions.
With extension packages like <a href="https://github.com/emacs-lsp/lsp-mode">LSP UI</a> it offers a lot of visual eyecandy.
Further it integrates well with <a href="https://github.com/emacs-lsp/dap-mode">DAP mode</a> for support of the Debug Adapter Protocol.</p>
<p>You can install LSP-mode via <code>M-x package-install</code> and then run it via the <code>M-x lsp</code> command or load it automatically in WGSL/WESL buffers with</p>
<pre><code class="language-emacs-lisp">(add-hook 'wgsl-mode-hook 'lsp-deferred)
</code></pre>
<p>For more information on how to set up LSP mode and its extension package see the instructions in the <a href="https://emacs-lsp.github.io/lsp-mode/page/installation">LSP mode manual</a>.
Also see the <a href="https://emacs-lsp.github.io/lsp-mode/page/lsp-wgsl-analyzer"><code>wgsl-analyzer</code> section</a> for <code>wgsl-analyzer</code> specific options and commands, which you can optionally bind to keys.</p>
<h2 id="vimneovim"><a class="header" href="#vimneovim">Vim/Neovim</a></h2>
<p>There are several LSP client implementations for Vim or Neovim:</p>
<h3 id="using-coc-wgsl-analyzer"><a class="header" href="#using-coc-wgsl-analyzer">Using <code>coc-wgsl-analyzer</code></a></h3>
<ol>
<li>
<p>Install coc.nvim by following the instructions at <a href="https://github.com/neoclide/coc.nvim">coc.nvim</a> (Node.js required)</p>
</li>
<li>
<p>Run <code>:CocInstall coc-wgsl-analyzer</code> to install <a href="https://github.com/wgsl-analyzer/coc-wgsl-analyzer"><code>coc-wgsl-analyzer</code></a>, this extension implements <em>most</em> of the features supported in the VS Code extension:</p>
<ul>
<li>automatically install and upgrade stable/nightly releases</li>
<li>same configurations as VS Code extension, <code>wgsl-analyzer.server.path</code>, <code>wgsl-analyzer.cargo.features</code> etc.</li>
<li>same commands too, <code>wgsl-analyzer.analyzerStatus</code>, <code>wgsl-analyzer.ssr</code> etc.</li>
<li>inlay hints for variables and method chaining, <em>Neovim Only</em></li>
</ul>
</li>
</ol>
<blockquote>
<p>[!NOTE]
<code>coc-wgsl-analyzer</code> is capable of installing or updating the <code>wgsl-analyzer</code> binary on its own.</p>
</blockquote>
<!--  -->
<blockquote>
<p>[!NOTE]
for code actions, use <code>coc-codeaction-cursor</code> and <code>coc-codeaction-selected</code>; <code>coc-codeaction</code> and <code>coc-codeaction-line</code> are unlikely to be useful.</p>
</blockquote>
<h3 id="using-languageclient-neovim"><a class="header" href="#using-languageclient-neovim">Using LanguageClient-neovim</a></h3>
<ol>
<li>
<p>Install LanguageClient-neovim by following <a href="https://github.com/autozimu/LanguageClient-neovim">the instructions</a></p>
<ul>
<li>The GitHub project wiki has extra tips on configuration</li>
</ul>
</li>
<li>
<p>Configure by adding this to your Vim/Neovim config file (replacing the existing WGSL or WESL-specific line if it exists):</p>
<pre><code class="language-vim">let g:LanguageClient_serverCommands = {
\ 'wgsl': ['wgsl-analyzer'],
\ 'wesl': ['wgsl-analyzer'],
\ }
</code></pre>
</li>
</ol>
<h3 id="using-lsp"><a class="header" href="#using-lsp">Using lsp</a></h3>
<ol>
<li>
<p>Install the <code>wgsl-analyzer</code> language server</p>
</li>
<li>
<p>Configure the <code>.wgsl</code> and <code>.wesl</code> filetype</p>
<p>Create <code>/ftdetect/wgsl.lua</code> and <code>/ftdetect/wesl.lua</code> in your neovim configuration.</p>
<pre><code class="language-lua">vim.api.nvim_create_autocmd({ "BufRead", "BufNewFile" }, { pattern = "*.wgsl",  command = "setfiletype wgsl" })
</code></pre>
<pre><code class="language-lua">vim.api.nvim_create_autocmd({ "BufRead", "BufNewFile" }, { pattern = "*.wesl",  command = "setfiletype wesl" })
</code></pre>
</li>
<li>
<p>Configure the nvim lsp</p>
<pre><code class="language-lua">local lspconfig = require('lspconfig')
lspconfig.wgsl_analyzer.setup({})
</code></pre>
</li>
</ol>
<h3 id="using-cocnvim"><a class="header" href="#using-cocnvim">Using coc.nvim</a></h3>
<ul>
<li>Requires CoC to be installed: <a href="https://github.com/neoclide/coc.nvim">https://github.com/neoclide/coc.nvim</a></li>
<li>Requires cargo to be installed to build binaries:</li>
</ul>
<ol>
<li>
<p>Install the language server</p>
<pre><code class="language-bash">cargo install --git https://github.com/wgsl-analyzer/wgsl-analyzer.git wgsl-analyzer
</code></pre>
<p>(if you are not familiar with using and setting up cargo, you might run into problems finding your binary.
Ensure that $HOME/.cargo/bin is in your $PATH. More Info about $PATH: <a href="https://linuxconfig.org/linux-path-environment-variable">https://linuxconfig.org/linux-path-environment-variable</a>)</p>
</li>
<li>
<p>open Neovim / Vim and type <code>:CocConfig</code> to configure coc.nvim.</p>
</li>
<li>
<p>under <code>.languageserver: { ... }</code> create a new field <code>"wgsl-analyzer-language-server"</code>. The field should look like this:</p>
<pre><code class="language-jsonc">//  {
//    "languageserver": {
        "wgsl-analyzer-language-server": {
          "command": "wgsl-analyzer", // alternatively you can specify the absolute path to your binary.
          "filetypes": ["wgsl", "wesl"],
        },
//      ...
//  }
</code></pre>
</li>
<li>
<p>In order for your editor to recognize WGSL files as such, you need to put this into your <code>vim.rc</code></p>
<pre><code class="language-vim">" Recognize wgsl
au BufNewFile,BufRead *.wgsl set filetype=wgsl
</code></pre>
</li>
</ol>
<h3 id="using-nvim-cmpcmp_nvim_lsp"><a class="header" href="#using-nvim-cmpcmp_nvim_lsp">Using nvim-cmp/cmp_nvim_lsp</a></h3>
<p>Requires <a href="https://github.com/hrsh7th/nvim-cmp">nvim-cmp</a> and <a href="https://github.com/hrsh7th/cmp-nvim-lsp">cmp_nvim_lsp</a>.</p>
<ol>
<li>
<p>Your existing setup should look similar to this:</p>
<pre><code class="language-lua">local capabilities = vim.lsp.protocol.make_client_capabilities()
capabilities = vim.tbl_deep_extend("force", capabilities, require("cmp_nvim_lsp").default_capabilities())

local lspconfig = require("lspconfig")
</code></pre>
</li>
<li>
<p>Pass capabilities to the <code>wgsl-analyzer</code> setup:</p>
<pre><code class="language-lua">lspconfig.wgsl_analyzer.setup({
   filetypes = { "wgsl", "wesl" },
   capabilities = capabilities,
})
</code></pre>
</li>
</ol>
<h3 id="youcompleteme"><a class="header" href="#youcompleteme">YouCompleteMe</a></h3>
<p>Install YouCompleteMe by following <a href="https://github.com/ycm-core/YouCompleteMe#installation">the instructions</a>.</p>
<p><code>wgsl-analyzer</code> is the default in ycm, it should work out of the box.</p>
<h3 id="ale"><a class="header" href="#ale">ALE</a></h3>
<p>To use the LSP server in <a href="https://github.com/dense-analysis/ale">ale</a>:</p>
<pre><code class="language-vim">let g:ale_linters = {'wgsl': ['analyzer'], 'wesl': ['analyzer']}
</code></pre>
<h3 id="nvim-lsp"><a class="header" href="#nvim-lsp">nvim-lsp</a></h3>
<p>Neovim 0.5 has built-in language server support.
For a quick start configuration of <code>wgsl-analyzer</code>, use <a href="https://github.com/neovim/nvim-lspconfig#wgsl-analyzer">neovim/nvim-lspconfig</a>.
Once <code>neovim/nvim-lspconfig</code> is installed, use <code>lua require'lspconfig'.wgsl_analyzer.setup({})</code> in your <code>init.vim</code>.</p>
<p>You can also pass LSP settings to the server:</p>
<pre><code class="language-lua">lua &lt;&lt; EOF
local lspconfig = require'lspconfig'

local on_attach = function(client)
  require'completion'.on_attach(client)
end

lspconfig.wgsl_analyzer.setup({
  on_attach = on_attach,
  settings = {
    ["wgsl-analyzer"] = {
      
    }
  }
})
EOF
</code></pre>
<p>If you are running Neovim 0.10 or later, you can enable inlay hints via <code>on_attach</code>:</p>
<pre><code class="language-lua">lspconfig.wgsl_analyzer.setup({
  on_attach = function(client, bufnr)
    vim.lsp.inlay_hint.enable(true, { bufnr = bufnr })
  end
})
</code></pre>
<p>Note that the hints are only visible after <code>wgsl-analyzer</code> has finished loading <strong>and</strong> you have to edit the file to trigger a re-render.</p>
<h3 id="vim-lsp"><a class="header" href="#vim-lsp">vim-lsp</a></h3>
<p>vim-lsp is installed by following <a href="https://github.com/prabirshrestha/vim-lsp">the plugin instructions</a>.
It can be as simple as adding this line to your <code>.vimrc</code>:</p>
<pre><code class="language-vim">Plug 'prabirshrestha/vim-lsp'
</code></pre>
<p>Next you need to register the <code>wgsl-analyzer</code> binary.
If it is available in <code>$PATH</code>, you may want to add this to your <code>.vimrc</code>:</p>
<pre><code class="language-vim">if executable('wgsl-analyzer')
  au User lsp_setup call lsp#register_server({
    \   'name': 'wgsl-analyzer Language Server',
    \   'cmd': {server_info-&gt;['wgsl-analyzer']},
    \   'whitelist': ['wgsl', 'wesl'],
    \ })
endif
</code></pre>
<p>There is no dedicated UI for the server configuration, so you would need to send any options as a value of the <code>initialization_options</code> field, as described in the <a href="./configuration.html">Configuration</a> section.
Here is an example of how to enable the proc-macro support:</p>
<pre><code class="language-vim">if executable('wgsl-analyzer')
  au User lsp_setup call lsp#register_server({
    \   'name': 'wgsl-analyzer Language Server',
    \   'cmd': {server_info-&gt;['wgsl-analyzer']},
    \   'whitelist': ['wgsl', 'wesl'],
    \   'initialization_options': {
    \     'cargo': {
    \       'buildScripts': {
    \         'enable': v:true,
    \       },
    \     },
    \     'procMacro': {
    \       'enable': v:true,
    \     },
    \   },
    \ })
endif
</code></pre>
<h2 id="sublime-text"><a class="header" href="#sublime-text">Sublime Text</a></h2>
<h3 id="sublime-text-4"><a class="header" href="#sublime-text-4">Sublime Text 4</a></h3>
<p>Follow the instructions in <a href="https://github.com/sublimelsp/LSP-rust-analyzer">LSP-rust-analyzer</a>, but substitute <code>rust</code> with <code>wgsl</code> where applicable.</p>
<p>Install <a href="https://packagecontrol.io/packages/LSP-file-watcher-chokidar">LSP-file-watcher-chokidar</a> to enable file watching (<code>workspace/didChangeWatchedFiles</code>).</p>
<h3 id="sublime-text-3"><a class="header" href="#sublime-text-3">Sublime Text 3</a></h3>
<ul>
<li>Install the <a href="https://packagecontrol.io/packages/LSP">LSP package</a>.</li>
<li>From the command palette, run <code>LSP: Enable Language Server Globally</code> and select <code>wgsl-analyzer</code>.</li>
</ul>
<p>If it worked, you should see "wgsl-analyzer, Line X, Column Y" on the left side of the status bar, and after waiting a bit, functionalities like tooltips on hovering over variables should become available.</p>
<p>If you get an error saying <code>No such file or directory: 'wgsl-analyzer'</code>, see the <a href="./wgsl-analyzer_binary.html"><code>wgsl-analyzer</code> binary installation</a> section.</p>
<h2 id="gnome-builder"><a class="header" href="#gnome-builder">GNOME Builder</a></h2>
<p>No support.</p>
<h2 id="eclipse-ide"><a class="header" href="#eclipse-ide">Eclipse IDE</a></h2>
<p>No support.</p>
<h2 id="kate-text-editor"><a class="header" href="#kate-text-editor">Kate Text Editor</a></h2>
<p>Support for the language server protocol is built into Kate through the LSP plugin, which is included by default.</p>
<p>To change <code>wgsl-analyzer</code> config options, start from the following example and put it into Kate's "User Server Settings" tab (located under the LSP Client settings):</p>
<pre><code class="language-json">{
  "servers": {
    "wgsl": {
      "command": ["wgsl-analyzer"],
      "url": "https://github.com/wgsl-analyzer/wgsl-analyzer",
      "highlightingModeRegex": "^WGSL$"
    },
    "wesl": {
      "command": ["wgsl-analyzer"],
      "url": "https://github.com/wgsl-analyzer/wgsl-analyzer",
      "highlightingModeRegex": "^WESL$"
    }
  }
}
</code></pre>
<p>Then click on apply, and restart the LSP server for your WGSL code or WESL project.</p>
<h2 id="juci"><a class="header" href="#juci">juCi++</a></h2>
<p><a href="https://gitlab.com/cppit/jucipp">juCi++</a> has built-in support for the language server protocol.</p>
<h2 id="kakoune"><a class="header" href="#kakoune">Kakoune</a></h2>
<p><a href="https://kakoune.org">Kakoune</a> supports LSP with the help of <a href="https://github.com/kak-lsp/kak-lsp"><code>kak-lsp</code></a>.
Follow the <a href="https://github.com/kak-lsp/kak-lsp#installation">instructions</a> to install <code>kak-lsp</code>.
To configure <code>kak-lsp</code>, refer to the <a href="https://github.com/kak-lsp/kak-lsp#configuring-kak-lsp">configuration section</a>.
It is about copying the <a href="https://github.com/kak-lsp/kak-lsp/blob/master/kak-lsp.toml">configuration file</a> to the right place.
The latest versions should use <code>wgsl-analyzer</code> by default.</p>
<p>Finally, you need to configure Kakoune to talk to <code>kak-lsp</code> (see <a href="https://github.com/kak-lsp/kak-lsp#usage">Usage section</a>).
A basic configuration will only get you LSP but you can also activate inlay diagnostics and auto-formatting on save.
The following might help you understand all of this:</p>
<pre><code class="language-kakoune">eval %sh{kak-lsp --kakoune -s $kak_session}  # Not needed if you load it with plug.kak.
hook global WinSetOption filetype=(wgsl|wesl) %{
  # Enable LSP
  lsp-enable-window

  # Auto-formatting on save
  hook window BufWritePre .* lsp-formatting-sync

  # Configure inlay hints (only on save)
  hook window -group wgsl-inlay-hints BufWritePost .* wgsl-analyzer-inlay-hints
  hook -once -always window WinSetOption filetype=.* %{
    remove-hooks window wgsl-inlay-hints
  }
}
</code></pre>
<h2 id="helix"><a class="header" href="#helix">Helix</a></h2>
<p><a href="https://docs.helix-editor.com">Helix</a> supports LSP by default.
However, it will not install <code>wgsl-analyzer</code> automatically.
You can follow instructions for <a href="./wgsl-analyzer_binary.html">installing the <code>wgsl-analyzer</code> binary</a>.</p>
<h2 id="visual-studio-2022"><a class="header" href="#visual-studio-2022">Visual Studio 2022</a></h2>
<p>No support.</p>
<h2 id="lapce"><a class="header" href="#lapce">Lapce</a></h2>
<p>No support.</p>
<h2 id="zed"><a class="header" href="#zed">Zed</a></h2>
<p>No support.</p>
<h2 id="intellij-ides"><a class="header" href="#intellij-ides">IntelliJ IDEs</a></h2>
<p>This includes:</p>
<ul>
<li>IntelliJ IDEA Ultimate</li>
<li>WebStorm</li>
<li>PhpStorm</li>
<li>PyCharm Professional</li>
<li>DataSpell</li>
<li>RubyMine</li>
<li>CLion</li>
<li>Aqua</li>
<li>DataGrip</li>
<li>GoLand</li>
<li>Rider</li>
<li>RustRover</li>
</ul>
<p>No support.</p>
<p>See #207</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="troubleshooting"><a class="header" href="#troubleshooting">Troubleshooting</a></h1>
<p>Start with looking at the <code>wgsl-analyzer</code> version.
Try the <strong>wgsl-analyzer: Show WA Version</strong> command in the <strong>Command Palette</strong>.
(Open the command pallete with  <kbd>Ctrl</kbd>+<kbd>Shift</kbd>+<kbd>P</kbd>)  <!-- spellchecker:disable-line -->
You can also run <code>wgsl-analyzer --version</code> in the command line.
If the date is more than a week ago, it is better to update your installation of <code>wgsl-analyzer</code> to the newest version.</p>
<p>The next thing to check would be panic messages in <code>wgsl-analyzer</code>'s log.
Log messages are printed to stderr, in VS Code you can see them in the <code>Output &gt; wgsl-analyzer Language Server</code> tab of the panel.
To see more logs, set the <code>WA_LOG=info</code> environment variable, this can be done either by setting the environment variable manually or by using <code>wgsl-analyzer.server.extraEnv</code>.
Note that both of these approaches require the server to be restarted.</p>
<p>To fully capture LSP messages between the editor and the server, run the <code>wgsl-analyzer: Toggle LSP Logs</code> command and check <code>Output &gt; wgsl-analyzer Language Server Trace</code>.</p>
<p>The root cause for many "nothing works" problems is that <code>wgsl-analyzer</code> fails to understand the project structure.
To debug that, first note the <code>wgsl-analyzer</code> section in the status bar.
If it has an error icon and red, that is the problem (hover will have somewhat helpful error message).
<strong>wgsl-analyzer: Status</strong> prints dependency information for the current file.
Finally, <code>WA_LOG=project_model=debug</code> enables verbose logs during project loading.</p>
<p>If <code>wgsl-analyzer</code> outright crashes, try running <code>wgsl-analyzer analysis-stats /path/to/project/directory/</code> on the command line.
This command type checks the whole project in batch mode bypassing LSP machinery.</p>
<p>When filing issues, it is useful (but not necessary) to try to minimize examples.</p>
<p>An ideal bug reproduction looks like this:</p>
<pre><code class="language-bash">$ git clone https://github.com/username/repo.git &amp;&amp; cd repo &amp;&amp; git switch --detach commit-hash
$ wgsl-analyzer --version
wgsl-analyzer dd12184e4 2021-05-08 dev
$ wgsl-analyzer analysis-stats .
💀 💀 💀
</code></pre>
<p>It is especially useful when the <code>repo</code> does not use external crates or the standard library.</p>
<p>If you want to go as far as to modify the source code to debug the problem, be sure to take a look at the <a href="https://github.com/wgsl-analyzer/wgsl-analyzer/tree/master/docs/dev">dev docs</a>!</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="configuration"><a class="header" href="#configuration">Configuration</a></h1>
<p><strong>Source:</strong> <a href="https://github.com/wgsl-analyzer/wgsl-analyzer/blob/main/crates/wgsl-analyzer/src/config.rs">config.rs</a></p>
<p>The <a href="./installation.html">Installation</a> section contains details on configuration for some of the editors.
In general, <code>wgsl-analyzer</code> is configured via LSP messages, which means that it is up to the editor to decide on the exact format and location of configuration files.</p>
<p>Some editors, such as <a href="./vs_code.html">VS Code</a> or <a href="./other_editors.html#coc-wgsl-analyzer">COC plugin in Vim</a>, provide <code>wgsl-analyzer</code>-specific configuration UIs.
Other editors may require you to know a bit more about the interaction with <code>wgsl-analyzer</code>.</p>
<p>For the latter category, it might help to know that the initial configuration is specified as a value of the <code>initializationOptions</code> field of the <a href="https://microsoft.github.io/language-server-protocol/specifications/specification-current/#initialize"><code>InitializeParameters</code> message, in the LSP protocol</a>.
The spec says that the field type is <code>any?</code>, but <code>wgsl-analyzer</code> is looking for a JSON object that is constructed using settings from the list below.
The name of the setting, ignoring the <code>wgsl-analyzer.</code> prefix, is used as a path, and the value of the setting becomes the JSON property value.</p>
<p>Please consult your editor's documentation to learn more about how to configure <a href="https://microsoft.github.io/language-server-protocol">LSP servers</a>.</p>
<p>To verify which configuration is actually used by <code>wgsl-analyzer</code>, set the <code>WA_LOG</code> environment variable to <code>wgsl_analyzer=info</code> and look for config-related messages.
Logs should show both the JSON that <code>wgsl-analyzer</code> sees as well as the updated config.</p>
<p>This is the list of config options <code>wgsl-analyzer</code> supports:</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="security"><a class="header" href="#security">Security</a></h1>
<p>At the moment, <code>wgsl-analyzer</code> assumes that all code is trusted.
Here is a <strong>non-exhaustive</strong> list of ways to make <code>wgsl-analyzer</code> execute arbitrary code:</p>
<ul>
<li>
<p>VS Code plugin reads configuration from project directory, and that can be used to override paths to various executables, like <code>wgslfmt</code> or <code>wgsl-analyzer</code> itself.</p>
</li>
<li>
<p><code>wgsl-analyzer</code>'s syntax trees library uses a lot of <code>unsafe</code> and has not been properly audited for memory safety.</p>
</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="privacy"><a class="header" href="#privacy">Privacy</a></h1>
<p>The LSP server and the Code extension may access the network if the user configures it to import shaders from the internet.</p>
<p>Any other editor plugins are not under the control of the <code>wgsl-analyzer</code> developers.
For any privacy concerns, you should check with their respective developers.</p>
<p>For <code>wgsl-analyzer</code> developers, <code>cargo xtask release</code> uses the GitHub API to put together the release notes.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="features"><a class="header" href="#features">Features</a></h1>
<div style="break-before: page; page-break-before: always;"></div><h1 id="assists"><a class="header" href="#assists">Assists</a></h1>
<p>Assists, or code actions, are small local refactorings available in a particular context.
They are usually triggered by a shortcut or by clicking a light bulb icon in the editor.
Cursor position or selection is signified by the <code>┃</code> character.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="diagnostics"><a class="header" href="#diagnostics">Diagnostics</a></h1>
<p>Most errors and warnings provided by <code>wgsl-analyzer</code> come from <code>wgsl-analyzer</code>'s own analysis.
Some of these diagnostics do not respect <code>// wgsl-analyzer</code> diagnostic control comments yet.
They can be turned off using the <code>wgsl-analyzer.diagnostics.enable</code>, <code>wgsl-analyzer.diagnostics.experimental.enable</code>, or <code>wgsl-analyzer.diagnostics.disabled</code> settings.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="editor-features"><a class="header" href="#editor-features">Editor Features</a></h1>
<h2 id="vs-code-1"><a class="header" href="#vs-code-1">VS Code</a></h2>
<h3 id="color-configurations"><a class="header" href="#color-configurations">Color configurations</a></h3>
<p>It is possible to change the foreground/background color and font family/size of inlay hints.
Just add this to your <code>settings.json</code>:</p>
<pre><code class="language-json">{
  "editor.inlayHints.fontFamily": "Courier New",
  "editor.inlayHints.fontSize": 11,

  "workbench.colorCustomizations": {
    // Name of the theme you are currently using
    "[Default Dark+]": {
      "editorInlayHint.foreground": "#868686f0",
      "editorInlayHint.background": "#3d3d3d48",

      // Overrides for specific kinds of inlay hints
      "editorInlayHint.typeForeground": "#fdb6fdf0",
      "editorInlayHint.parameterForeground": "#fdb6fdf0",
    }
  }
}
</code></pre>
<h3 id="semantic-style-customizations"><a class="header" href="#semantic-style-customizations">Semantic style customizations</a></h3>
<p>You can customize the look of different semantic elements in the source code.
For example, mutable bindings are underlined by default, and you can override this behavior by adding the following section to your <code>settings.json</code>:</p>
<pre><code class="language-json">{
  "editor.semanticTokenColorCustomizations": {
    "rules": {
      "*.mutable": {
        "fontStyle": "" // underline is the default
      }
    }
  }
}
</code></pre>
<p>Most themes do not support styling unsafe operations differently yet.
You can fix this by adding overrides for the rules <code>operator.unsafe</code>, <code>function.unsafe</code>, and <code>method.unsafe</code>:</p>
<pre><code class="language-json">{
  "editor.semanticTokenColorCustomizations": {
    "rules": {
      "operator.unsafe": "#ff6600",
      "function.unsafe": "#ff6600",
      "method.unsafe": "#ff6600"
    }
  }
}
</code></pre>
<p>In addition to the top-level rules, you can specify overrides for specific themes.
For example, if you wanted to use a darker text color on a specific light theme, you might write:</p>
<pre><code class="language-json">{
  "editor.semanticTokenColorCustomizations": {
    "rules": {
      "operator.unsafe": "#ff6600"
    },
    "[Ayu Light]": {
      "rules": {
        "operator.unsafe": "#572300"
      }
    }
  }
}
</code></pre>
<p>Make sure you include the brackets around the theme name.
For example, use <code>"[Ayu Light]"</code> to customize the theme Ayu Light.</p>
<h3 id="special-when-clause-context-for-keybindings"><a class="header" href="#special-when-clause-context-for-keybindings">Special <code>when</code> clause context for keybindings</a></h3>
<p>You may use the <code>inWeslProject</code> context to configure keybindings for WGSL/WESL projects only.
For example:</p>
<pre><code class="language-json">{
  "key": "ctrl+alt+d",
  "command": "wgsl-analyzer.openDocs",
  "when": "inWeslProject"
}
</code></pre>
<p>More about <a href="https://code.visualstudio.com/docs/getstarted/keybindings#_when-clause-contexts"><code>when</code> clause contexts</a>.</p>
<h3 id="setting-runnable-environment-variables"><a class="header" href="#setting-runnable-environment-variables">Setting runnable environment variables</a></h3>
<p>You can use the <code>wgsl-analyzer.runnables.extraEnv</code> setting to define runnable environment-specific substitution variables.
The simplest way for all runnables in a bunch:</p>
<pre><code class="language-json">"wgsl-analyzer.runnables.extraEnv": {
  "RUN_SLOW_TESTS": "1"
}
</code></pre>
<p>Or it is possible to specify vars more granularly:</p>
<pre><code class="language-json">"wgsl-analyzer.runnables.extraEnv": [
  {
    // "mask": null, // null mask means that this rule will be applied for all runnables
    "env": {
      "APP_ID": "1",
      "APP_DATA": "asdf"
    }
  },
  {
    "mask": "test_name",
    "env": {
      "APP_ID": "2" // overwrites only APP_ID
    }
  }
]
</code></pre>
<p>You can use any valid regular expression as a mask.
Also, note that a full runnable name is something like <strong><code>run bin_or_example_name</code></strong>, <strong><code>test some::mod::test_name</code></strong>, or <strong><code>test-mod some::mod</code></strong>.
It is possible to distinguish binaries, single tests, and test modules with these masks: <code>"^run"</code>, <code>"^test "</code> (the trailing space matters!), and <code>"^test-mod"</code> respectively.</p>
<p>If needed, you can set different values for different platforms:</p>
<pre><code class="language-json">"wgsl-analyzer.runnables.extraEnv": [
  {
    "platform": "win32", // windows only
    "env": {
      "APP_DATA": "windows specific data"
    }
  },
  {
    "platform": ["linux"],
    "env": {
      "APP_DATA": "linux data"
    }
  },
  { // for all platforms
    "env": {
      "APP_COMMON_DATA": "xxx"
    }
  }
]
</code></pre>
<h3 id="compiler-feedback-from-external-commands"><a class="header" href="#compiler-feedback-from-external-commands">Compiler feedback from external commands</a></h3>
<p>You can configure VS Code to run a command in the background and use the <code>$wgsl-analyzer-watch</code> problem matcher to generate inline error markers from its output.
To do this, you need to create a new <a href="https://code.visualstudio.com/docs/editor/tasks">VS Code Task</a> and set <code>"wgsl-analyzer.checkOnSave": false</code> in preferences.
Example <code>.vscode/tasks.json</code>:</p>
<pre><code class="language-json">{
  "label": "Watch",
  "group": "build",
  "type": "shell",
  "command": "example-tool watch",
  "problemMatcher": "$wgsl-analyzer-watch",
  "isBackground": true
}
</code></pre>
<h3 id="live-share"><a class="header" href="#live-share">Live Share</a></h3>
<p>VS Code Live Share has partial support for <code>wgsl-analyzer</code>.</p>
<p>Live Share <em>requires</em> the official Microsoft build of VS Code; OSS builds will not work correctly.</p>
<p>The host's <code>wgsl-analyzer</code> instance will be shared with all guests joining the session.
The guests do not have to have the <code>wgsl-analyzer</code> extension installed for this to work.</p>
<p>If you are joining a Live Share session and <em>do</em> have <code>wgsl-analyzer</code> installed locally, then commands from the command palette will not work correctly.
This is because they will attempt to communicate with the <em>local</em> server, not the server of the session host.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="contributing-quick-start"><a class="header" href="#contributing-quick-start">Contributing Quick Start</a></h1>
<p><code>wgsl-analyzer</code> is an ordinary Rust project, which is organized as a Cargo workspace, builds on stable, and does not depend on C libraries.</p>
<p>Simply run the following to get started:</p>
<pre><code class="language-bash">cargo test
</code></pre>
<p>To learn more about how <code>wgsl-analyzer</code> works, see <a href="contributing/architecture.html">Architecture</a>.
It also explains the high-level layout of the source code.
Do skim through that document.</p>
<p>We also publish rustdoc docs to pages: <a href="https://wgsl-analyzer.github.io/wgsl-analyzer/ide">https://wgsl-analyzer.github.io/wgsl-analyzer/ide</a>.
Note that the internal documentation is very incomplete.</p>
<p>Various organizational and process issues are discussed in this document.</p>
<h2 id="getting-in-touch"><a class="header" href="#getting-in-touch">Getting in Touch</a></h2>
<p>Discussion happens in this Discord server:</p>
<p><a href="https://discord.gg/3QUGyyz984">https://discord.gg/3QUGyyz984</a></p>
<ul>
<li><a href="contributing/index.html#issue-labels">Issue Labels</a></li>
<li><a href="contributing/index.html#code-style--review-process">Code Style &amp; Review Process</a></li>
<li><a href="contributing/index.html#cookbook">Cookbook</a>
<ul>
<li><a href="contributing/index.html#ci">CI</a></li>
<li><a href="contributing/index.html#launching-wgsl-analyzer">Launching wgsl-analyzer</a></li>
<li><a href="contributing/index.html#typescript-tests">TypeScript Tests</a></li>
<li><a href="contributing/index.html#how-to">How to</a></li>
<li><a href="contributing/index.html#logging">Logging</a></li>
<li><a href="contributing/index.html#profiling">Profiling</a></li>
<li><a href="contributing/index.html#release-process">Release Process</a></li>
<li><a href="contributing/index.html#permissions">Permissions</a></li>
</ul>
</li>
<li><a href="contributing/index.html#triage-team">Triage Team</a></li>
</ul>
<h2 id="issue-labels"><a class="header" href="#issue-labels">Issue Labels</a></h2>
<p><a href="https://github.com/wgsl-analyzer/wgsl-analyzer/labels">https://github.com/wgsl-analyzer/wgsl-analyzer/labels</a></p>
<ul>
<li>[A-Analyzer]: Affects the wgsl-analyzer crate</li>
<li>[A-Base-DB]: Affects the base_db crate</li>
<li>[A-Build-System]: CI stuff</li>
<li>[A-Completion]: Affects the ide_completion crate</li>
<li>[A-Cross-Cutting]: Affects many crates</li>
<li>[A-Formatter]: Affects the wgsl-formatter crate</li>
<li>[A-HIR]: Affects the hir or hir_def crate</li>
<li>[A-IDE]: Affects the ide crate</li>
<li>[A-Meta]: Affects non-code files such as documentation</li>
<li>[A-wgslfmt]: Affects the wgslfmt crate</li>
<li>[C-Bug]: Something isn't working</li>
<li>[C-Dependencies]: Bump and migrate a dependency</li>
<li>[C-Documentation]: Improvements or additions to documentation</li>
<li>[C-Enhancement]: Improvement over an existing feature</li>
<li>[C-Feature]: New feature or request</li>
<li>[D-Complex]: Large implications, lots of changes, much thought</li>
<li>[D-Modest]: "Normal" difficulty of solving</li>
<li>[D-Straightforward]: Relatively easy to solve</li>
<li>[D-Trivial]: Good for newcomers</li>
<li>[S-Adopt-Me]: Extra attention is needed</li>
<li>[S-Blocked]: Blocked on something else happening</li>
<li>[S-Duplicate]: This issue or pull request already exists</li>
<li>[S-Needs-Design]: The way this should be done is not yet clear</li>
<li>[S-Needs-Investigation]: The cause of the issue is TBD</li>
<li>[S-Needs-Triage]: Hasn't been triaged yet</li>
<li>[S-Ready-to-Implement]: This issue is actionable and a solution can be proposed</li>
<li>[S-Ready-to-Review]: This change is in a good state and needs someone (anyone!) to review it</li>
<li>[S-Waiting-on-Author]: A change or a response from the author is needed</li>
<li>[S-Won't-Fix]: This will not be worked on</li>
</ul>
<h2 id="code-style--review-process"><a class="header" href="#code-style--review-process">Code Style &amp; Review Process</a></h2>
<p>See the <a href="contributing/style.html">Style Guide</a>.</p>
<h2 id="cookbook"><a class="header" href="#cookbook">Cookbook</a></h2>
<h3 id="ci"><a class="header" href="#ci">CI</a></h3>
<p>We use GitHub Actions for CI.
Most of the things, including formatting, are checked by <code>cargo test</code>.
If <code>cargo test</code> passes locally, that is a good sign that CI will be green as well.
The only exception is that some long-running tests are skipped locally by default.
Use <code>env RUN_SLOW_TESTS=1 cargo test</code> to run the full suite.</p>
<p>We use bors to enforce the <a href="https://graydon2.dreamwidth.org/1597.html">not rocket science</a> rule.</p>
<h3 id="launching-wgsl-analyzer"><a class="header" href="#launching-wgsl-analyzer">Launching wgsl-analyzer</a></h3>
<p>Debugging the language server can be tricky.
LSP is rather chatty, so driving it from the command line is not really feasible, driving it via VS Code requires interacting with two processes.</p>
<p>For this reason, the best way to see how <code>wgsl-analyzer</code> works is to <strong>find a relevant test and execute it</strong>.</p>
<p>Launching a VS Code instance with a locally built language server is also possible.
There is <strong>"Run Extension (Debug Build)"</strong> launch configuration for this in VS Code.</p>
<p>In general, I use one of the following workflows for fixing bugs and implementing features:</p>
<p>If the problem concerns only internal parts of <code>wgsl-analyzer</code> (i.e. I do not need to touch the <code>wgsl-analyzer</code> crate or TypeScript code), there is a unit-test for it.
So, I use <strong>wgsl-analyzer: Run</strong> action in VS Code to run this single test, and then just do printf-driven development/debugging.
As a sanity check after I am done, I use <code>cargo xtask install --server</code> and <strong>Reload Window</strong> action in VS Code to verify that the thing works as I expect.</p>
<p>If the problem concerns only the VS Code extension, I use <strong>Run Installed Extension</strong> launch configuration from <code>launch.json</code>.
Notably, this uses the usual <code>wgsl-analyzer</code> binary from <code>PATH</code>.
For this, it is important to have the following in your <code>settings.json</code> file:</p>
<pre><code class="language-json">{
    "wgsl-analyzer.server.path": "wgsl-analyzer"
}
</code></pre>
<p>After I am done with the fix, I use <code>cargo xtask install --client</code> to try the new extension for real.</p>
<p>If I need to fix something in the <code>wgsl-analyzer</code> crate, I feel sad because it is on the boundary between the two processes, and working there is slow.
I usually just <code>cargo xtask install --server</code> and poke changes from my live environment.
Note that this uses <code>--release</code>, which is usually faster overall, because loading stdlib into debug version of <code>wgsl-analyzer</code> takes a lot of time.
Note that you should only use the <code>eprint!</code> family of macros for debugging: stdout is used for LSP communication, and <code>print!</code> would break it.</p>
<p>If I need to fix something simultaneously in the server and in the client, I feel even more sad.
I do not have a specific workflow for this case.</p>
<!--
Additionally, I use `cargo run --release -p wgsl-analyzer -- analysis-stats path/to/some/WESL/code` to run a batch analysis.
This is primarily useful for performance optimizations, or for bug minimization.
-->
<h3 id="typescript-tests"><a class="header" href="#typescript-tests">TypeScript Tests</a></h3>
<p>If you change files under <code>editors/code</code> and would like to run the tests and linter, install npm and run:</p>
<pre><code class="language-bash">cd editors/code
npm ci
npm run ci
</code></pre>
<p>Run <code>npm run</code> to see all available scripts.</p>
<h3 id="how-to"><a class="header" href="#how-to">How to</a></h3>
<!-- TODO: make wgsl-analyzer pulls as examples -->
<ul>
<li>... add an assist? <a href="https://github.com/rust-lang/rust-analyzer/pull/7535">#7535</a></li>
<li>... add a new protocol extension? <a href="https://github.com/rust-lang/rust-analyzer/pull/4569">#4569</a></li>
<li>... add a new configuration option? <a href="https://github.com/rust-lang/rust-analyzer/pull/7451">#7451</a></li>
<li>... add a new completion? <a href="https://github.com/rust-lang/rust-analyzer/pull/6964">#6964</a></li>
<li>... allow new syntax in the parser? <a href="https://github.com/rust-lang/rust-analyzer/pull/7338">#7338</a></li>
</ul>
<h3 id="logging"><a class="header" href="#logging">Logging</a></h3>
<p>Logging is done by both <code>wgsl-analyzer</code> and VS Code, so it might be tricky to figure out where logs go.</p>
<p>Inside wgsl-analyzer, we use the <a href="https://docs.rs/tracing"><code>tracing</code></a> crate for logging, and <a href="https://docs.rs/tracing-subscriber"><code>tracing-subscriber</code></a> for logging frontend.
By default, log goes to stderr, but the stderr itself is processed by VS Code.
<code>--log-file &lt;PATH&gt;</code> CLI argument allows logging to file.
Setting the <code>WA_LOG_FILE=&lt;PATH&gt;</code> environment variable will also log to file, it will also override <code>--log-file</code>.</p>
<p>To see stderr in the running VS Code instance, go to the "Output" tab of the panel and select <code>wgsl-analyzer</code>.
This shows <code>eprintln!</code> as well.
Note that <code>stdout</code> is used for the actual protocol, so <code>println!</code> will break things.</p>
<p>To log all communication between the server and the client, there are two choices:</p>
<ul>
<li>
<p>You can log on the server side, by running something like</p>
<pre><code class="language-bash">env WA_LOG=lsp_server=debug code .
</code></pre>
</li>
<li>
<p>You can log on the client side, by the <code>wgsl-analyzer: Toggle LSP Logs</code> command or enabling <code>"wgsl-analyzer.trace.server": "verbose"</code> workspace setting.
These logs are shown in a separate tab in the output and could be used with LSP inspector.
Kudos to <a href="https://github.com/DJMcNab">@DJMcNab</a> for setting this awesome infra up!</p>
</li>
</ul>
<p>There are also several VS Code commands which might be of interest:</p>
<ul>
<li>
<p><code>wgsl-analyzer: Status</code> shows some memory-usage statistics.</p>
</li>
<li>
<p><code>wgsl-analyzer: View Hir</code> shows the HIR expressions within the function containing the cursor.</p>
</li>
<li>
<p>If <code>wgsl-analyzer.showSyntaxTree</code> is enabled in settings, <code>WGSL/WESL Syntax Tree: Focus on WGSL/WESL Syntax Tree View</code> shows the syntax tree of the current file.</p>
<p>You can click on nodes in the WGSL/WESL editor to go to the corresponding syntax node.</p>
<p>You can click on <code>Reveal Syntax Element</code> next to a syntax node to go to the corresponding code and highlight the proper text range.</p>
<p>If you trigger Go to Definition in the inspected source file, the syntax tree view should scroll to and select the appropriate syntax node token.</p>
<p>You can click on <code>Copy</code> next to a syntax node to copy a text representation of the node.</p>
<p><img src="https://github.com/user-attachments/assets/2d20ae87-0abf-495f-bee8-54aa2494a00d" alt="demo" /></p>
</li>
</ul>
<h3 id="profiling"><a class="header" href="#profiling">Profiling</a></h3>
<p>We have a built-in hierarchical profiler, you can enable it by using <code>WA_PROFILE</code> env-var:</p>
<pre><code class="language-bash">WA_PROFILE=*             // dump everything
WA_PROFILE=foo|bar|baz   // enabled only selected entries
WA_PROFILE=*@3&gt;10        // dump everything, up to depth 3, if it takes more than 10 ms
</code></pre>
<p>Some <code>wgsl-analyzer</code> contributors have <code>export WA_PROFILE='*&gt;10'</code> in their shell profile.</p>
<p>For machine-readable JSON output, we have the <code>WA_PROFILE_JSON</code> env variable.
We support filtering only by span name:</p>
<pre><code class="language-bash">WA_PROFILE=* // dump everything
WA_PROFILE_JSON="vfs_load|parallel_prime_caches|discover_command" // dump selected spans
</code></pre>
<p>We also have a "counting" profiler which counts number of instances of popular structs.
It is enabled by <code>WA_COUNT=1</code>.</p>
<!--
To measure time for from-scratch analysis, use something like this:

```bash
cargo run --release -p wgsl-analyzer -- analysis-stats ../chalk/
```

For measuring time of incremental analysis, use either of these:

```bash
cargo run --release -p wgsl-analyzer -- analysis-bench ../chalk/ --highlight ../chalk/chalk-engine/src/logic.rs
cargo run --release -p wgsl-analyzer -- analysis-bench ../chalk/ --complete ../chalk/chalk-engine/src/logic.rs:94:0
```

Look for `fn benchmark_xxx` tests for a quick way to reproduce performance problems.
-->
<h3 id="release-process"><a class="header" href="#release-process">Release Process</a></h3>
<p>Release process is handled by <code>release</code>, <code>dist</code>, <code>publish-release-notes</code> and <code>promote</code> xtasks, <code>release</code> being the main one.</p>
<p><code>release</code> assumes that you have checkouts of <code>wgsl-analyzer</code> and <code>wgsl-analyzer.github.io</code> in the same directory:</p>
<pre><code class="language-bash">./wgsl-analyzer
./wgsl-analyzer.github.io
</code></pre>
<p>The remote for <code>wgsl-analyzer</code> must be called <code>upstream</code> (I use <code>origin</code> to point to my fork).</p>
<p><code>release</code> calls the GitHub API calls to scrape pull request comments and categorize them in the changelog.
This step uses the <code>curl</code> and <code>jq</code> applications, which need to be available in <code>PATH</code>.
Finally, you need to obtain a GitHub personal access token and set the <code>GITHUB_TOKEN</code> environment variable.</p>
<p>Release steps:</p>
<ol>
<li>Set the <code>GITHUB_TOKEN</code> environment variable.</li>
<li>Inside wgsl-analyzer, run <code>cargo xtask release</code>.
This will:
<ul>
<li>checkout the <code>release</code> branch</li>
<li>reset it to <code>upstream/nightly</code></li>
<li>push it to <code>upstream</code>.
This triggers GitHub Actions which:
<ul>
<li>runs <code>cargo xtask dist</code> to package binaries and VS Code extension</li>
<li>makes a GitHub release</li>
<li>publishes the VS Code extension to the marketplace</li>
<li>call the GitHub API for PR details</li>
<li>create a new changelog in <code>wgsl-analyzer.github.io</code></li>
</ul>
</li>
</ul>
</li>
<li>While the release is in progress, fill in the changelog.</li>
<li>Commit &amp; push the changelog.</li>
<li>Run <code>cargo xtask publish-release-notes &lt;CHANGELOG&gt;</code> -- this will convert the changelog entry in AsciiDoc to Markdown and update the body of GitHub Releases entry.</li>
</ol>
<p>If the GitHub Actions release fails because of a transient problem like a timeout, you can re-run the job from the Actions console.
If it fails because of something that needs to be fixed, remove the release tag (if needed), fix the problem, then start over.
Make sure to remove the new changelog post created when running <code>cargo xtask release</code> a second time.</p>
<p>We release "nightly" every night automatically and promote the latest nightly to "stable" manually, every week.</p>
<p>We do not do "patch" releases, unless something truly egregious comes up.
To do a patch release, cherry-pick the fix on top of the current <code>release</code> branch and push the branch.
There is no need to write a changelog for a patch release, it is OK to include the notes about the fix into the next weekly one.
Note: we tag releases by dates, releasing a patch release on the same day should work (by overwriting a tag), but I am not 100% sure.</p>
<h3 id="permissions"><a class="header" href="#permissions">Permissions</a></h3>
<h2 id="triage-team"><a class="header" href="#triage-team">Triage Team</a></h2>
<p>We have a dedicated triage team that helps manage issues and pull requests on GitHub.
Members of the triage team have permissions to:</p>
<ul>
<li>Label issues and pull requests</li>
<li>Close and reopen issues</li>
<li>Assign issues and PRs to milestones</li>
</ul>
<p>This team plays a crucial role in ensuring that the project remains organized and that contributions are properly reviewed and addressed.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="architecture"><a class="header" href="#architecture">Architecture</a></h1>
<p>This document describes the high-level architecture of wgsl-analyzer.
If you want to familiarize yourself with the code base, you are just in the right place!</p>
<p>Since <code>wgsl-analyzer</code> is largely copied from <a href="https://github.com/rust-lang/rust-analyzer"><code>rust-analyzer</code></a>, you might also enjoy the <a href="https://www.youtube.com/playlist?list=PLhb66M_x9UmrqXhQuIpWC5VgTdrGxMx3y">Explaining Rust Analyzer</a> series on YouTube.
It goes deeper than what is covered in this document, but will take some time to watch.</p>
<p>See also these implementation-related blog posts:</p>
<ul>
<li><a href="https://rust-analyzer.github.io/blog/2019/11/13/find-usages.html">https://rust-analyzer.github.io/blog/2019/11/13/find-usages.html</a></li>
<li><a href="https://rust-analyzer.github.io/blog/2020/07/20/three-architectures-for-responsive-ide.html">https://rust-analyzer.github.io/blog/2020/07/20/three-architectures-for-responsive-ide.html</a></li>
<li><a href="https://rust-analyzer.github.io/blog/2020/09/16/challeging-LR-parsing.html">https://rust-analyzer.github.io/blog/2020/09/16/challeging-LR-parsing.html</a></li>
<li><a href="https://rust-analyzer.github.io/blog/2020/09/28/how-to-make-a-light-bulb.html">https://rust-analyzer.github.io/blog/2020/09/28/how-to-make-a-light-bulb.html</a></li>
<li><a href="https://rust-analyzer.github.io/blog/2020/10/24/introducing-ungrammar.html">https://rust-analyzer.github.io/blog/2020/10/24/introducing-ungrammar.html</a></li>
</ul>
<p>For older, by now mostly outdated stuff, see the <a href="contributing/./guide.html">guide</a> and <a href="https://www.youtube.com/playlist?list=PL85XCvVPmGQho7MZkdW-wtPtuJcFpzycE">another playlist</a>.</p>
<h2 id="birds-eye-view"><a class="header" href="#birds-eye-view">Bird's Eye View</a></h2>
<p><img src="https://user-images.githubusercontent.com/4789492/107129398-0ab70f00-687a-11eb-9bfc-d4eb023aec06.png" alt="Bird&#39;s Eye View diagram" /></p>
<ul>
<li><a href="contributing/architecture.html#entry-points">Entry Points</a></li>
<li><a href="contributing/architecture.html#code-map">Code Map</a>
<ul>
<li><a href="contributing/architecture.html#xtask"><code>xtask</code></a></li>
<li><a href="contributing/architecture.html#editorscode"><code>editors/code</code></a></li>
<li><a href="contributing/architecture.html#lib"><code>lib</code></a></li>
<li><a href="contributing/architecture.html#cratesparser"><code>crates/parser</code></a></li>
<li><a href="contributing/architecture.html#cratessyntax"><code>crates/syntax</code></a></li>
<li><a href="contributing/architecture.html#cratesbase-db"><code>crates/base-db</code></a></li>
<li><a href="contributing/architecture.html#crateshir-def-crateshir_ty"><code>crates/hir-def</code>, <code>crates/hir_ty</code></a></li>
<li><a href="contributing/architecture.html#crateshir"><code>crates/hir</code></a></li>
<li><a href="contributing/architecture.html#crateside-crateside-db-crateside-assists-crateside-completion-crateside-diagnostics-crateside-ssr"><code>crates/ide</code>, <code>crates/ide-db</code>, <code>crates/ide-assists</code>, <code>crates/ide-completion</code>, <code>crates/ide-diagnostics</code>, <code>crates/ide-ssr</code></a></li>
<li><a href="contributing/architecture.html#crateswgsl-analyzer"><code>crates/wgsl-analyzer</code></a></li>
<li><a href="contributing/architecture.html#cratestoolchain-cratesproject-model-cratesflycheck"><code>crates/toolchain</code>, <code>crates/project-model</code>, <code>crates/flycheck</code></a></li>
<li><a href="contributing/architecture.html#cratescfg"><code>crates/cfg</code></a></li>
<li><a href="contributing/architecture.html#cratesvfs-cratesvfs-notify-cratespaths"><code>crates/vfs</code>, <code>crates/vfs-notify</code>, <code>crates/paths</code></a></li>
<li><a href="contributing/architecture.html#cratesstdx"><code>crates/stdx</code></a></li>
<li><a href="contributing/architecture.html#cratesprofile"><code>crates/profile</code></a></li>
<li><a href="contributing/architecture.html#cratesspan"><code>crates/span</code></a></li>
</ul>
</li>
<li><a href="contributing/architecture.html#cross-cutting-concerns">Cross-Cutting Concerns</a>
<ul>
<li><a href="contributing/architecture.html#stability-guarantees">Stability Guarantees</a></li>
<li><a href="contributing/architecture.html#code-generation">Code generation</a></li>
<li><a href="contributing/architecture.html#cancellation">Cancellation</a></li>
<li><a href="contributing/architecture.html#testing">Testing</a></li>
<li><a href="contributing/architecture.html#performance-testing">Performance Testing</a></li>
<li><a href="contributing/architecture.html#error-handling">Error Handling</a></li>
<li><a href="contributing/architecture.html#observability">Observability</a></li>
<li><a href="contributing/architecture.html#configurability">Configurability</a></li>
</ul>
</li>
</ul>
<p>On the highest level, <code>wgsl-analyzer</code> is a thing which accepts input source code from the client and produces a structured semantic model of the code.</p>
<p>More specifically, input data consists of a set of test files (<code>(PathBuf, String)</code> pairs) and information about project structure, captured in the so-called <code>CrateGraph</code>.
The crate graph specifies which files are crate roots, which cfg flags are specified for each crate, and what dependencies exist between the crates.
This is the input (ground) state.
The analyzer keeps all this input data in memory and never does any IO.
Because the input data is source code, which typically measures in tens of megabytes at most, keeping everything in memory is OK.</p>
<p>A "structured semantic model" is basically an object-oriented representation of modules, functions, and types which appear in the source code.
This representation is fully "resolved": all expressions have types, all references are bound to declarations, etc.
This is derived state.</p>
<p>The client can submit a small delta of input data (typically, a change to a single file) and get a fresh code model which accounts for changes.</p>
<p>The underlying engine makes sure that the model is computed lazily (on-demand) and can be quickly updated for small modifications.</p>
<h2 id="entry-points"><a class="header" href="#entry-points">Entry Points</a></h2>
<p><code>crates/wgsl-analyzer/src/bin/main.rs</code> contains the main function which spawns LSP.
This is <em>the</em> entry point, but it front-loads a lot of complexity, so it is fine to just skim through it.</p>
<p><code>crates/wgsl-analyzer/src/handlers/request.rs</code> implements all LSP requests and is a great place to start if you are already familiar with LSP.</p>
<p><code>Analysis</code> and <code>AnalysisHost</code> types define the main API for consumers of IDE services.</p>
<h2 id="code-map"><a class="header" href="#code-map">Code Map</a></h2>
<p>This section talks briefly about various important directories and data structures.
Pay attention to the <strong>Architecture Invariant</strong> sections.
They often talk about things which are deliberately absent in the source code.</p>
<p>Note also which crates are <strong>API Boundaries</strong>.
Remember, <a href="https://www.tedinski.com/2018/02/06/system-boundaries.html">rules at the boundary are different</a>.</p>
<h3 id="xtask"><a class="header" href="#xtask"><code>xtask</code></a></h3>
<p>This is <code>wgsl-analyzer</code>'s "build system".
We use <a href="https://github.com/rust-lang/cargo"><code>cargo</code></a> to compile Rust code, but there are also various other tasks, such as release management or local installation.
Those are handled by Rust code in the <code>xtask</code> directory.</p>
<h3 id="editorscode"><a class="header" href="#editorscode"><code>editors/code</code></a></h3>
<p>The VS Code extension.</p>
<h3 id="lib"><a class="header" href="#lib"><code>lib</code></a></h3>
<p><code>wgsl-analyzer</code>-independent libraries which we publish to crates.io.
It is not heavily utilized at the moment.</p>
<h3 id="cratesparser"><a class="header" href="#cratesparser"><code>crates/parser</code></a></h3>
<!-- TODO
It is a hand-written recursive descent parser, which produces a sequence of events like "start node X", "finish node Y".
It works similarly to
[kotlin's parser](https://github.com/JetBrains/kotlin/blob/4d951de616b20feca92f3e9cc9679b2de9e65195/compiler/frontend/src/org/jetbrains/kotlin/parsing/KotlinParsing.java),
which is a good source of inspiration for dealing with syntax errors and incomplete input.
Original [libsyntax parser](https://github.com/Rust-lang/Rust/blob/6b99adeb11313197f409b4f7c4083c2ceca8a4fe/src/libsyntax/parse/parser.rs) is what we use for the definition of the Rust language.
`TreeSink` and `TokenSource` traits bridge the tree-agnostic parser from `grammar` with `rowan` trees.
-->
<p><strong>Architecture Invariant:</strong> the parser is independent of the particular tree structure and particular representation of the tokens.
It transforms one flat stream of events into another flat stream of events.
Token independence allows us to parse out both text-based source code and <code>tt</code>-based macro input.
Tree independence allows us to more easily vary the syntax tree implementation.
It should also unlock efficient light-parsing approaches.
For example, you can extract the set of names defined in a file (for typo correction) without building a syntax tree.</p>
<p><strong>Architecture Invariant:</strong> parsing never fails, the parser produces <code>(T, Vec&lt;Error&gt;)</code> rather than <code>Result&lt;T, Error&gt;</code>.</p>
<h3 id="cratessyntax"><a class="header" href="#cratessyntax"><code>crates/syntax</code></a></h3>
<p>WESL syntax tree structure and parser.</p>
<!-- https://github.com/gpuweb/gpuweb/tree/main/design -->
<p>See <a href="https://github.com/rust-lang/rfcs/pull/2256">RFC</a> and <a href="contributing/./syntax.html">./syntax.md</a> for some design notes.</p>
<ul>
<li><a href="https://github.com/rust-analyzer/rowan"><code>rowan</code></a> library is used for constructing syntax trees.</li>
<li><code>ast</code> provides a type safe API on top of the raw <code>rowan</code> tree.</li>
<li><code>ungrammar</code> description of the grammar, which is used to generate <code>syntax_kinds</code> and <code>ast</code> modules, using <code>cargo test -p xtask</code> command.</li>
</ul>
<p>Tests for wa_syntax are mostly data-driven.
<code>test_data/parser</code> contains subdirectories with a bunch of <code>.rs</code> (test vectors) and <code>.txt</code> files with corresponding syntax trees.
During testing, we check <code>.rs</code> against <code>.txt</code>.
If the <code>.txt</code> file is missing, it is created (this is how you update tests).
Additionally, running the xtask test suite with <code>cargo test -p xtask</code> will walk the grammar module and collect all <code>// test test_name</code> comments into files inside <code>test_data/parser/inline</code> directory.</p>
<p>To update test data, run with <code>UPDATE_EXPECT</code> variable:</p>
<pre><code class="language-bash">env UPDATE_EXPECT=1 cargo qt
</code></pre>
<p>After adding a new inline test you need to run <code>cargo test -p xtask</code> and also update the test data as described above.</p>
<p>Note <a href="https://github.com/wgsl-analyzer/wgsl-analyzer/tree/main/crates/syntax/src/lib.rs#L252"><code>api_walkthrough</code></a>
in particular: it shows off various methods of working with syntax tree.</p>
<!-- TODO put a wgsl-analyzer PR here -->
<p>See <a href="https://github.com/wgsl-analyzer/wgsl-analyzer/pull/TODO">#TODO</a> for an example PR which fixes a bug in the grammar.</p>
<p><strong>Architecture Invariant:</strong> <code>syntax</code> crate is completely independent from the rest of wgsl-analyzer.
It knows nothing about salsa or LSP.
This is important because it is possible to make useful tooling using only the syntax tree.
Without semantic information, you do not need to be able to <em>build</em> code, which makes the tooling more robust.
See also <a href="https://mlfbrown.com/paper.pdf">https://mlfbrown.com/paper.pdf</a>.
You can view the <code>syntax</code> crate as an entry point to wgsl-analyzer.
<code>syntax</code> crate is an <strong>API Boundary</strong>.</p>
<p><strong>Architecture Invariant:</strong> syntax tree is a value type.
The tree is fully determined by the contents of its syntax nodes, it does not need global context (like an interner) and does not store semantic info.
Using the tree as a store for semantic info is convenient in traditional compilers, but does not work nicely in the IDE.
Specifically, assists, and refactors require transforming syntax trees, and that becomes awkward if you need to do something with the semantic info.</p>
<p><strong>Architecture Invariant:</strong> syntax tree is built for a single file.
This is to enable parallel parsing of all files.</p>
<p><strong>Architecture Invariant:</strong>  Syntax trees are by design incomplete and do not enforce well-formedness.
If an AST method returns an <code>Option</code>, it <em>can</em> be <code>None</code> at runtime, even if this is forbidden by the grammar.</p>
<h3 id="cratesbase-db"><a class="header" href="#cratesbase-db"><code>crates/base-db</code></a></h3>
<p>We use the <a href="https://github.com/salsa-rs/salsa">salsa</a> crate for incremental and on-demand computation.
Roughly, you can think of salsa as a key-value store, but it can also compute derived values using specified functions.
The <code>base-db</code> crate provides basic infrastructure for interacting with salsa.
Crucially, it defines most of the "input" queries: facts supplied by the client of the analyzer.
Reading the docs of the <code>base_db::input</code> module should be useful: everything else is strictly derived from those inputs.</p>
<p><strong>Architecture Invariant:</strong> particularities of the build system are <em>not</em> the part of the ground state.
In particular, <code>base-db</code> knows nothing about cargo.
For example, <code>cfg</code> flags are a part of <code>base_db</code>, but <code>feature</code>s are not.
A <code>foo</code> feature is a Cargo-level concept, which is lowered by Cargo to <code>--cfg feature=foo</code> argument on the command line.
The <code>CrateGraph</code> structure is used to represent the dependencies between the crates abstractly.</p>
<p><strong>Architecture Invariant:</strong> <code>base-db</code> does not know about file system and file paths.
Files are represented with opaque <code>FileId</code>, there is no operation to get an <code>std::path::Path</code> out of the <code>FileId</code>.</p>
<h3 id="crateshir-def-crateshir_ty"><a class="header" href="#crateshir-def-crateshir_ty"><code>crates/hir-def</code>, <code>crates/hir_ty</code></a></h3>
<p>These crates are the <em>brain</em> of wgsl-analyzer.
This is the compiler part of the IDE.</p>
<p><code>hir-xxx</code> crates have a strong <a href="https://en.wikipedia.org/wiki/Entity_component_system">ECS</a> flavor, in that they work with raw ids and directly query the database.
There is very little abstraction here.
These crates integrate deeply with salsa and chalk.</p>
<p>Name resolution and type inference all happen here.
These crates also define various intermediate representations of the core.</p>
<p><code>ItemTree</code> condenses a single <code>SyntaxTree</code> into a "summary" data structure, which is stable over modifications to function bodies.</p>
<p><code>DefMap</code> contains the module tree of a crate and stores module scopes.</p>
<p><code>Body</code> stores information about expressions.</p>
<p><strong>Architecture Invariant:</strong> these crates are not, and will never be, an api boundary.</p>
<p><strong>Architecture Invariant:</strong> these crates explicitly care about being incremental.
The core invariant we maintain is "typing inside a function's body never invalidates global derived data".
i.e., if you change the body of <code>foo</code>, all facts about <code>bar</code> should remain intact.</p>
<p><strong>Architecture Invariant:</strong> hir exists only in context of particular crate instance with specific CFG flags.
The same syntax may produce several instances of HIR if the crate participates in the crate graph more than once.</p>
<h3 id="crateshir"><a class="header" href="#crateshir"><code>crates/hir</code></a></h3>
<p>The top-level <code>hir</code> crate is an <strong>API Boundary</strong>.
If you think about "using wgsl-analyzer as a library", <code>hir</code> crate is most likely the interface that you will be talking to.</p>
<p>It wraps ECS-style internal API into a more OO-flavored API (with an extra <code>db</code> argument for each call).</p>
<p><strong>Architecture Invariant:</strong> <code>hir</code> provides a static, fully resolved view of the code.
While internal <code>hir-*</code> crates <em>compute</em> things, <code>hir</code>, from the outside, looks like an inert data structure.</p>
<p><code>hir</code> also handles the delicate task of going from syntax to the corresponding <code>hir</code>.
Remember that the mapping here is one-to-many.
See <code>Semantics</code> type and <code>source_to_def</code> module.</p>
<p>Note in particular a curious recursive structure in <code>source_to_def</code>.
We first resolve the parent <em>syntax</em> node to the parent <em>hir</em> element.
Then we ask the <em>hir</em> parent what <em>syntax</em> children does it have.
Then we look for our node in the set of children.</p>
<p>This is the heart of many IDE features, like goto definition, which start with figuring out the hir node at the cursor.
This is some kind of (yet unnamed) uber-IDE pattern, as it is present in Roslyn and Kotlin as well.</p>
<!-- markdownlint-disable line-length -->
<h3 id="crateside-crateside-db-crateside-assists-crateside-completion-crateside-diagnostics-crateside-ssr"><a class="header" href="#crateside-crateside-db-crateside-assists-crateside-completion-crateside-diagnostics-crateside-ssr"><code>crates/ide</code>, <code>crates/ide-db</code>, <code>crates/ide-assists</code>, <code>crates/ide-completion</code>, <code>crates/ide-diagnostics</code>, <code>crates/ide-ssr</code></a></h3>
<!-- markdownlint-restore line-length -->
<p>The <code>ide</code> crate builds on top of <code>hir</code> semantic model to provide high-level IDE features like completion or goto definition.
It is an <strong>API Boundary</strong>.
If you want to use IDE parts of <code>wgsl-analyzer</code> via LSP, custom flatbuffers-based protocol or just as a library in your text editor, this is the right API.</p>
<p><strong>Architecture Invariant:</strong> <code>ide</code> crate's API is build out of POD types with public fields.
The API uses editor's terminology, it talks about offsets and string labels rather than in terms of definitions or types.
It is effectively the view in MVC and viewmodel in <a href="https://en.wikipedia.org/wiki/Model%E2%80%93view%E2%80%93viewmodel">MVVM</a>.
All arguments and return types are conceptually serializable.
In particular, syntax trees and hir types are generally absent from the API (but are used heavily in the implementation).
Shout outs to LSP developers for popularizing the idea that "UI" is a good place to draw a boundary at.</p>
<p><code>ide</code> is also the first crate which has the notion of change over time.
<code>AnalysisHost</code> is a state to which you can transactionally <code>apply_change</code>.
<code>Analysis</code> is an immutable snapshot of the state.</p>
<p>Internally, <code>ide</code> is split across several crates.
<code>ide-assists</code>, <code>ide-completion</code>, <code>ide-diagnostics</code> and <code>ide-ssr</code> implement large isolated features.
<code>ide-db</code> implements common IDE functionality (notably, reference search is implemented here).
The <code>ide</code> contains a public API, as well as implementation for a plethora of smaller features.</p>
<p><strong>Architecture Invariant:</strong> <code>ide</code> crate strives to provide a <em>perfect</em> API.
Although at the moment it has only one consumer, the LSP server, LSP <em>does not</em> influence its API design.
Instead, we keep in mind a hypothetical <em>ideal</em> client - an IDE tailored specifically for WGSL and WESL, every nook and cranny of which is packed with language-specific goodies.</p>
<h3 id="crateswgsl-analyzer"><a class="header" href="#crateswgsl-analyzer"><code>crates/wgsl-analyzer</code></a></h3>
<p>This crate defines the <code>wgsl-analyzer</code> binary, so it is the <strong>entry point</strong>.
It implements the language server.</p>
<p><strong>Architecture Invariant:</strong> <code>wgsl-analyzer</code> is the only crate that knows about LSP and JSON serialization.
If you want to expose a data structure <code>X</code> from ide to LSP, do not make it serializable.
Instead, create a serializable counterpart in <code>wgsl-analyzer</code> crate and manually convert between the two.</p>
<p><code>GlobalState</code> is the state of the server.
The <code>main_loop</code> defines the server event loop, which accepts requests and sends responses.
Requests that modify the state or might block a user's typing are handled on the main thread.
All other requests are processed in background.</p>
<p><strong>Architecture Invariant:</strong> the server is stateless, a-la HTTP.
Sometimes state needs to be preserved between requests.
For example, "what is the <code>edit</code> for the fifth completion item of the last completion edit?".
For this, the second request should include enough info to re-create the context from scratch.
This generally means including all the parameters of the original request.</p>
<p><code>reload</code> module contains the code that handles configuration and Cargo.toml changes.
This is a tricky business.</p>
<p><strong>Architecture Invariant:</strong> <code>wgsl-analyzer</code> should be partially available even when the build is broken.
Reloading process should not prevent IDE features from working.</p>
<h3 id="cratestoolchain-cratesproject-model-cratesflycheck"><a class="header" href="#cratestoolchain-cratesproject-model-cratesflycheck"><code>crates/toolchain</code>, <code>crates/project-model</code>, <code>crates/flycheck</code></a></h3>
<p>These crates deal with invoking <code>cargo</code> to learn about project structure and get compiler errors for the "check on save" feature.</p>
<p>They use <code>crates/paths</code> heavily instead of <code>std::path</code>.
A single <code>wgsl-analyzer</code> process can serve many projects, so it is important that the server's current working directory does not leak.</p>
<h3 id="cratescfg"><a class="header" href="#cratescfg"><code>crates/cfg</code></a></h3>
<p>This crate is responsible for parsing, evaluation, and general definition of <code>cfg</code> attributes.</p>
<h3 id="cratesvfs-cratesvfs-notify-cratespaths"><a class="header" href="#cratesvfs-cratesvfs-notify-cratespaths"><code>crates/vfs</code>, <code>crates/vfs-notify</code>, <code>crates/paths</code></a></h3>
<p>These crates implement a virtual file system.
They provide consistent snapshots of the underlying file system and insulate messy OS paths.</p>
<p><strong>Architecture Invariant:</strong> vfs does not assume a single unified file system.
i.e., a single <code>wgsl-analyzer</code> process can act as a remote server for two different machines, where the same <code>/tmp/foo.rs</code> path points to different files.
For this reason, all path APIs generally take some existing path as a "file system witness".</p>
<h3 id="cratesstdx"><a class="header" href="#cratesstdx"><code>crates/stdx</code></a></h3>
<p>This crate contains various non-wgsl-analyzer specific utils, which could have been in std, as well
as copies of unstable std items we would like to make use of already, like <code>std::str::split_once</code>.</p>
<h3 id="cratesprofile"><a class="header" href="#cratesprofile"><code>crates/profile</code></a></h3>
<p>This crate contains utilities for CPU and memory profiling.</p>
<!--
TODO: See https://github.com/wgsl-analyzer/wgsl-analyzer/issues/361
### `crates/intern`

This crate contains infrastructure for globally interning things via `Arc`.
-->
<h3 id="cratesspan"><a class="header" href="#cratesspan"><code>crates/span</code></a></h3>
<p>This crate exposes types and functions related to <code>wgsl-analyzer</code>'s span for macros.</p>
<p>A span is effectively a text range relative to some item in a file with a given <code>SyntaxContext</code> (hygiene).</p>
<h2 id="cross-cutting-concerns"><a class="header" href="#cross-cutting-concerns">Cross-Cutting Concerns</a></h2>
<p>This sections talks about the things which are everywhere and nowhere in particular.</p>
<h3 id="stability-guarantees"><a class="header" href="#stability-guarantees">Stability Guarantees</a></h3>
<p>One of the reasons <code>wgsl-analyzer</code> moves relatively fast is that we do not introduce new stability guarantees.
Instead, as much as possible we leverage existing ones.</p>
<p>Examples:</p>
<ul>
<li>The <code>ide</code> API of <code>wgsl-analyzer</code> is explicitly unstable, but the LSP interface is stable, and here we just implement a stable API managed by someone else.</li>
<li><a href="https://www.w3.org/TR/WGSL">WGSL spec</a> is almost stable, and it is the primary input to <code>wgsl-analyzer</code>.</li>
</ul>
<p>Exceptions:</p>
<ul>
<li>We ship some LSP extensions, and we try to keep those somewhat stable.
Here, we need to work with a finite set of editor maintainers, so not providing rock-solid guarantees works.</li>
</ul>
<h3 id="code-generation"><a class="header" href="#code-generation">Code generation</a></h3>
<p>Some components in this repository are generated through automatic processes.
Generated code is updated automatically on <code>cargo test</code>.
Generated code is generally committed to the git repository.</p>
<p>In particular, we generate:</p>
<ul>
<li>
<p>Various sections of the manual:</p>
<ul>
<li>features</li>
<li>assists</li>
<li>config</li>
</ul>
</li>
<li>
<p>Documentation tests for assists</p>
</li>
</ul>
<p>See the <code>xtask\src\codegen\assists_doc_tests.rs</code> module for details.</p>
<h3 id="cancellation"><a class="header" href="#cancellation">Cancellation</a></h3>
<p>Suppose that the IDE is in the process of computing syntax highlighting when the user types <code>foo</code>.
What should happen?
<code>wgsl-analyzer</code>s answer is that the highlighting process should be cancelled - its results are now stale, and it also blocks modification of the inputs.</p>
<p>The salsa database maintains a global revision counter.
When applying a change, salsa bumps this counter and waits until all other threads using salsa finish.
If a thread does salsa-based computation and notices that the counter is incremented, it panics with a special value (see <code>Canceled::throw</code>).
That is, <code>wgsl-analyzer</code> requires unwinding.</p>
<p><code>ide</code> is the boundary where the panic is caught and transformed into a <code>Result&lt;T, Cancelled&gt;</code>.</p>
<h3 id="testing"><a class="header" href="#testing">Testing</a></h3>
<p>wgsl-analyzer has three interesting <a href="https://www.tedinski.com/2018/04/10/making-tests-a-positive-influence-on-design.html">system boundaries</a> to concentrate tests on.</p>
<p>The outermost boundary is the <code>wgsl-analyzer</code> crate, which defines an LSP interface in terms of stdio.
We do integration testing of this component, by feeding it with a stream of LSP requests and checking responses.
These tests are known as "heavy", because they interact with Cargo and read real files from disk.
For this reason, we try to avoid writing too many tests on this boundary: in a statically typed language, it is hard to make an error in the protocol itself if messages are themselves typed.
Heavy tests are only run when <code>RUN_SLOW_TESTS</code> env var is set.</p>
<p>The middle, and most important, boundary is <code>ide</code>.</p>
<!-- TODO check this -->
<p>Unlike <code>wgsl-analyzer</code>, which exposes API, <code>ide</code> uses WGSL API and is intended for use by various tools.
A typical test creates an <code>AnalysisHost</code>, calls some <code>Analysis</code> functions and compares the results against expectation.</p>
<p>The innermost and most elaborate boundary is <code>hir</code>.
It has a much richer vocabulary of types than <code>ide</code>, but the basic testing setup is the same: we create a database, run some queries, assert result.</p>
<p>For comparisons, we use the <code>expect</code> crate for snapshot testing.</p>
<p>To test various analysis corner cases and avoid forgetting about old tests, we use so-called marks.
See the <a href="https://docs.rs/cov-mark/latest/cov_mark">cov_mark</a> crate documentation for more.</p>
<p><strong>Architecture Invariant:</strong> <code>wgsl-analyzer</code> tests do not use <code>libcore</code> or <code>libstd</code>.
All required library code must be a part of the tests.
This ensures fast test execution.</p>
<p><strong>Architecture Invariant:</strong> tests are data driven and do not test the API.
Tests which directly call various API functions are a liability, because they make refactoring the API significantly more complicated.
Most of the tests look like this:</p>
<pre><code class="language-rust">#[track_caller]
fn check(input: &amp;str, expect: expect_test::Expect) {
    // The single place that actually exercises a particular API
}

#[test]
fn foo() {
    check("foo", expect![["bar"]]);
}

#[test]
fn spam() {
    check("spam", expect![["eggs"]]);
}
// ...and a hundred more tests that do not care about the specific API at all.</code></pre>
<p>To specify input data, we use a single string literal in a special format, which can describe a set of WGSL files.
See the <code>Fixture</code> its module for fixture examples and documentation.</p>
<p><strong>Architecture Invariant:</strong> all code invariants are tested by <code>#[test]</code> tests.
There is no additional checks in CI, formatting, and tidy tests are run with <code>cargo test</code>.</p>
<p><strong>Architecture Invariant:</strong> tests do not depend on any kind of external resources, they are perfectly reproducible.</p>
<h3 id="performance-testing"><a class="header" href="#performance-testing">Performance Testing</a></h3>
<p>TBA, take a look at the <code>metrics</code> xtask and <code>#[test] fn benchmark_xxx()</code> functions.</p>
<h3 id="error-handling"><a class="header" href="#error-handling">Error Handling</a></h3>
<p><strong>Architecture Invariant:</strong> core parts of <code>wgsl-analyzer</code> (<code>ide</code>/<code>hir</code>) do not interact with the outside world and thus cannot fail.
Only parts touching LSP are allowed to do IO.</p>
<p>Internals of <code>wgsl-analyzer</code> need to deal with broken code, but this is not an error condition.
wgsl-analyzer is robust: various analysis compute <code>(T, Vec&lt;Error&gt;)</code> rather than <code>Result&lt;T, Error&gt;</code>.</p>
<p><code>wgsl-analyzer</code> is a complex, long-running process.
It will always have bugs and panics;
to mitigate this, a panic in an isolated feature should not bring down the whole process.
Each LSP-request is protected by a <code>catch_unwind</code>.
We use <code>always</code> and <code>never</code> macros instead of <code>assert</code> to gracefully recover from impossible conditions.</p>
<h3 id="observability"><a class="header" href="#observability">Observability</a></h3>
<p><code>wgsl-analyzer</code> is a long-running process, so it is important to understand what happens inside.
We have several instruments for that.</p>
<p>The event loop that runs <code>wgsl-analyzer</code> is very explicit.
Rather than spawning futures or scheduling callbacks (open), the event loop accepts an <code>enum</code> of possible events (closed).
It is easy to see all the things that trigger <code>wgsl-analyzer</code> processing together with their performance.</p>
<p><code>wgsl-analyzer</code> includes a simple hierarchical profiler (<code>hprof</code>).
It is enabled with <code>WA_PROFILE='*&gt;50'</code> env var (log all (<code>*</code>) actions which take more than <code>50</code> ms) and produces output like:</p>
<!-- TODO replace with real example -->
<pre><code class="language-text">85ms - handle_completion
    68ms - import_on_the_fly
        67ms - import_assets::search_for_relative_paths
             0ms - crate_def_map:wait (804 calls)
             0ms - find_path (16 calls)
             2ms - find_similar_imports (1 calls)
             0ms - generic_params_query (334 calls)
            59ms - trait_solve_query (186 calls)
         0ms - Semantics::analyze_impl (1 calls)
         1ms - render_resolution (8 calls)
     0ms - Semantics::analyze_impl (5 calls)
</code></pre>
<p>This is cheap enough to enable in production.</p>
<p>Similarly, we save live object counting (<code>WA_COUNT=1</code>).
It is not cheap enough to enable in production, and this is a bug which should be fixed.</p>
<h3 id="configurability"><a class="header" href="#configurability">Configurability</a></h3>
<p><code>wgsl-analyzer</code> strives to be as configurable as possible while offering reasonable defaults where no configuration exists yet.
The rule of thumb is to enable most features by default unless they are buggy or degrade performance too much.
There will always be features that some people find more annoying than helpful, so giving the users the ability to tweak or disable these is a big part of offering a good user experience.
Enabling them by default is a matter of discoverability, as many users do not know about some features even though they are presented in the manual.
Mind the code-architecture gap: at the moment, we are using fewer feature flags than we really should.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="debugging-vs-code-plugin-and-the-language-server"><a class="header" href="#debugging-vs-code-plugin-and-the-language-server">Debugging VS Code plugin and the language server</a></h1>
<h2 id="prerequisites"><a class="header" href="#prerequisites">Prerequisites</a></h2>
<ul>
<li>Install <a href="https://lldb.llvm.org">LLDB</a> and the <a href="https://marketplace.visualstudio.com/items?itemName=vadimcn.vscode-lldb">LLDB Extension</a>.</li>
<li>Open the root folder in VS Code. Here you can access the preconfigured debug setups.</li>
</ul>
<p><img src="https://user-images.githubusercontent.com/36276403/74611090-92ec5380-5101-11ea-8a41-598f51f3f3e3.png" alt="Debug options view" /></p>
<ul>
<li>
<p>Install all TypeScript dependencies</p>
<pre><code class="language-bash">cd editors/code
npm ci
</code></pre>
</li>
</ul>
<h2 id="common-knowledge"><a class="header" href="#common-knowledge">Common knowledge</a></h2>
<ul>
<li>All debug configurations open a new <code>[Extension Development Host]</code> VS Code instance
where <strong>only</strong> the <code>wgsl-analyzer</code> extension being debugged is enabled.</li>
<li>To activate the extension you need to open any WESL project's folder in <code>[Extension Development Host]</code>.</li>
</ul>
<h2 id="debug-typescript-vs-code-extension"><a class="header" href="#debug-typescript-vs-code-extension">Debug TypeScript VS Code extension</a></h2>
<ul>
<li><code>Run Installed Extension</code> - runs the extension with the globally installed <code>wgsl-analyzer</code> binary.</li>
<li><code>Run Extension (Debug Build)</code> - runs extension with the locally built LSP server (<code>target/debug/wgsl-analyzer</code>).</li>
</ul>
<p>TypeScript debugging is configured to watch your source edits and recompile.
To apply changes to an already running debug process, press <kbd>Ctrl</kbd>+<kbd>Shift</kbd>+<kbd>P</kbd>
and run the following command in your <code>[Extension Development Host]</code></p>
<pre><code class="language-text">&gt; Developer: Reload Window
</code></pre>
<h2 id="debugging-the-lsp-server"><a class="header" href="#debugging-the-lsp-server">Debugging the LSP server</a></h2>
<ul>
<li>
<p>When attaching a debugger to an already running <code>wgsl-analyzer</code> server on Linux,
you might need to enable <code>ptrace</code> for unrelated processes by running:</p>
<pre><code class="language-bash">echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope
</code></pre>
</li>
<li>
<p>By default, the LSP server is built without debug information.
To enable it, you will need to change <code>Cargo.toml</code>:</p>
<pre><code class="language-toml">  [profile.dev]
  debug = 2
</code></pre>
</li>
</ul>
<ol>
<li>Select <code>Run Extension (Debug Build)</code> to run your locally built <code>target/debug/wgsl-analyzer</code>.</li>
<li>In the original VS Code window once again select the <code>Attach To Server</code> debug configuration.</li>
<li>A list of running processes should appear. Select the <code>wgsl-analyzer</code> from this repo.</li>
<li>Navigate to <code>crates/wgsl-analyzer/src/main_loop.rs</code> and add a breakpoint to the <code>on_request</code> function.</li>
<li>Go back to the <code>[Extension Development Host]</code> instance and hover over a Rust variable and your breakpoint should hit.</li>
</ol>
<p>If you need to debug the server from the very beginning, including its initialization
code, you can use the <code>--wait-dbg</code> command line argument or <code>WA_WAIT_DBG</code> environment variable.
The server will spin at the beginning of the <code>try_main</code> function (see <code>crates\wgsl-analyzer\src\bin\main.rs</code>)</p>
<pre><code class="language-rust">let mut d = 4;
while d == 4 { // set a breakpoint here and change the value
	d = 4;
}</code></pre>
<p>However for this to work, you will need to enable debug_assertions in your build</p>
<pre><code class="language-bash">RUSTFLAGS='--cfg debug_assertions' cargo build --release
</code></pre>
<h2 id="demo"><a class="header" href="#demo">Demo</a></h2>
<ul>
<li><a href="https://www.youtube.com/watch?v=T-hvpK6s4wM">Debugging TypeScript VScode extension</a>.</li>
<li><a href="https://www.youtube.com/watch?v=EaNb5rg4E0M">Debugging the LSP server (rust-analyzer, same advice applies)</a>.</li>
</ul>
<h2 id="troubleshooting-1"><a class="header" href="#troubleshooting-1">Troubleshooting</a></h2>
<h3 id="cannot-find-the-wgsl-analyzer-process"><a class="header" href="#cannot-find-the-wgsl-analyzer-process">Cannot find the <code>wgsl-analyzer</code> process</a></h3>
<p>It could be a case of just jumping the gun.</p>
<p>Make sure you open a WGSL or WESL file in the <code>[Extension Development Host]</code> and try again.</p>
<h3 id="cannot-connect-to-wgsl-analyzer"><a class="header" href="#cannot-connect-to-wgsl-analyzer">Cannot connect to <code>wgsl-analyzer</code></a></h3>
<p>Make sure you have run <code>echo 0 | sudo tee /proc/sys/kernel/yama/ptrace_scope</code>.</p>
<p>By default this should reset back to 1 every time you log in.</p>
<h3 id="breakpoints-are-never-being-hit"><a class="header" href="#breakpoints-are-never-being-hit">Breakpoints are never being hit</a></h3>
<p>Check your version of <code>lldb</code>.
If it is version 6 and lower, use the <code>classic</code> adapter type.
It is <code>lldb.adapterType</code> in settings file.</p>
<p>If you are running <code>lldb</code> version 7, change the lldb adapter type to <code>bundled</code> or <code>native</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="guide-to-wgsl-analyzer"><a class="header" href="#guide-to-wgsl-analyzer">Guide to wgsl-analyzer</a></h1>
<!-- markdownlint-disable MD013 -->
<h2 id="about-the-guide"><a class="header" href="#about-the-guide">About the guide</a></h2>
<p>This guide describes the current state of <code>wgsl-analyzer</code> as of the 2025-xx-xx release (git tag <a href="https://github.com/wgsl-analyzer/wgsl-analyzer/tree/2025-xx-xx">2025-xx-xx</a>).
Its purpose is to document various problems and architectural solutions related to the problem of building an IDE-first compiler for Rust.</p>
<ul>
<li><a href="contributing/guide.html#the-big-picture">The big picture</a></li>
<li><a href="contributing/guide.html#ide-api">IDE API</a></li>
<li><a href="contributing/guide.html#inputs">Inputs</a></li>
<li><a href="contributing/guide.html#source-roots-aka-filesystems-are-horrible">Source roots (a.k.a. "Filesystems are horrible")</a></li>
<li><a href="contributing/guide.html#language-server-protocol">Language Server Protocol</a></li>
<li><a href="contributing/guide.html#salsa">Salsa</a></li>
<li><a href="contributing/guide.html#salsa-input-queries">Salsa Input Queries</a></li>
<li><a href="contributing/guide.html#from-text-to-semantic-model">From text to semantic model</a></li>
<li><a href="contributing/guide.html#syntax-trees">Syntax trees</a></li>
<li><a href="contributing/guide.html#building-a-module-tree">Building a Module Tree</a></li>
<li><a href="contributing/guide.html#location-interner-pattern">Location Interner pattern</a></li>
<li><a href="contributing/guide.html#macros-and-recursive-locations">Macros and recursive locations</a></li>
<li><a href="contributing/guide.html#name-resolution">Name resolution</a></li>
<li><a href="contributing/guide.html#source-map-pattern">Source Map pattern</a></li>
<li><a href="contributing/guide.html#type-inference">Type inference</a></li>
<li><a href="contributing/guide.html#tying-it-all-together-completion">Tying it all together: completion</a></li>
</ul>
<!-- TODO nothing here is correct -->
<h2 id="the-big-picture"><a class="header" href="#the-big-picture">The big picture</a></h2>
<p>On the highest possible level, rust-analyzer is a stateful component.
A client may apply changes to the analyzer (new contents of <code>foo.rs</code> file is "<code>fn main() {}</code>") and it may ask semantic questions about the current state (what is the definition of the identifier with offset 92 in file <code>bar.rs</code>?).
Two important properties hold:</p>
<ul>
<li>
<p>Analyzer does not do any I/O.
It starts in an empty state and all input data is provided via <code>apply_change</code> API.</p>
</li>
<li>
<p>Only queries about the current state are supported.
One can, of course, simulate undo and redo by keeping a log of changes and inverse changes respectively.</p>
</li>
</ul>
<h2 id="ide-api"><a class="header" href="#ide-api">IDE API</a></h2>
<p>To see the bigger picture of how the IDE features work, examine the <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/ide/src/lib.rs#L161-L213"><code>AnalysisHost</code></a> and <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/ide/src/lib.rs#L220-L761"><code>Analysis</code></a> pair of types.
<code>AnalysisHost</code> has three methods:</p>
<ul>
<li><code>default()</code> for creating an empty analysis instance</li>
<li><code>apply_change(&amp;mut self)</code> to make changes (this is how you get from an empty state to something interesting)</li>
<li><code>analysis(&amp;self)</code> to get an instance of <code>Analysis</code></li>
</ul>
<p><code>Analysis</code> has a ton of methods for IDEs, like <code>goto_definition</code>, or <code>completions</code>.
Both inputs and outputs of <code>Analysis</code>' methods are formulated in terms of files and offsets, and <strong>not</strong> in terms of Rust concepts like structs, traits, etc.
The "typed" API with Rust-specific types is slightly lower in the stack, we will talk about it later.</p>
<p>The reason for this separation of <code>Analysis</code> and <code>AnalysisHost</code> is that we want to apply changes "uniquely", but we might also want to fork an <code>Analysis</code> and send it to another thread for background processing.
That is, there is only a single <code>AnalysisHost</code>, but there may be several (equivalent) <code>Analysis</code>.</p>
<p>Note that all of the <code>Analysis</code> API return <code>Cancellable&lt;T&gt;</code>.
This is required to be responsive in an IDE setting.
Sometimes a long-running query is being computed and the user types something in the editor and asks for completion.
In this case, we cancel the long-running computation (so it returns <code>Err(Cancelled)</code>), apply the change and execute the request for completion.
We never use stale data to answer requests.
Under the cover, <code>AnalysisHost</code> "remembers" all outstanding <code>Analysis</code> instances.
The <code>AnalysisHost::apply_change</code> method cancels all <code>Analysis</code>es, blocks until all of them are <code>Dropped</code> and then applies changes in-place.
This may be familiar to Rustaceans who use read-write locks for interior mutability.</p>
<p>Next, the inputs to the <code>Analysis</code> are discussed in detail.</p>
<h2 id="inputs"><a class="header" href="#inputs">Inputs</a></h2>
<p>rust-analyzer never does any I/O itself.
All inputs get passed explicitly via the <code>AnalysisHost::apply_change</code> method, which accepts a single argument, a <code>Change</code>.
<a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/hir-expand/src/change.rs#L10-L42"><code>Change</code></a> is a wrapper for <code>FileChange</code> that adds proc-macro knowledge.
<a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/base-db/src/change.rs#L14-L78"><code>FileChange</code></a> is a builder for a single change "transaction," so it suffices to study its methods to understand all the input data.</p>
<p>The <code>change_file</code> method controls the set of the input files, where each file has an integer id (<code>FileId</code>, picked by the client) and text (<code>Option&lt;Arc&lt;str&gt;&gt;</code>).
Paths are tricky; they will be explained below, in the source roots section, together with the <code>set_roots</code> method.
The "source root" <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/base-db/src/input.rs#L38"><code>is_library</code></a> flag along with the concept of <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/base-db/src/change.rs#L80-L86"><code>durability</code></a> allows us to add a group of files that are assumed to rarely change.
It is mostly an optimization and does not change the fundamental picture.</p>
<p>The <code>set_crate_graph</code> method allows us to control how the input files are partitioned into compilation units -- crates.
It also controls (in theory, not implemented yet) <code>cfg</code> flags.
<code>CrateGraph</code> is a directed acyclic graph of crates.
Each crate has a root <code>FileId</code>, a set of active <code>cfg</code> flags, and a set of dependencies.
Each dependency is a pair of a crate and a name.
It is possible to have two crates with the same root <code>FileId</code> but different <code>cfg</code>-flags/dependencies.
This model is lower than Cargo's model of packages: each Cargo package consists of several targets, each of which is a separate crate (or several crates, if you try different feature combinations).</p>
<p>Procedural macros are inputs as well, roughly modeled as a crate with a bunch of additional black box <code>dyn Fn(TokenStream) -&gt; TokenStream</code> functions.</p>
<p>Next, the process of building an LSP server on top of <code>Analysis</code> is discussed.
However, before that, it is important to address the issue with paths.</p>
<h2 id="source-roots-aka-filesystems-are-horrible"><a class="header" href="#source-roots-aka-filesystems-are-horrible">Source roots (a.k.a. "Filesystems are horrible")</a></h2>
<p>This is a non-essential section, feel free to skip.</p>
<p>The previous section said that the filesystem path is an attribute of a file, but this is not the whole truth.
Making it an absolute <code>PathBuf</code> will be bad for several reasons.
First, filesystems are full of (platform-dependent) edge cases:</p>
<ul>
<li>It is hard (requires a syscall) to decide if two paths are equivalent.</li>
<li>Some filesystems are case-sensitive (e.g. macOS).</li>
<li>Paths are not necessarily UTF-8.</li>
<li>Symlinks can form cycles.</li>
</ul>
<p>Second, this might hurt the reproducibility and hermeticity of builds.
In theory, moving a project from <code>/foo/bar/my-project</code> to <code>/spam/eggs/my-project</code> should not change a bit in the output.
However, if the absolute path is a part of the input, it is at least in theory observable, and <em>could</em> affect the output.</p>
<p>Yet another problem is that we really <em>really</em> want to avoid doing I/O, but with Rust the set of "input" files is not necessarily known up-front.
In theory, you can have <code>#[path="/dev/random"] mod foo;</code>.</p>
<p>To solve (or explicitly refuse to solve) these problems rust-analyzer uses the concept of a "source root".
Roughly speaking, source roots are the contents of a directory on a file system, like <code>/home/matklad/projects/rustraytracer/**.rs</code>.</p>
<p>More precisely, all files (<code>FileId</code>s) are partitioned into disjoint <code>SourceRoot</code>s.
Each file has a relative UTF-8 path within the <code>SourceRoot</code>.
<code>SourceRoot</code> has an identity (integer ID).
Crucially, the root path of the source root itself is unknown to the analyzer: A client is supposed to maintain a mapping between <code>SourceRoot</code> IDs (which are assigned by the client) and actual <code>PathBuf</code>s.
<code>SourceRoot</code>s give a sane tree model of the file system to the analyzer.</p>
<p>Note that <code>mod</code>, <code>#[path]</code> and <code>include!()</code> can only reference files from the same source root.
It is of course possible to explicitly add extra files to the source root, even <code>/dev/random</code>.</p>
<h2 id="language-server-protocol"><a class="header" href="#language-server-protocol">Language Server Protocol</a></h2>
<p>The <code>Analysis</code> API is exposed via the JSON RPC-based language server protocol.
The hard part here is managing changes (which can come either from the file system or from the editor) and concurrency (we want to spawn background jobs for things like syntax highlighting).
We use the event loop pattern to manage the zoo, and the loop is the <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/rust-analyzer/src/main_loop.rs#L114-L140"><code>GlobalState::run</code></a> function initiated by <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/rust-analyzer/src/main_loop.rs#L31-L54"><code>main_loop</code></a> after <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/rust-analyzer/src/global_state.rs#L148-L215"><code>GlobalState::new</code></a> does a one-time initialization and tearing down of the resources.</p>
<p>A typical analyzer session involves several steps.</p>
<p>First, we need to figure out what to analyze.
To do this, we run <code>cargo metadata</code> to learn about Cargo packages for the current workspace and dependencies, and we run <code>rustc --print sysroot</code> and scan the "sysroot" (the directory containing the current Rust toolchain's files) to learn about crates like <code>std</code>.
This happens in the <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/rust-analyzer/src/reload.rs#L186-L257"><code>GlobalState::fetch_workspaces</code></a> method.
We load this configuration at the start of the server in <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/rust-analyzer/src/global_state.rs#L148-L215"><code>GlobalState::new</code></a>, but it is also triggered by workspace change events and requests to reload the workspace from the client.</p>
<p>The <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/project-model/src/workspace.rs#L57-L100"><code>ProjectModel</code></a> we get after this step is very Cargo and sysroot specific, it needs to be lowered to get the input in the form of <code>Change</code>.
This happens in the <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/rust-analyzer/src/global_state.rs#L217-L356"><code>GlobalState::process_changes</code></a> method.
Specifically:</p>
<ul>
<li>Create <code>SourceRoot</code>s for each Cargo package(s) and sysroot.</li>
<li>Schedule a filesystem scan of the roots.</li>
<li>Create an analyzer's <code>Crate</code> for each Cargo <strong>target</strong> and sysroot crate.</li>
<li>Set up dependencies between the crates.</li>
</ul>
<p>The results of the scan (which may take a while) will be processed in the body of the main loop, just like any other change.
Here, the following are handled:</p>
<ul>
<li><a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/rust-analyzer/src/main_loop.rs#L273">File system changes</a></li>
<li><a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/rust-analyzer/src/main_loop.rs#L801-L803">Changes from the editor</a></li>
</ul>
<p>After a single loop's turn, we group the changes into one <code>Change</code> and <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/rust-analyzer/src/global_state.rs#L333">apply</a> it.
This always happens on the main thread and blocks the loop.</p>
<p>To handle requests, like <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/rust-analyzer/src/main_loop.rs#L767">"goto definition"</a>, we create an instance of the <code>Analysis</code> and <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/rust-analyzer/src/dispatch.rs#L138"><code>schedule</code></a> the task (which consumes <code>Analysis</code>) on the thread pool.
<a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/rust-analyzer/src/handlers/request.rs#L610-L623">The task</a> calls the corresponding <code>Analysis</code> method, while massaging the types into the LSP representation.
Keep in mind that if we are executing "goto definition" on the thread pool and a new change comes in, the task will be canceled as soon as the main loop calls <code>apply_change</code> on the <code>AnalysisHost</code>.</p>
<p>This concludes the overview of the analyzer's programming <em>interface</em>.
Next, explore the implementation details.</p>
<h2 id="salsa"><a class="header" href="#salsa">Salsa</a></h2>
<p>The most straightforward way to implement an "apply change, get analysis, repeat" API would be to maintain the input state and to compute all possible analysis information from scratch after every change.
This works, but scales poorly with the size of the project.
To make this fast, we need to take advantage of the fact that most of the changes are small, and that analysis results are unlikely to change significantly between invocations.</p>
<p>To do this we use <a href="https://github.com/salsa-rs/salsa">salsa</a>: a framework for incremental on-demand computation.
You can skip the rest of the section if you are familiar with <code>rustc</code>'s red-green algorithm (which is used for incremental compilation).</p>
<p>It is better to refer to salsa's docs to learn about it.
Here is a small excerpt:</p>
<p>The key idea of salsa is that you define your program as a set of queries.
Every query is used like a function <code>K -&gt; V</code> that maps from some key of type <code>K</code> to a value of type <code>V</code>.
Queries come in two basic varieties:</p>
<ul>
<li>
<p><strong>Inputs</strong>: the base inputs to your system.
You can change these whenever you like.</p>
</li>
<li>
<p><strong>Functions</strong>: pure functions (no side effects) that transform your inputs into other values.
The results of queries are memoized to avoid recomputing them a lot.
When you make changes to the inputs, we will figure out (fairly intelligently) when we can reuse these memoized values and when we have to recompute them.</p>
</li>
</ul>
<p>For further discussion, it's important to understand one bit of "fairly intelligently".
Suppose we have two functions, <code>f1</code> and <code>f2</code>, and one input, <code>z</code>.
We call <code>f1(X)</code> which in turn calls <code>f2(Y)</code> which inspects <code>i(Z)</code>.
<code>i(Z)</code> returns some value <code>V1</code>, <code>f2</code> uses that and returns <code>R1</code>, <code>f1</code> uses that and returns <code>O</code>.
Now, suppose <code>i</code> at <code>Z</code> is changed to <code>V2</code> from <code>V1</code>.
Try to compute <code>f1(X)</code> again.
Because <code>f1(X)</code> (transitively) depends on <code>i(Z)</code>, we cannot just reuse its value as is.
However, if <code>f2(Y)</code> is <em>still</em> equal to <code>R1</code> (despite <code>i</code>'s change), we, in fact, <em>can</em> reuse <code>O</code> as the result of <code>f1(X)</code>.
And that is how salsa works: it recomputes results in <em>reverse</em> order, starting from inputs and progressing towards outputs, stopping as soon as it sees an intermediate value that has not changed.
If this sounds confusing to you, do not worry: it is confusing.
This illustration by @killercup might help:</p>
<p><img src="https://user-images.githubusercontent.com/1711539/51460907-c5484780-1d6d-11e9-9cd2-d6f62bd746e0.png" alt="step 1" /></p>
<p><img src="https://user-images.githubusercontent.com/1711539/51460915-c9746500-1d6d-11e9-9a77-27d33a0c51b5.png" alt="step 2" /></p>
<p><img src="https://user-images.githubusercontent.com/1711539/51460920-cda08280-1d6d-11e9-8d96-a782aa57a4d4.png" alt="step 3" /></p>
<p><img src="https://user-images.githubusercontent.com/1711539/51460927-d1340980-1d6d-11e9-851e-13c149d5c406.png" alt="step 4" /></p>
<h2 id="salsa-input-queries"><a class="header" href="#salsa-input-queries">Salsa Input Queries</a></h2>
<p>All analyzer information is stored in a salsa database.
<code>Analysis</code> and <code>AnalysisHost</code> types are essentially newtype wrappers for <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/ide-db/src/lib.rs#L69-L324"><code>RootDatabase</code></a> -- a salsa database.</p>
<p>Salsa input queries are defined in <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/base-db/src/lib.rs#L58-L65"><code>SourceDatabase</code></a> and <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/base-db/src/lib.rs#L76-L88"><code>SourceDatabaseExt</code></a> (which are a part of <code>RootDatabase</code>).
They closely mirror the familiar <code>Change</code> structure: indeed, what <code>apply_change</code> does is it sets the values of input queries.</p>
<h2 id="from-text-to-semantic-model"><a class="header" href="#from-text-to-semantic-model">From text to semantic model</a></h2>
<p>The bulk of the rust-analyzer is transforming input text into a semantic model of Rust code: a web of entities like modules, structs, functions, and traits.</p>
<p>An important fact to realize is that (unlike most other languages like C# or Java) there is not a one-to-one mapping between the source code and the semantic model.
A single function definition in the source code might result in several semantic functions: for example, the same source file might get included as a module in several crates or a single crate might be present in the compilation DAG several times, with different sets of <code>cfg</code>s enabled.
The IDE-specific task of mapping source code into a semantic model is inherently imprecise for this reason and gets handled by the <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/hir/src/source_analyzer.rs"><code>source_analyzer</code></a>.</p>
<p>The semantic interface is declared in the <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/hir/src/semantics.rs"><code>semantics</code></a> module.
Each entity is identified by an integer ID and has a bunch of methods which take a salsa database as an argument and return other entities (which are also IDs).
Internally, these methods invoke various queries on the database to build the model on demand.
Here is <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/hir-ty/src/db.rs#L29-L275">the list of queries</a>.</p>
<p>The first step of building the model is parsing the source code.</p>
<h2 id="syntax-trees"><a class="header" href="#syntax-trees">Syntax trees</a></h2>
<p>An important property of the Rust language is that each file can be parsed in isolation.
Unlike, say, <code>C++</code>, an <code>include</code> cannot change the meaning of the syntax.
For this reason, rust-analyzer can build a syntax tree for each "source file", which could then be reused by several semantic models if this file happens to be a part of several crates.</p>
<p>The representation of syntax trees that rust-analyzer uses is similar to that of <code>Roslyn</code> and Swift's new <a href="https://github.com/apple/swift/tree/5e2c815edfd758f9b1309ce07bfc01c4bc20ec23/lib/Syntax">libsyntax</a>.
Swift's docs give an excellent overview of the approach, so I skip this part here and instead outline the main characteristics of the syntax trees:</p>
<ul>
<li>
<p>Syntax trees are fully lossless.
Converting <strong>any</strong> text to a syntax tree and back is a total identity function.
All whitespace and comments are explicitly represented in the tree.</p>
</li>
<li>
<p>Syntax nodes have generic <code>(next|previous)_sibling</code>, <code>parent</code>, <code>(first|last)_child</code> functions.
You can get from any one node to any other node in the file using only these functions.</p>
</li>
<li>
<p>Syntax nodes know their range (start offset and length) in the file.</p>
</li>
<li>
<p>Syntax nodes share the ownership of their syntax tree: if you keep a reference to a single function, the whole enclosing file is alive.</p>
</li>
<li>
<p>Syntax trees are immutable and the cost of replacing the subtree is proportional to the depth of the subtree.
Read Swift's docs to learn how immutable + parent pointers + cheap modification is possible.</p>
</li>
<li>
<p>Syntax trees are built on a best-effort basis.
All accessor methods return <code>Option</code>s.
The tree for <code>fn foo</code> will contain a function declaration with <code>None</code> for parameter list and body.</p>
</li>
<li>
<p>Syntax trees do not know the file they are built from, they only know about the text.</p>
</li>
</ul>
<p>The implementation is based on the generic <a href="https://github.com/rust-analyzer/rowan/tree/100a36dc820eb393b74abe0d20ddf99077b61f88">rowan</a> crate on top of which a <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/syntax/src/ast/generated.rs">Rust-specific</a> AST is generated.</p>
<p>The next step in constructing the semantic model is ...</p>
<h2 id="building-a-module-tree"><a class="header" href="#building-a-module-tree">Building a Module Tree</a></h2>
<p>The algorithm for building a tree of modules is to start with a crate root (remember, each <code>Crate</code> from a <code>CrateGraph</code> has a <code>FileId</code>), collect all <code>mod</code> declarations and recursively process child modules.
This is handled by the <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/hir-def/src/nameres.rs#L307-L324"><code>crate_def_map_query</code></a>, with two slight variations.</p>
<p>First, rust-analyzer builds a module tree for all crates in a source root simultaneously.
The main reason for this is historical (<code>module_tree</code> predates <code>CrateGraph</code>), but this approach also enables accounting for files which are not part of any crate.
That is, if you create a file but do not include it as a submodule anywhere, you still get semantic completion, and you get a warning about a free-floating module (the actual warning is not implemented yet).</p>
<p>The second difference is that <code>crate_def_map_query</code> does not <em>directly</em> depend on the <code>SourceDatabase::parse</code> query.
Why would calling the parse directly be bad?
Suppose the user changes the file slightly, by adding an insignificant whitespace.
Adding whitespace changes the parse tree (because it includes whitespace), and that means recomputing the whole module tree.</p>
<p>We deal with this problem by introducing an intermediate <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/hir-def/src/nameres.rs#L326-L354"><code>block_def_map_query</code></a>.
This query processes the syntax tree and extracts a set of declared submodule names.
Now, changing the whitespace results in <code>block_def_map_query</code> being re-executed for a <em>single</em> module, but because the result of this query stays the same, we do not have to re-execute <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/hir-def/src/nameres.rs#L307-L324"><code>crate_def_map_query</code></a>.
In fact, we only need to re-execute it when we add/remove new files or when we change mod declarations.</p>
<p>We store the resulting modules in a <code>Vec</code>-based indexed arena.
The indices in the arena become module IDs.
And this brings us to the next topic: assigning IDs in the general case.</p>
<h2 id="location-interner-pattern"><a class="header" href="#location-interner-pattern">Location Interner pattern</a></h2>
<p>One way to assign IDs is how we have dealt with modules: Collect all items into a single array in some specific order and use the index in the array as an ID.
The main drawback of this approach is that these IDs are not stable: Adding a new item can shift the IDs of all other items.
This works for modules because adding a module is a comparatively rare operation, but would be less convenient for, for example, functions.</p>
<p>Another solution here is positional IDs: We can identify a function as "the function with name <code>foo</code> in a ModuleId(92) module".
Such locations are stable: adding a new function to the module (unless it is also named <code>foo</code>) does not change the location.
However, such "ID" types cease to be a <code>Copy</code>able integer and in general can become pretty large if we account for nesting (for example: "third parameter of the <code>foo</code> function of the <code>bar</code> <code>impl</code> in the <code>baz</code> module").</p>
<p><a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/hir-expand/src/lib.rs#L96-L106"><code>Intern</code> and <code>Lookup</code></a> traits allow us to combine the benefits of positional and numeric IDs.
Implementing both traits effectively creates a bidirectional append-only map between locations and integer IDs (typically newtype wrappers for <a href="https://docs.rs/salsa/0.16.1/salsa/struct.InternId.html"><code>salsa::InternId</code></a>) which can "intern" a location and return an integer ID back.
The salsa database we use includes a couple of <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/hir-expand/src/lib.rs#L108-L122">interners</a>.
How to "garbage collect" unused locations is an open question.</p>
<p>For example, we use <code>Intern</code> and <code>Lookup</code> implementations to assign IDs to definitions of functions, structs, enums, etc.
The location, <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/hir-def/src/lib.rs#L209-L212"><code>ItemLoc</code></a> contains two bits of information:</p>
<ul>
<li>the ID of the module which contains the definition,</li>
<li>the ID of the specific item in the module's source code.</li>
</ul>
<p>We "could" use a text offset for the location of a particular item, but that would play badly with salsa: offsets change after edits.
So, as a rule of thumb, we avoid using offsets, text ranges, or syntax trees as keys and values for queries.
What we do instead is we store the "index" of the item among all of the items of a file (so, a positional based ID, but localized to a single file).</p>
<p>One thing we have glossed over for the time being is support for macros.
We have only proof of concept handling of macros at the moment, but they are extremely interesting from an "assigning IDs" perspective.</p>
<h2 id="macros-and-recursive-locations"><a class="header" href="#macros-and-recursive-locations">Macros and recursive locations</a></h2>
<p>The tricky bit about macros is that they effectively create new source files.
While we can use <code>FileId</code>s to refer to original files, we cannot just assign them willy-nilly to the pseudo files of macro expansion.
Instead, we use a special ID, <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/span/src/lib.rs#L148-L160"><code>HirFileId</code></a> to refer to either a usual file or a macro-generated file:</p>
<pre><code class="language-rust">enum HirFileId {
  FileId(FileId),
  Macro(MacroCallId),
}</code></pre>
<p><code>MacroCallId</code> is an interned ID that identifies a particular macro invocation.
Simplifying, it is a <code>HirFileId</code> of a file containing the call plus the offset of the macro call in the file.</p>
<p>Note how <code>HirFileId</code> is defined in terms of <code>MacroCallId</code> which is defined in terms of <code>HirFileId</code>!
This does not recur infinitely though: any chain of <code>HirFileId</code>s bottoms out in <code>HirFileId::FileId</code>, that is, some source file actually written by the user.</p>
<p>Note also that in the actual implementation, the two variants are encoded in a single <code>u32</code>, which are differentiated by the MSB (most significant bit).
If the MSB is 0, the value represents a <code>FileId</code>, otherwise the remaining 31 bits represent a <code>MacroCallId</code>.</p>
<p>Now that we understand how to identify a definition, in a source or in a macro-generated file, we can discuss name resolution a bit.</p>
<h2 id="name-resolution"><a class="header" href="#name-resolution">Name resolution</a></h2>
<p>Name resolution faces the same problem as the module tree: if we look at the syntax tree directly, we will have to recompute name resolution after every modification.
The solution to the problem is the same: We <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/hir-def/src/item_tree.rs#L110-L154">lower</a> the source code of each module into a position-independent representation which does not change if we modify bodies of the items.
After that, we <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/hir-def/src/nameres/collector.rs#L404-L437">loop</a> resolving all imports until we have reached a fixed point.</p>
<p>And, given all our preparation with IDs and a position-independent representation, it is satisfying to <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/hir-def/src/nameres/tests/incremental.rs#L31">test</a> that typing inside a function body does not invalidate name resolution results.</p>
<p>An interesting fact about name resolution is that it "erases" all of the intermediate paths from the imports.
In the end, we know which items are defined and which items are imported in each module, but, if the import was <code>use foo::bar::baz</code>, we deliberately forget what modules <code>foo</code> and <code>bar</code> resolve to.</p>
<p>To serve "goto definition" requests on intermediate segments we need this info in the IDE, however.
Luckily, we need it only for a tiny fraction of imports, so we just ask the module explicitly, "What does the path <code>foo::bar</code> resolve to?".
This is a general pattern: we try to compute the minimal possible amount of information during analysis while allowing the IDE to ask for additional specific bits.</p>
<p>Name resolution is also a good place to introduce another salsa pattern used throughout the analyzer:</p>
<h2 id="source-map-pattern"><a class="header" href="#source-map-pattern">Source Map pattern</a></h2>
<p>Due to an obscure edge case in completion, the IDE needs to know the syntax node of a use statement that imported the given completion candidate.
We cannot just store the syntax node as a part of name resolution: this will break incrementality, due to the fact that syntax changes after every file modification.</p>
<p>We solve this problem during the lowering step of name resolution.
Along with the <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/hir-def/src/item_tree.rs"><code>ItemTree</code></a> output, the lowering query additionally produces an <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/hir-expand/src/ast_id_map.rs#L136-L142"><code>AstIdMap</code></a> via an <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/hir-def/src/item_tree/lower.rs#L32"><code>ast_id_map</code></a> query.
The <code>ItemTree</code> contains <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/hir-def/src/item_tree.rs#L559-L563">imports</a>, but in a position-independent form based on <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/hir-expand/src/ast_id_map.rs#L29"><code>AstId</code></a>.
The <code>AstIdMap</code> contains a mapping from position-independent <code>AstId</code>s to (position-dependent) syntax nodes.</p>
<h2 id="type-inference"><a class="header" href="#type-inference">Type inference</a></h2>
<p>First of all, the implementation of type inference in rust-analyzer was spearheaded by <a href="https://github.com/flodiebold">@flodiebold</a>.
<a href="https://github.com/rust-lang/rust-analyzer/pull/327">#327</a> was an awesome Christmas present, thank you, Florian!</p>
<p>Type inference runs on a per-function granularity and uses the patterns we have discussed previously.</p>
<p>First, we <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/hir-def/src/body.rs">lower the AST</a> of a function body into a position-independent representation.
In this representation, each expression is assigned a <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/hir-def/src/hir.rs#L37">positional ID</a>.
Alongside the lowered expression, <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/hir-def/src/body.rs#L84-L88">a source map</a> is produced, which maps between expression ids and original syntax.
This lowering step also deals with "incomplete" source trees by replacing missing expressions with an explicit <code>Missing</code> expression.</p>
<p>Given the lowered body of the function, we can now run <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/hir-ty/src/infer.rs#L76-L131">type inference</a> and construct a mapping from <code>ExprId</code>s to types.</p>
<h2 id="tying-it-all-together-completion"><a class="header" href="#tying-it-all-together-completion">Tying it all together: completion</a></h2>
<p>To conclude the overview of the rust-analyzer, let us trace the request for (type-inference powered!) code completion!</p>
<p>We start by <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/rust-analyzer/src/main_loop.rs#L213">receiving a message</a> from the language client.
We decode the message as a request for completion and <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/rust-analyzer/src/dispatch.rs#L197-L211">schedule it on the threadpool</a>.
This is the place where we <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/rust-analyzer/src/dispatch.rs#L292">catch</a> canceled errors if, immediately after completion, the client sends some modification.</p>
<p>In <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/rust-analyzer/src/handlers/request.rs#L850-L876">the handler</a>, we deserialize LSP requests into rust-analyzer specific data types (by converting a file URL into a numeric <code>FileId</code>), <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/ide/src/lib.rs#L605-L615">ask analysis for completion</a>, and serialize results into the LSP.</p>
<p>The <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/ide-completion/src/lib.rs#L148-L229">completion implementation</a> is finally the place where we start doing the actual work.
The first step is to collect the <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/ide-completion/src/context.rs#L407-L441"><code>CompletionContext</code></a> -- a struct that describes the cursor position in terms of Rust syntax and semantics.
For example, <code>expected_name: Option&lt;NameOrNameReference&gt;</code> is the syntactic representation for the expected name of what we are completing (usually the parameter name of a function argument), while <code>expected_type: Option&lt;Type&gt;</code> is the semantic model for the expected type of what we are completing.</p>
<p>To construct the context, we first do an <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/ide-completion/src/context.rs#L644-L648">"IntelliJ Trick"</a>: we insert a dummy identifier at the cursor's position and parse this modified file to get a reasonably looking syntax tree.
Then we do a bunch of "classification" routines to figure out the context.
For example, we <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/ide-completion/src/context/analysis.rs#L463">find a parent <code>fn</code> node</a>, get a <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/ide-completion/src/context/analysis.rs#L466">semantic model</a> for it (using the lossy <code>source_analyzer</code> infrastructure), and use it to determine the <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/ide-completion/src/context/analysis.rs#L467">expected type at the cursor position</a>.</p>
<p>The second step is to run a <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/ide-completion/src/lib.rs#L157-L226">series of independent completion routines</a>.
Let us take a closer look at <a href="https://github.com/rust-lang/rust-analyzer/blob/2024-01-01/crates/ide-completion/src/completions/dot.rs#L11-L41"><code>complete_dot</code></a>, which completes fields and methods in <code>foo.bar|</code>.
First, we extract a semantic receiver type out of the <code>DotAccess</code> argument.
Then, using the semantic model for the type, we determine if the receiver implements the <code>Future</code> trait, and add a <code>.await</code> completion item in the affirmative case.
Finally, we add all fields &amp; methods from the type to completion.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="lsp-extensions"><a class="header" href="#lsp-extensions">LSP Extensions</a></h1>
<!---
crates/wgsl-analyzer/src/lsp/extensions.rs hash: a2fcb0c9c05299e6

If you need to change the above hash to make the test pass, please check whether you
need to adjust this doc as well and ping this issue:

https://github.com/DavidAnson/markdownlint/blob/main/doc/md036.md
--->
<!-- markdownlint-disable no-duplicate-heading -->
<p>This document describes LSP extensions used by wgsl-analyzer.
It is a best-effort document; when in doubt, consult the source (and send a PR with clarification).
We aim to upstream all non-WESL-specific extensions to the protocol, but this is not a top priority.
All capabilities are enabled via the <code>experimental</code> field of <code>ClientCapabilities</code> or <code>ServerCapabilities</code>.
Requests which we hope to upstream live under the <code>experimental/</code> namespace.
Requests, which are likely to always remain specific to <code>wgsl-analyzer</code>, are under the <code>wgsl-analyzer/</code> namespace.</p>
<p>If you want to be notified about the changes to this document, subscribe to <a href="https://github.com/wgsl-analyzer/wgsl-analyzer/issues/171">#171</a>.</p>
<ul>
<li><a href="contributing/lsp-extensions.html#configuration-in-initializationoptions">Configuration in <code>initializationOptions</code></a></li>
<li><a href="contributing/lsp-extensions.html#snippet-textedit">Snippet <code>TextEdit</code></a>
<ul>
<li><a href="contributing/lsp-extensions.html#example">Example</a></li>
<li><a href="contributing/lsp-extensions.html#unresolved-questions">Unresolved Questions</a></li>
</ul>
</li>
<li><a href="contributing/lsp-extensions.html#codeaction-groups"><code>CodeAction</code> Groups</a>
<ul>
<li><a href="contributing/lsp-extensions.html#example-1">Example</a></li>
<li><a href="contributing/lsp-extensions.html#unresolved-questions-1">Unresolved Questions</a></li>
</ul>
</li>
<li><a href="contributing/lsp-extensions.html#parent-module">Parent Module</a>
<ul>
<li><a href="contributing/lsp-extensions.html#unresolved-question">Unresolved Question</a></li>
</ul>
</li>
<li><a href="contributing/lsp-extensions.html#join-lines">Join Lines</a>
<ul>
<li><a href="contributing/lsp-extensions.html#example-2">Example</a></li>
<li><a href="contributing/lsp-extensions.html#unresolved-question-1">Unresolved Question</a></li>
</ul>
</li>
<li><a href="contributing/lsp-extensions.html#on-enter">On Enter</a>
<ul>
<li><a href="contributing/lsp-extensions.html#example-3">Example</a></li>
<li><a href="contributing/lsp-extensions.html#unresolved-question-2">Unresolved Question</a></li>
</ul>
</li>
<li><a href="contributing/lsp-extensions.html#structural-search-replace-ssr">Structural Search Replace (SSR)</a>
<ul>
<li><a href="contributing/lsp-extensions.html#example-4">Example</a></li>
<li><a href="contributing/lsp-extensions.html#unresolved-question-3">Unresolved Question</a></li>
</ul>
</li>
<li><a href="contributing/lsp-extensions.html#matching-brace">Matching Brace</a>
<ul>
<li><a href="contributing/lsp-extensions.html#example-5">Example</a></li>
<li><a href="contributing/lsp-extensions.html#unresolved-question-4">Unresolved Question</a></li>
</ul>
</li>
<li><a href="contributing/lsp-extensions.html#open-external-documentation">Open External Documentation</a></li>
<li><a href="contributing/lsp-extensions.html#local-documentation">Local Documentation</a></li>
<li><a href="contributing/lsp-extensions.html#analyzer-status">Analyzer Status</a></li>
<li><a href="contributing/lsp-extensions.html#reload-workspace">Reload Workspace</a></li>
<li><a href="contributing/lsp-extensions.html#server-status">Server Status</a>
<ul>
<li><a href="contributing/lsp-extensions.html#controlling-flycheck">Controlling Flycheck</a></li>
</ul>
</li>
<li><a href="contributing/lsp-extensions.html#syntax-tree">Syntax Tree</a></li>
<li><a href="contributing/lsp-extensions.html#view-syntax-tree">View Syntax Tree</a></li>
<li><a href="contributing/lsp-extensions.html#view-file-text">View File Text</a></li>
<li><a href="contributing/lsp-extensions.html#view-itemtree">View ItemTree</a></li>
<li><a href="contributing/lsp-extensions.html#hover-actions">Hover Actions</a></li>
<li><a href="contributing/lsp-extensions.html#related-tests">Related tests</a></li>
<li><a href="contributing/lsp-extensions.html#hover-range">Hover Range</a>
<ul>
<li><a href="contributing/lsp-extensions.html#example-6">Example</a></li>
</ul>
</li>
<li><a href="contributing/lsp-extensions.html#move-item">Move Item</a></li>
<li><a href="contributing/lsp-extensions.html#workspace-symbols-filtering">Workspace Symbols Filtering</a></li>
<li><a href="contributing/lsp-extensions.html#client-commands">Client Commands</a></li>
<li><a href="contributing/lsp-extensions.html#colored-diagnostic-output">Colored Diagnostic Output</a></li>
<li><a href="contributing/lsp-extensions.html#view-recursive-memory-layout">View Recursive Memory Layout</a>
<ul>
<li><a href="contributing/lsp-extensions.html#unresolved-questions-2">Unresolved questions</a></li>
</ul>
</li>
</ul>
<h2 id="configuration-in-initializationoptions"><a class="header" href="#configuration-in-initializationoptions">Configuration in <code>initializationOptions</code></a></h2>
<p><strong>Upstream Issue:</strong> <a href="https://github.com/microsoft/language-server-protocol/issues/567">https://github.com/microsoft/language-server-protocol/issues/567</a></p>
<p>The <code>initializationOptions</code> field of the <code>InitializeParameters</code> of the initialization request should contain the <code>"wgsl-analyzer"</code> section of the configuration.</p>
<p><code>wgsl-analyzer</code> normally sends a <code>"workspace/configuration"</code> request with <code>{ "items": ["wgsl-analyzer"] }</code> payload.
However, the server cannot do this during initialization.
At the same time, some essential configuration parameters are needed early on, before servicing requests.
For this reason, we ask that <code>initializationOptions</code> contain the configuration, as if the server did make a <code>"workspace/configuration"</code> request.</p>
<p>If a language client does not know about <code>wgsl-analyzer</code>'s configuration options, it can get sensible defaults by doing any of the following:</p>
<ul>
<li>Not sending <code>initializationOptions</code></li>
<li>Sending <code>"initializationOptions": null</code></li>
<li>Sending <code>"initializationOptions": {}</code></li>
</ul>
<h2 id="snippet-textedit"><a class="header" href="#snippet-textedit">Snippet <code>TextEdit</code></a></h2>
<p><strong>Upstream Issue:</strong> <a href="https://github.com/microsoft/language-server-protocol/issues/724">https://github.com/microsoft/language-server-protocol/issues/724</a></p>
<p><strong>Experimental Client Capability:</strong> <code>{ "snippetTextEdit": boolean }</code></p>
<p>If this capability is set, <code>WorkspaceEdit</code>s returned from <code>codeAction</code> requests
and <code>TextEdit</code>s returned from <code>textDocument/onTypeFormatting</code> requests might contain <code>SnippetTextEdit</code>s instead of the usual <code>TextEdit</code>s:</p>
<pre><code class="language-typescript">interface SnippetTextEdit extends TextEdit {
  insertTextFormat?: InsertTextFormat;
  annotationId?: ChangeAnnotationIdentifier;
}
</code></pre>
<pre><code class="language-typescript">export interface TextDocumentEdit {
  textDocument: OptionalVersionedTextDocumentIdentifier;
  edits: (TextEdit | SnippetTextEdit)[];
}
</code></pre>
<p>When applying such code action or text edit, the editor should insert a snippet, with tab stops and placeholders.
At the moment, wgsl-analyzer guarantees that only a single <code>TextDocumentEdit</code> will have edits which can be <code>InsertTextFormat.Snippet</code>.
Any additional <code>TextDocumentEdit</code>s will only have edits which are <code>InsertTextFormat.PlainText</code>.</p>
<h3 id="example"><a class="header" href="#example">Example</a></h3>
<!-- TODO real example -->
<!-- "Add `derive`" code action transforms `struct S;` into `#[derive($0)] struct S;` -->
<h3 id="unresolved-questions"><a class="header" href="#unresolved-questions">Unresolved Questions</a></h3>
<ul>
<li>Where exactly are <code>SnippetTextEdit</code>s allowed (only in code actions at the moment)?</li>
<li>Can snippets span multiple files? (so far, no)</li>
</ul>
<h2 id="codeaction-groups"><a class="header" href="#codeaction-groups"><code>CodeAction</code> Groups</a></h2>
<p><strong>Upstream Issue:</strong> <a href="https://github.com/microsoft/language-server-protocol/issues/994">https://github.com/microsoft/language-server-protocol/issues/994</a></p>
<p><strong>Experimental Client Capability:</strong> <code>{ "codeActionGroup": boolean }</code></p>
<p>If this capability is set, <code>CodeAction</code>s returned from the server contain an additional field, <code>group</code>:</p>
<pre><code class="language-typescript">interface CodeAction {
  title: string;
  group?: string;
  ...
}
</code></pre>
<p>All code actions with the same <code>group</code> should be grouped under a single (extendable) entry in the lightbulb menu.
The set of actions <code>[ { title: "foo" }, { group: "frobnicate", title: "bar" }, { group: "frobnicate", title: "baz" }]</code> should be rendered as</p>
<pre><code class="language-text">💡
  +-------------+
  | foo         |
  +-------------+-----+
  | frobnicate &gt;| bar |
  +-------------+-----+
                | baz |
                +-----+
</code></pre>
<p>Alternatively, selecting <code>frobnicate</code> could present a user with an additional menu to choose between <code>bar</code> and <code>baz</code>.</p>
<h3 id="example-1"><a class="header" href="#example-1">Example</a></h3>
<pre><code class="language-wesl">fn foo() {
    let x: Entry/*cursor here*/ = todo!();
}
</code></pre>
<p>Invoking code action at this position will yield two code actions for importing <code>Entry</code> from either <code>collections::HashMap</code> or <code>collection::BTreeMap</code>, grouped under a single "import" group.</p>
<h3 id="unresolved-questions-1"><a class="header" href="#unresolved-questions-1">Unresolved Questions</a></h3>
<ul>
<li>Is a fixed two-level structure enough?</li>
<li>Should we devise a general way to encode custom interaction protocols for GUI refactorings?</li>
</ul>
<h2 id="parent-module"><a class="header" href="#parent-module">Parent Module</a></h2>
<p><strong>Upstream Issue:</strong> <a href="https://github.com/microsoft/language-server-protocol/issues/1002">https://github.com/microsoft/language-server-protocol/issues/1002</a></p>
<p><strong>Experimental Server Capability:</strong> <code>{ "parentModule": boolean }</code></p>
<p>This request is sent from client to server to handle "Goto Parent Module" editor action.</p>
<p><strong>Method:</strong> <code>experimental/parentModule</code></p>
<p><strong>Request:</strong> <code>TextDocumentPositionParameters</code></p>
<p><strong>Response:</strong> <code>Location | Location[] | LocationLink[] | null</code></p>
<h3 id="unresolved-question"><a class="header" href="#unresolved-question">Unresolved Question</a></h3>
<ul>
<li>An alternative would be to use a more general "gotoSuper" request, which would work for super methods, super classes, and super modules.
This is the approach IntelliJ Rust is taking.
However, experience shows that super module (which generally has a feeling of navigation between files) should be separate.
If you want super module, but the cursor happens to be inside an overridden function, the behavior with a single "gotoSuper" request is surprising.</li>
</ul>
<h2 id="join-lines"><a class="header" href="#join-lines">Join Lines</a></h2>
<p><strong>Upstream Issue:</strong> <a href="https://github.com/microsoft/language-server-protocol/issues/992">https://github.com/microsoft/language-server-protocol/issues/992</a></p>
<p><strong>Experimental Server Capability:</strong> <code>{ "joinLines": boolean }</code></p>
<p>This request is sent from client to server to handle "Join Lines" editor action.</p>
<p><strong>Method:</strong> <code>experimental/joinLines</code></p>
<p><strong>Request:</strong></p>
<pre><code class="language-typescript">interface JoinLinesParameters {
    textDocument: TextDocumentIdentifier,
    /// Currently active selections/cursor offsets.
    /// This is an array to support multiple cursors.
    ranges: Range[],
}
</code></pre>
<p><strong>Response:</strong> <code>TextEdit[]</code></p>
<h3 id="example-2"><a class="header" href="#example-2">Example</a></h3>
<pre><code class="language-wesl">fn main() {
    /*cursor here*/let x = {
        92
    };
}
</code></pre>
<p><code>experimental/joinLines</code> yields (curly braces are automagically removed)</p>
<pre><code class="language-wesl">fn foo() {
    let x = 92;
}
</code></pre>
<h3 id="unresolved-question-1"><a class="header" href="#unresolved-question-1">Unresolved Question</a></h3>
<ul>
<li>What is the position of the cursor after <code>joinLines</code>?
Currently, this is left to editor's discretion, but it might be useful to specify on the server via snippets.
However, it then becomes unclear how it works with multi cursor.</li>
</ul>
<h2 id="on-enter"><a class="header" href="#on-enter">On Enter</a></h2>
<p><strong>Upstream Issue:</strong> <a href="https://github.com/microsoft/language-server-protocol/issues/1001">https://github.com/microsoft/language-server-protocol/issues/1001</a></p>
<p><strong>Experimental Server Capability:</strong> <code>{ "onEnter": boolean }</code></p>
<p>This request is sent from client to server to handle the <kbd>Enter</kbd> key press.</p>
<p><strong>Method:</strong> <code>experimental/onEnter</code></p>
<p><strong>Request:</strong> <code>TextDocumentPositionParameters</code></p>
<p><strong>Response:</strong></p>
<pre><code class="language-typescript">SnippetTextEdit[]
</code></pre>
<h3 id="example-3"><a class="header" href="#example-3">Example</a></h3>
<pre><code class="language-wesl">fn foo() {
    // Some /*cursor here*/ docs
    let x = 92;
}
</code></pre>
<p><code>experimental/onEnter</code> returns the following snippet</p>
<pre><code class="language-wesl">fn foo() {
    // Some
    // $0 docs
    let x = 92;
}
</code></pre>
<p>The primary goal of <code>onEnter</code> is to handle automatic indentation when opening a new line.
This is not yet implemented.
The secondary goal is to handle fixing up syntax, like continuing doc strings and comments, and escaping <code>\n</code> in string literals.</p>
<p>As proper cursor positioning is the main purpose of <code>onEnter</code>, it uses <code>SnippetTextEdit</code>.</p>
<h3 id="unresolved-question-2"><a class="header" href="#unresolved-question-2">Unresolved Question</a></h3>
<ul>
<li>How to deal with synchronicity of the request?
One option is to require the client to block until the server returns the response.
Another option is to do an operational transforms style merging of edits from client and server.
A third option is to do a record-replay: client applies heuristic on enter immediately, then applies all the user's keypresses.
When the server is ready with the response, the client rollbacks all the changes and applies the recorded actions on top of the correct response.</li>
<li>How to deal with multiple carets?</li>
<li>Should we extend this to arbitrary typed events and not just <code>onEnter</code>?</li>
</ul>
<h2 id="structural-search-replace-ssr"><a class="header" href="#structural-search-replace-ssr">Structural Search Replace (SSR)</a></h2>
<p><strong>Experimental Server Capability:</strong> <code>{ "ssr": boolean }</code></p>
<p>This request is sent from client to server to handle structural search replace -- automated syntax tree based transformation of the source.</p>
<p><strong>Method:</strong> <code>experimental/ssr</code></p>
<p><strong>Request:</strong></p>
<pre><code class="language-typescript">interface SsrParameters {
    /// Search query.
    /// The specific syntax is specified outside of the protocol.
    query: string,
    /// If true, only check the syntax of the query and do not compute the actual edit.
    parseOnly: boolean,
    /// The current text document.
    /// This and `position` will be used to determine in what scope paths in `query` should be resolved.
    textDocument: TextDocumentIdentifier;
    /// Position where SSR was invoked.
    position: Position;
    /// Current selections.
    /// Search/replace will be restricted to these if non-empty.
    selections: Range[];
}
</code></pre>
<p><strong>Response:</strong></p>
<pre><code class="language-typescript">WorkspaceEdit
</code></pre>
<h3 id="example-4"><a class="header" href="#example-4">Example</a></h3>
<p>SSR with query <code>foo($a, $b) ==&gt;&gt; ($a).foo($b)</code> will transform, eg <code>foo(y + 5, z)</code> into <code>(y + 5).foo(z)</code>.</p>
<h3 id="unresolved-question-3"><a class="header" href="#unresolved-question-3">Unresolved Question</a></h3>
<ul>
<li>Probably needs search without replace mode</li>
<li>Needs a way to limit the scope to certain files.</li>
</ul>
<h2 id="matching-brace"><a class="header" href="#matching-brace">Matching Brace</a></h2>
<p><strong>Upstream Issue:</strong> <a href="https://github.com/microsoft/language-server-protocol/issues/999">https://github.com/microsoft/language-server-protocol/issues/999</a></p>
<p><strong>Experimental Server Capability:</strong> <code>{ "matchingBrace": boolean }</code></p>
<p>This request is sent from client to server to handle "Matching Brace" editor action.</p>
<p><strong>Method:</strong> <code>experimental/matchingBrace</code></p>
<p><strong>Request:</strong></p>
<pre><code class="language-typescript">interface MatchingBraceParameters {
    textDocument: TextDocumentIdentifier,
    /// Position for each cursor
    positions: Position[],
}
</code></pre>
<p><strong>Response:</strong></p>
<pre><code class="language-typescript">Position[]
</code></pre>
<h3 id="example-5"><a class="header" href="#example-5">Example</a></h3>
<pre><code class="language-wesl">fn main() {
  let x: array&lt;()/*cursor here*/&gt; = array();
}
</code></pre>
<p><code>experimental/matchingBrace</code> yields the position of <code>&lt;</code>.
In many cases, matching braces can be handled by the editor.
However, some cases (like disambiguating between generics and comparison operations) need a real parser.
Moreover, it would be cool if editors did not need to implement even basic language parsing.</p>
<h3 id="unresolved-question-4"><a class="header" href="#unresolved-question-4">Unresolved Question</a></h3>
<ul>
<li>Should we return a nested brace structure, to allow <a href="https://paredit.org">paredit</a>-like actions of jump <em>out</em> of the current brace pair?
This is how <code>SelectionRange</code> request works.</li>
<li>Alternatively, should we perhaps flag certain <code>SelectionRange</code>s as being brace pairs?</li>
</ul>
<h2 id="open-external-documentation"><a class="header" href="#open-external-documentation">Open External Documentation</a></h2>
<p>This request is sent from the client to the server to obtain web and local URL(s) for documentation related to the symbol under the cursor, if available.</p>
<p><strong>Method:</strong> <code>experimental/externalDocs</code></p>
<p><strong>Request:</strong> <code>TextDocumentPositionParameters</code></p>
<p><strong>Response:</strong> <code>string | null</code></p>
<h2 id="local-documentation"><a class="header" href="#local-documentation">Local Documentation</a></h2>
<p><strong>Experimental Client Capability:</strong> <code>{ "localDocs": boolean }</code></p>
<p>If this capability is set, the <code>Open External Documentation</code> request returned from the server will have the following structure:</p>
<pre><code class="language-typescript">interface ExternalDocsResponse {
    web?: string;
    local?: string;
}
</code></pre>
<h2 id="analyzer-status"><a class="header" href="#analyzer-status">Analyzer Status</a></h2>
<p><strong>Method:</strong> <code>wgsl-analyzer/analyzerStatus</code></p>
<p><strong>Request:</strong></p>
<pre><code class="language-typescript">interface AnalyzerStatusParameters {
    textDocument?: TextDocumentIdentifier;
}
</code></pre>
<p><strong>Response:</strong> <code>string</code></p>
<p>Returns internal status message, mostly for debugging purposes.</p>
<h2 id="reload-workspace"><a class="header" href="#reload-workspace">Reload Workspace</a></h2>
<p><strong>Method:</strong> <code>wgsl-analyzer/reloadWorkspace</code></p>
<p><strong>Request:</strong> <code>null</code></p>
<p><strong>Response:</strong> <code>null</code></p>
<p>Reloads project information (that is, re-executes <code>cargo metadata</code>).</p>
<h2 id="server-status"><a class="header" href="#server-status">Server Status</a></h2>
<p><strong>Experimental Client Capability:</strong> <code>{ "serverStatusNotification": boolean }</code></p>
<p><strong>Method:</strong> <code>experimental/serverStatus</code></p>
<p><strong>Notification:</strong></p>
<pre><code class="language-typescript">interface ServerStatusParameters {
    /// `ok` means that the server is completely functional.
    ///
    /// `warning` means that the server is partially functional.
    /// It can answer correctly to most requests, but some results
    /// might be wrong due to, for example, some missing dependencies.
    ///
    /// `error` means that the server is not functional.
    /// For example, there is a fatal build configuration problem.
    /// The server might still give correct answers to simple requests,
    /// but most results will be incomplete or wrong.
    health: "ok" | "warning" | "error",
    /// Is there any pending background work which might change the status?
    /// For example, are dependencies being downloaded?
    quiescent: boolean,
    /// Explanatory message to show on hover.
    message?: string,
}
</code></pre>
<p>This notification is sent from server to client.
The client can use it to display <em>persistent</em> status to the user (in modline).
It is similar to the <code>showMessage</code>, but is intended for states rather than point-in-time events.</p>
<p>Note that this functionality is intended primarily to inform the end user about the state of the server.
In particular, it is valid for the client to completely ignore this extension.
Clients are discouraged from but are allowed to use the <code>health</code> status to decide if it is worth sending a request to the server.</p>
<h3 id="controlling-flycheck"><a class="header" href="#controlling-flycheck">Controlling Flycheck</a></h3>
<p>The flycheck/checkOnSave feature can be controlled via notifications sent by the client to the server.</p>
<p><strong>Method:</strong> <code>wgsl-analyzer/runFlycheck</code></p>
<p><strong>Notification:</strong></p>
<pre><code class="language-typescript">interface RunFlycheckParameters {
    /// The text document whose cargo workspace flycheck process should be started.
    /// If the document is null or does not belong to a cargo workspace all flycheck processes will be started.
    textDocument: lc.TextDocumentIdentifier | null;
}
</code></pre>
<p>Triggers the flycheck processes.</p>
<p><strong>Method:</strong> <code>wgsl-analyzer/clearFlycheck</code></p>
<p><strong>Notification:</strong></p>
<pre><code class="language-typescript">interface ClearFlycheckParameters {}
</code></pre>
<p>Clears the flycheck diagnostics.</p>
<p><strong>Method:</strong> <code>wgsl-analyzer/cancelFlycheck</code></p>
<p><strong>Notification:</strong></p>
<pre><code class="language-typescript">interface CancelFlycheckParameters {}
</code></pre>
<p>Cancels all running flycheck processes.</p>
<h2 id="syntax-tree"><a class="header" href="#syntax-tree">Syntax Tree</a></h2>
<p><strong>Method:</strong> <code>wgsl-analyzer/syntaxTree</code></p>
<p><strong>Request:</strong></p>
<pre><code class="language-typescript">interface SyntaxTreeParameters {
    textDocument: TextDocumentIdentifier,
    range?: Range,
}
</code></pre>
<p><strong>Response:</strong> <code>string</code></p>
<p>Returns textual representation of a parse tree for the file/selected region.
Primarily for debugging, but very useful for all people working on wgsl-analyzer itself.</p>
<h2 id="view-syntax-tree"><a class="header" href="#view-syntax-tree">View Syntax Tree</a></h2>
<p><strong>Method:</strong> <code>wgsl-analyzer/viewSyntaxTree</code></p>
<p><strong>Request:</strong></p>
<pre><code class="language-typescript">interface ViewSyntaxTreeParameters {
    textDocument: TextDocumentIdentifier,
}
</code></pre>
<p><strong>Response:</strong> <code>string</code></p>
<p>Returns json representation of the file's syntax tree.
Used to create a treeView for debugging and working on wgsl-analyzer itself.</p>
<h2 id="view-file-text"><a class="header" href="#view-file-text">View File Text</a></h2>
<p><strong>Method:</strong> <code>wgsl-analyzer/viewFileText</code></p>
<p><strong>Request:</strong> <code>TextDocumentIdentifier</code></p>
<p><strong>Response:</strong> <code>string</code></p>
<p>Returns the text of a file as seen by the server.
This is for debugging file sync problems.</p>
<h2 id="view-itemtree"><a class="header" href="#view-itemtree">View ItemTree</a></h2>
<p><strong>Method:</strong> <code>wgsl-analyzer/viewItemTree</code></p>
<p><strong>Request:</strong></p>
<pre><code class="language-typescript">interface ViewItemTreeParameters {
    textDocument: TextDocumentIdentifier,
}
</code></pre>
<p><strong>Response:</strong> <code>string</code></p>
<p>Returns a textual representation of the <code>ItemTree</code> of the currently open file, for debugging.</p>
<h2 id="hover-actions"><a class="header" href="#hover-actions">Hover Actions</a></h2>
<p><strong>Experimental Client Capability:</strong> <code>{ "hoverActions": boolean }</code></p>
<p>If this capability is set, the <code>Hover</code> request returned from the server might contain an additional field, <code>actions</code>:</p>
<pre><code class="language-typescript">interface Hover {
    ...
    actions?: CommandLinkGroup[];
}

interface CommandLink extends Command {
    /**
     * A tooltip for the command, when represented in the UI.
     */
    tooltip?: string;
}

interface CommandLinkGroup {
    title?: string;
    commands: CommandLink[];
}
</code></pre>
<p>Such actions on the client side are appended to a hover bottom as command links:</p>
<pre><code class="language-text">  +-----------------------------+
  | Hover content               |
  |                             |
  +-----------------------------+
  | _Action1_ | _Action2_       |  &lt;- first group, no TITLE
  +-----------------------------+
  | TITLE _Action1_ | _Action2_ |  &lt;- second group
  +-----------------------------+
  ...
</code></pre>
<h2 id="related-tests"><a class="header" href="#related-tests">Related tests</a></h2>
<p>This request is sent from client to server to get the list of tests for the specified position.</p>
<p><strong>Method:</strong> <code>wgsl-analyzer/relatedTests</code></p>
<p><strong>Request:</strong> <code>TextDocumentPositionParameters</code></p>
<p><strong>Response:</strong> <code>TestInfo[]</code></p>
<pre><code class="language-typescript">interface TestInfo {
    runnable: Runnable;
}
</code></pre>
<h2 id="hover-range"><a class="header" href="#hover-range">Hover Range</a></h2>
<p><strong>Upstream Issue:</strong> <a href="https://github.com/microsoft/language-server-protocol/issues/377">https://github.com/microsoft/language-server-protocol/issues/377</a></p>
<p><strong>Experimental Server Capability:</strong> { "hoverRange": boolean }</p>
<p>This extension allows passing a <code>Range</code> as a <code>position</code> field of <code>HoverParameters</code>.
The primary use-case is to use the hover request to show the type of the expression currently selected.</p>
<pre><code class="language-typescript">interface HoverParameters extends WorkDoneProgressParameters {
    textDocument: TextDocumentIdentifier;
    position: Range | Position;
}
</code></pre>
<p>Whenever the client sends a <code>Range</code>, it is understood as the current selection and any hover included in the range will show the type of the expression if possible.</p>
<h3 id="example-6"><a class="header" href="#example-6">Example</a></h3>
<pre><code class="language-wesl">fn main() {
    let expression = $01 + 2 * 3$0;
}
</code></pre>
<p>Triggering a hover inside the selection above will show a result of <code>i32</code>.</p>
<h2 id="move-item"><a class="header" href="#move-item">Move Item</a></h2>
<p><strong>Upstream Issue:</strong> <a href="https://github.com/rust-lang/rust-analyzer/issues/6823">https://github.com/rust-lang/rust-analyzer/issues/6823</a></p>
<p>This request is sent from client to server to move item under cursor or selection in some direction.</p>
<p><strong>Method:</strong> <code>experimental/moveItem</code></p>
<p><strong>Request:</strong> <code>MoveItemParameters</code></p>
<p><strong>Response:</strong> <code>SnippetTextEdit[]</code></p>
<pre><code class="language-typescript">export interface MoveItemParameters {
    textDocument: TextDocumentIdentifier,
    range: Range,
    direction: Direction
}

export const enum Direction {
    Up = "Up",
    Down = "Down"
}
</code></pre>
<h2 id="workspace-symbols-filtering"><a class="header" href="#workspace-symbols-filtering">Workspace Symbols Filtering</a></h2>
<p><strong>Upstream Issue:</strong> <a href="https://github.com/microsoft/language-server-protocol/issues/941">https://github.com/microsoft/language-server-protocol/issues/941</a></p>
<p><strong>Experimental Server Capability:</strong> <code>{ "workspaceSymbolScopeKindFiltering": boolean }</code></p>
<p>Extends the existing <code>workspace/symbol</code> request with ability to filter symbols by broad scope and kind of symbol.
If this capability is set, <code>workspace/symbol</code> parameter gains two new optional fields:</p>
<pre><code class="language-typescript">interface WorkspaceSymbolParameters {
    /**
     * Return only the symbols of specified kinds.
     */
    searchKind?: WorkspaceSymbolSearchKind;
    ...
}

const enum WorkspaceSymbolSearchKind {
    OnlyTypes = "onlyTypes",
    AllSymbols = "allSymbols"
}
</code></pre>
<h2 id="client-commands"><a class="header" href="#client-commands">Client Commands</a></h2>
<p><strong>Upstream Issue:</strong> <a href="https://github.com/microsoft/language-server-protocol/issues/642">https://github.com/microsoft/language-server-protocol/issues/642</a></p>
<p><strong>Experimental Client Capability:</strong> <code>{ "commands?": ClientCommandOptions }</code></p>
<p>Certain LSP types originating on the server, notably code lenses, embed commands.
Commands can be serviced either by the server or by the client.
However, the server does not know which commands are available on the client.</p>
<p>This extensions allows the client to communicate this info.</p>
<pre><code class="language-typescript">export interface ClientCommandOptions {
    /**
     * The commands to be executed on the client
     */
    commands: string[];
}
</code></pre>
<h2 id="colored-diagnostic-output"><a class="header" href="#colored-diagnostic-output">Colored Diagnostic Output</a></h2>
<p><strong>Experimental Client Capability:</strong> <code>{ "colorDiagnosticOutput": boolean }</code></p>
<p>If this capability is set, the "full compiler diagnostics" provided by <code>checkOnSave</code>
will include ANSI color and style codes to render the diagnostic in a similar manner
as <code>cargo</code>. This is translated into <code>--message-format=json-diagnostic-rendered-ansi</code>
when flycheck is run, instead of the default <code>--message-format=json</code>.</p>
<p>The full compiler rendered diagnostics are included in the server response
regardless of this capability:</p>
<pre><code class="language-typescript">// https://microsoft.github.io/language-server-protocol/specifications/specification-current#diagnostic
export interface Diagnostic {
    ...
    data?: {
        /**
         * The human-readable compiler output as it would be printed to a terminal.
         * Includes ANSI color and style codes if the client has set the experimental
         * `colorDiagnosticOutput` capability.
         */
        rendered?: string;
    };
}
</code></pre>
<h2 id="view-recursive-memory-layout"><a class="header" href="#view-recursive-memory-layout">View Recursive Memory Layout</a></h2>
<p><strong>Method:</strong> <code>wgsl-analyzer/viewRecursiveMemoryLayout</code></p>
<p><strong>Request:</strong> <code>TextDocumentPositionParameters</code></p>
<p><strong>Response:</strong></p>
<pre><code class="language-typescript">export interface RecursiveMemoryLayoutNode = {
    /// Name of the item, or [ROOT], `.n` for tuples
    item_name: string;
    /// Full name of the type (type aliases are ignored)
    typename: string;
    /// Size of the type in bytes
    size: number;
    /// Alignment of the type in bytes
    alignment: number;
    /// Offset of the type relative to its parent (or 0 if its the root)
    offset: number;
    /// Index of the node's parent (or -1 if its the root)
    parent_index: number;
    /// Index of the node's children (or -1 if it does not have children)
    children_start: number;
    /// Number of child nodes (unspecified it does not have children)
    children_length: number;
};

export interface RecursiveMemoryLayout = {
    nodes: RecursiveMemoryLayoutNode[];
};
</code></pre>
<p>Returns a vector of nodes representing items in the datatype as a tree, <code>RecursiveMemoryLayout::nodes[0]</code> is the root node.</p>
<p>If <code>RecursiveMemoryLayout::nodes::length == 0</code> we could not find a suitable type.</p>
<p>Generic Types do not give anything because they are incomplete. Fully specified generic
types do not give anything if they are selected directly but do work when a child of
other types <a href="https://github.com/rust-lang/rust-analyzer/issues/15010">this is consistent with other behavior</a>.</p>
<h3 id="unresolved-questions-2"><a class="header" href="#unresolved-questions-2">Unresolved questions</a></h3>
<ul>
<li>How should enums/unions be represented? currently they do not produce any children because they have multiple distinct sets of children.</li>
<li>Should niches be represented? currently they are not reported.</li>
<li>A visual representation of the memory layout is not specified, see the provided implementation for an example, however it may not translate well to terminal based editors or other such things.</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="setup-guide"><a class="header" href="#setup-guide">Setup Guide</a></h1>
<p>This guide gives a simplified, opinionated setup for developers contributing to <code>wgsl-analyzer</code> using Visual Studio Code.
It enables developers to make changes and Visual Studio Code Insiders to test those changes.
This guide will assume you have Visual Studio Code and Visual Studio Code Insiders installed.</p>
<h2 id="prerequisites-1"><a class="header" href="#prerequisites-1">Prerequisites</a></h2>
<p>Since <code>wgsl-analyzer</code> is a Rust project, you will need to install Rust.
You can download and install the latest stable version of <a href="https://www.rust-lang.org/tools/install">Rust</a>.</p>
<h2 id="step-by-step-setup"><a class="header" href="#step-by-step-setup">Step-by-Step Setup</a></h2>
<ol>
<li>Fork the <a href="https://github.com/wgsl-analyzer/wgsl-analyzer"><code>wgsl-analyzer</code> repository</a> and clone the fork to your local machine.</li>
<li>Open the project in Visual Studio Code.</li>
<li>Open a terminal and run <code>cargo build</code> to build the project.</li>
<li>Install the language server locally by running the following command:</li>
</ol>
<pre><code class="language-bash">cargo xtask install --server --code-bin code-insiders --dev-rel
</code></pre>
<p>In the output of this command, there should be a file path provided to the installed binary on your local machine.
It should look something like the following output below:</p>
<pre><code class="language-text">Installing &lt;path-to-wgsl-analyzer-binary&gt;
Installed package `wgsl-analyzer v0.0.0 (&lt;path-to-wgsl-analyzer-binary&gt;)` (executable `wgsl-analyzer.exe`)
</code></pre>
<p>In Visual Studio Code Insiders, you will want to open your User Settings (JSON) from the Command Palette.
From there, you should ensure that the <code>wgsl-analyzer.server.path</code> key is set to the <code>&lt;path-to-wgsl-analyzer-binary&gt;</code>.
This will tell Visual Studio Code Insiders to use the locally installed version that you can debug.</p>
<p>The User Settings (JSON) file should contain the following:</p>
<pre><code class="language-json">{
    "wgsl-analyzer.server.path": "&lt;path-to-wgsl-analyzer-binary&gt;"
}
</code></pre>
<p>Now you should be able to make changes to <code>wgsl-analyzer</code> in Visual Studio Code and then view the changes in Visual Studio Code Insiders.</p>
<h2 id="debugging-wgsl-analyzer"><a class="header" href="#debugging-wgsl-analyzer">Debugging <code>wgsl-analyzer</code></a></h2>
<p>The simplest way to debug <code>wgsl-analyzer</code> is to use the <code>eprintln!</code> macro.
The reason why we use <code>eprintln!</code> instead of <code>println!</code> is because the language server uses <code>stdout</code> to send messages.
Instead, debug using <code>stderr</code>.</p>
<p>An example debugging statement could go into the <code>main_loop.rs</code> file which can be found at <code>crates/wgsl-analyzer/src/main_loop.rs</code>.
Inside the <code>main_loop</code> add the following <code>eprintln!</code> to test debugging <code>wgsl-analyzer</code>:</p>
<pre><code class="language-rust">eprintln!("Hello, world!");</code></pre>
<p>Now we run <code>cargo build</code> and <code>cargo xtask install --server --code-bin code-insiders --dev-rel</code> to reinstall the server.</p>
<p>Now on Visual Studio Code Insiders, we should be able to open the Output tab on our terminal and switch to <code>wgsl-analyzer</code> Language Server to see the <code>eprintln!</code> statement we just wrote.</p>
<p>If you are able to see your output, you now have a complete workflow for debugging <code>wgsl-analyzer</code>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="style"><a class="header" href="#style">Style</a></h1>
<p>Our approach to "clean code" is two-fold:</p>
<ul>
<li>We generally do not block PRs on style changes.</li>
<li>At the same time, all code in <code>wgsl-analyzer</code> is constantly refactored.</li>
</ul>
<p>It is explicitly OK for a reviewer to flag only some nits in the PR, and then send a follow-up cleanup PR for things which are easier to explain by example, cc-ing the original author.
Sending small cleanup PRs (like renaming a single local variable) is encouraged.</p>
<p>When reviewing pull requests, prefer extending this document to leaving non-reusable comments on the pull request itself.</p>
<h2 id="general"><a class="header" href="#general">General</a></h2>
<h3 id="scale-of-changes"><a class="header" href="#scale-of-changes">Scale of Changes</a></h3>
<p>Everyone knows that it is better to send small &amp; focused pull requests.
The problem is, sometimes you <em>have</em> to, e.g., rewrite the whole compiler, and that just does not fit into a set of isolated PRs.</p>
<p>The main things to keep an eye on are the boundaries between various components.
There are three kinds of changes:</p>
<ol>
<li>
<p>Internals of a single component are changed.
Specifically, you do not change any <code>pub</code> items.
A good example here would be an addition of a new assist.</p>
</li>
<li>
<p>API of a component is expanded.
Specifically, you add a new <code>pub</code> function which was not there before.
A good example here would be the expansion of the assist API, for example, to implement lazy assists or assist groups.</p>
</li>
<li>
<p>A new dependency between components is introduced.
Specifically, you add a <code>pub use</code> re-export from another crate or you add a new line to the <code>[dependencies]</code> section of <code>Cargo.toml</code>.
A good example here would be adding reference search capability to the assists crate.</p>
</li>
</ol>
<p>For the first group, the change is generally merged as long as:</p>
<ul>
<li>it works for the happy case,</li>
<li>it has tests,</li>
<li>it does not panic for the unhappy case.</li>
</ul>
<p>For the second group, the change would be subjected to quite a bit of scrutiny and iteration.
The new API needs to be right (or at least easy to change later).
The actual implementation does not matter that much.
It is very important to minimize the number of changed lines of code for changes of the second kind.
Often, you start doing a change of the first kind, only to realize that you need to elevate to a change of the second kind.
In this case, we will probably ask you to split API changes into a separate PR.</p>
<p>Changes of the third group should be pretty rare, so we do not specify any specific process for them.
That said, adding an innocent-looking <code>pub use</code> is a very simple way to break encapsulation, keep an eye on it!</p>
<p>Note: if you enjoyed this abstract hand-waving about boundaries, you might appreciate <a href="https://www.tedinski.com/2018/02/06/system-boundaries.html">https://www.tedinski.com/2018/02/06/system-boundaries.html</a>.</p>
<h3 id="cratesio-dependencies"><a class="header" href="#cratesio-dependencies">Crates.io Dependencies</a></h3>
<p>We try to be very conservative with the usage of crates.io dependencies.
Do not use small "helper" crates (exception: <code>itertools</code> and <code>either</code> are allowed).
If there is some general reusable bit of code you need, consider adding it to the <code>stdx</code> crate.
A useful exercise is to read Cargo.lock and see if some <em>transitive</em> dependencies do not make sense for <code>wgsl-analyzer</code>.</p>
<p><strong>Rationale:</strong> keep compile times low, create ecosystem pressure for faster compiles, reduce the number of things which might break.</p>
<h3 id="commit-style"><a class="header" href="#commit-style">Commit Style</a></h3>
<p>We do not have specific rules around git history hygiene.
Maintaining clean git history is strongly encouraged, but not enforced.
Use rebase workflow, it is OK to rewrite history during the PR review process.
After you are happy with the state of the code, please use <a href="https://git-scm.com/book/en/v2/Git-Tools-Rewriting-History">interactive rebase</a> to squash fixup commits.</p>
<p>Avoid @mentioning people in commit messages and pull request descriptions (they are added to commit messages by bors).
Such messages create a lot of duplicate notification traffic during rebases.</p>
<p>If possible, write Pull Request titles and descriptions from the user's perspective:</p>
<pre><code class="language-text">## GOOD
Make goto definition work inside macros

## BAD
Use original span for FileId
</code></pre>
<p>This makes it easier to prepare a changelog.</p>
<p>If the change adds a new user-visible functionality, consider recording a GIF with <a href="https://github.com/phw/peek">peek</a> and pasting it into the PR description.</p>
<p>To make writing the release notes easier, you can mark a pull request as a feature, fix, internal change, or minor.
Minor changes are excluded from the release notes, while the other types are distributed in their corresponding sections.
There are two ways to mark this:</p>
<ul>
<li>use a <code>feat:</code>, <code>feature:</code>, <code>fix:</code>, <code>internal:</code>, or <code>minor:</code> prefix in the PR title</li>
<li>write <code>changelog [feature|fix|internal|skip] [description]</code> in a comment or in the PR description; the description is optional and will replace the title if included.</li>
</ul>
<p>These comments do not have to be added by the PR author.
Editing a comment or the PR description or title is also fine, as long as it happens before the release.</p>
<p><strong>Rationale:</strong> clean history is potentially useful, but rarely used.
But many users read changelogs.
Including a description and GIF suitable for the changelog means less work for the maintainers on the release day.</p>
<h3 id="clippy"><a class="header" href="#clippy">Clippy</a></h3>
<p>We use Clippy to improve the code, but if some lints annoy you, allow them in the <a href="contributing/../../Cargo.toml">Cargo.toml</a> [workspace.lints.clippy] section.</p>
<h2 id="code"><a class="header" href="#code">Code</a></h2>
<h3 id="minimal-tests"><a class="header" href="#minimal-tests">Minimal Tests</a></h3>
<p>Most tests in <code>wgsl-analyzer</code> start with a snippet of WESL code.
These snippets should be minimal.
If you copy-paste a snippet of real code into the tests, make sure to remove everything which could be removed.
It also makes sense to format snippets more compactly (for example, by placing enum definitions like <code>enum E { Foo, Bar }</code> on a single line), as long as they are still readable.
When using multiline fixtures, use unindented raw string literals:</p>
<pre><code class="language-rust">    #[test]
    fn inline_field_shorthand() {
        check_assist(
            inline_local_variable,
            r#"
struct S { foo: i32}
fn main() {
    let $0foo = 92;
    S { foo }
}
"#,
            r#"
struct S { foo: i32}
fn main() {
    S { foo: 92 }
}
"#,
        );
    }</code></pre>
<p><strong>Rationale:</strong></p>
<p>There are many benefits to this:</p>
<ul>
<li>less to read or to scroll past</li>
<li>easier to understand what exactly is tested</li>
<li>less stuff printed during printf-debugging</li>
<li>less time to run tests</li>
</ul>
<p>Formatting ensures that you can use your editor's "number of selected characters" feature to correlate offsets with tests' source code.</p>
<h3 id="marked-tests"><a class="header" href="#marked-tests">Marked Tests</a></h3>
<p>Use <a href="https://github.com/matklad/cov-mark"><code>cov_mark::hit! / cov_mark::check!</code></a> when testing specific conditions.
Do not place several marks into a single test or condition.
Do not reuse marks between several tests.</p>
<p><strong>Rationale:</strong> marks provide an easy way to find the canonical test for each bit of code.
This makes it much easier to understand.
More than one mark per test / code branch does not add significantly to understanding.</p>
<h3 id="should_panic"><a class="header" href="#should_panic"><code>#[should_panic]</code></a></h3>
<p>Do not use <code>#[should_panic]</code> tests.
Instead, explicitly check for <code>None</code>, <code>Err</code>, etc.</p>
<p><strong>Rationale:</strong> <code>#[should_panic]</code> is a tool for library authors to make sure that the API does not fail silently when misused.
<code>wgsl-analyzer</code> is not a library.
We do not need to test for API misuse, and we have to handle any user input without panics.
Panic messages in the logs from the <code>#[should_panic]</code> tests are confusing.</p>
<h3 id="ignore"><a class="header" href="#ignore"><code>#[ignore]</code></a></h3>
<p>Do not <code>#[ignore]</code> tests.
If the test currently does not work, assert the wrong behavior and add a fixme explaining why it is wrong.</p>
<p><strong>Rationale:</strong> noticing when the behavior is fixed, making sure that even the wrong behavior is acceptable (i.e., not a panic).</p>
<h3 id="function-preconditions"><a class="header" href="#function-preconditions">Function Preconditions</a></h3>
<p>Express function preconditions in types and force the caller to provide them (rather than checking in callee):</p>
<pre><code class="language-rust">// GOOD
fn frobnicate(walrus: Walrus) {
    ...
}

// BAD
fn frobnicate(walrus: Option&lt;Walrus&gt;) {
    let walrus = match walrus {
        Some(it) =&gt; it,
        None =&gt; return,
    };
    ...
}</code></pre>
<p><strong>Rationale:</strong> this makes control flow explicit at the call site.
Call-site has more context.
It often happens that the precondition falls out naturally or can be bubbled up higher in the stack.</p>
<p>Avoid splitting precondition check and precondition use across functions:</p>
<pre><code class="language-rust">// GOOD
fn main() {
    let string: &amp;str = ...;
    if let Some(contents) = string_literal_contents(string) {

    }
}

fn string_literal_contents(string: &amp;str) -&gt; Option&lt;&amp;str&gt; {
    if string.starts_with('"') &amp;&amp; string.ends_with('"') {
        Some(&amp;string[1..string.len() - 1])
    } else {
        None
    }
}

// BAD
fn main() {
    let string: &amp;str = ...;
    if is_string_literal(string) {
        let contents = &amp;string[1..string.len() - 1];
    }
}

fn is_string_literal(string: &amp;str) -&gt; bool {
    string.starts_with('"') &amp;&amp; string.ends_with('"')
}</code></pre>
<p>In the "Not as good" version, the precondition that <code>1</code> is a valid char boundary is checked in <code>is_string_literal</code> and used in <code>foo</code>.
In the "Good" version, the precondition check and usage are checked in the same block, and then encoded in the types.</p>
<p><strong>Rationale:</strong> non-local code properties degrade under change.</p>
<p>When checking a boolean precondition, prefer <code>if !invariant</code> to <code>if negated_invariant</code>:</p>
<pre><code class="language-rust">// GOOD
if !(index &lt; length) {
    return None;
}

// BAD
if index &gt;= length {
    return None;
}</code></pre>
<p><strong>Rationale:</strong> it is useful to see the invariant relied upon by the rest of the function clearly spelled out.</p>
<h3 id="control-flow"><a class="header" href="#control-flow">Control Flow</a></h3>
<p>As a special case of the previous rule, do not hide control flow inside functions, push it to the caller:</p>
<pre><code class="language-rust">// GOOD
if cond {
    foo();
}

fn foo() {
  ...
}

// BAD
bar();

fn bar() {
    if !cond {
        return;
    }
    ...
}</code></pre>
<h3 id="assertions"><a class="header" href="#assertions">Assertions</a></h3>
<p>Assert liberally.
Prefer <a href="https://docs.rs/always-assert/0.1.2/always_assert/macro.never.html"><code>stdx::never!</code></a> to standard <code>assert!</code>.</p>
<p><strong>Rationale:</strong> See <a href="https://github.com/wgsl-analyzer/wgsl-analyzer/blob/main/docs/book/src/contributing/architecture.md#error-handling">cross cutting concern: error handling</a>.</p>
<h3 id="getters--setters"><a class="header" href="#getters--setters">Getters &amp; Setters</a></h3>
<p>If a field can have any value without breaking invariants, make the field public.
Conversely, if there is an invariant, document it, enforce it in the "constructor" function, make the field private, and provide a getter.
Never provide setters.</p>
<p>Getters should return borrowed data:</p>
<pre><code class="language-rust">struct Person {
    // Invariant: never empty
    first_name: String,
    middle_name: Option&lt;String&gt;
}

// GOOD
impl Person {
    fn first_name(&amp;self) -&gt; &amp;str { self.first_name.as_str() }
    fn middle_name(&amp;self) -&gt; Option&lt;&amp;str&gt; { self.middle_name.as_ref() }
}

// BAD
impl Person {
    fn first_name(&amp;self) -&gt; String { self.first_name.clone() }
    fn middle_name(&amp;self) -&gt; &amp;Option&lt;String&gt; { &amp;self.middle_name }
}</code></pre>
<p><strong>Rationale:</strong> we do not provide public API.
It is cheaper to refactor than to pay getters rent.
Non-local code properties degrade under change.
Privacy makes invariant local.
Borrowed owned types (<code>&amp;String</code>) disclose irrelevant details about internal representation.
Irrelevant (neither right nor wrong) things obscure correctness.</p>
<h3 id="useless-types"><a class="header" href="#useless-types">Useless Types</a></h3>
<p>More generally, always prefer types on the left</p>
<pre><code class="language-rust">// GOOD      BAD
&amp;[T]         &amp;Vec&lt;T&gt;
&amp;str         &amp;String
Option&lt;&amp;T&gt;   &amp;Option&lt;T&gt;
&amp;Path        &amp;PathBuf</code></pre>
<p><strong>Rationale:</strong> types on the left are strictly more general.
Even when generality is not required, consistency is important.</p>
<h3 id="constructors"><a class="header" href="#constructors">Constructors</a></h3>
<p>Prefer <code>Default</code> to zero-argument <code>new</code> function.</p>
<pre><code class="language-rust">// GOOD
#[derive(Default)]
struct Foo {
    bar: Option&lt;Bar&gt;
}

// BAD
struct Foo {
    bar: Option&lt;Bar&gt;
}

impl Foo {
    fn new() -&gt; Foo {
        Foo { bar: None }
    }
}</code></pre>
<p>Prefer <code>Default</code> even if it has to be implemented manually.</p>
<p><strong>Rationale:</strong> less typing in the common case, uniformity.</p>
<p>Use <code>Vec::new</code> rather than <code>vec![]</code>.</p>
<p><strong>Rationale:</strong> uniformity, strength reduction.</p>
<p>Avoid using "dummy" states to implement a <code>Default</code>.
If a type does not have a sensible default, empty value, do not hide it.
Let the caller explicitly decide what the right initial state is.</p>
<h3 id="functions-over-objects"><a class="header" href="#functions-over-objects">Functions Over Objects</a></h3>
<p>Avoid creating "doer" objects.
That is, objects which are created only to execute a single action.</p>
<pre><code class="language-rust">// GOOD
do_thing(arg1, arg2);

// BAD
ThingDoer::new(arg1, arg2).do();</code></pre>
<p>Note that this concerns only outward API.
When implementing <code>do_thing</code>, it might be very useful to create a context object.</p>
<pre><code class="language-rust">pub fn do_thing(
  an_input: Argument1,
  another_input: Argument2,
) -&gt; Result {
    let mut context = Context { an_input, another_input };
    context.run()
}

struct Context {
    an_input: Argument1,
    another_input: Argument2,
}

impl Context {
    fn run(self) -&gt; Result {
        ...
    }
}</code></pre>
<p>The difference is that <code>Context</code> is an implementation detail here.</p>
<p>Sometimes a middle ground is acceptable if this can save some busywork:</p>
<pre><code class="language-rust">ThingDoer::do(an_input, another_input);

pub struct ThingDoer {
    an_input: Argument1,
    another_input: Argument2,
}

impl ThingDoer {
    pub fn do(
        an_input: Argument1,
        another_input: Argument2,
    ) -&gt; Result {
        ThingDoer { an_input, another_input }.run()
    }

    fn run(self) -&gt; Result {
        ...
    }
}</code></pre>
<p><strong>Rationale:</strong> not bothering the caller with irrelevant details, not mixing user API with implementor API.</p>
<h3 id="functions-with-many-parameters"><a class="header" href="#functions-with-many-parameters">Functions with many parameters</a></h3>
<p>Avoid creating functions with many optional or boolean parameters.
Introduce a <code>Config</code> struct instead.</p>
<pre><code class="language-rust">// GOOD
pub struct AnnotationConfig {
    pub binary_target: bool,
    pub annotate_runnables: bool,
    pub annotate_impls: bool,
}

pub fn annotations(
    db: &amp;RootDatabase,
    file_id: FileId,
    config: AnnotationConfig
) -&gt; Vec&lt;Annotation&gt; {
    ...
}

// BAD
pub fn annotations(
    db: &amp;RootDatabase,
    file_id: FileId,
    binary_target: bool,
    annotate_runnables: bool,
    annotate_impls: bool,
) -&gt; Vec&lt;Annotation&gt; {
    ...
}</code></pre>
<p><strong>Rationale:</strong> reducing churn.
If the function has many parameters, they most likely change frequently.
By packing them into a struct we protect all intermediary functions from changes.</p>
<p>Do not implement <code>Default</code> for the <code>Config</code> struct, the caller has more context to determine better defaults.
Do not store <code>Config</code> as a part of the <code>state</code>, pass it explicitly.
This gives more flexibility for the caller.</p>
<p>If there is variation not only in the input parameters, but in the return type as well, consider introducing a <code>Command</code> type.</p>
<pre><code class="language-rust">// MAYBE GOOD
pub struct Query {
    pub name: String,
    pub case_sensitive: bool,
}

impl Query {
    pub fn all(self) -&gt; Vec&lt;Item&gt; { ... }
    pub fn first(self) -&gt; Option&lt;Item&gt; { ... }
}

// MAYBE BAD
fn query_all(name: String, case_sensitive: bool) -&gt; Vec&lt;Item&gt; { ... }
fn query_first(name: String, case_sensitive: bool) -&gt; Option&lt;Item&gt; { ... }</code></pre>
<h3 id="prefer-separate-functions-over-parameters"><a class="header" href="#prefer-separate-functions-over-parameters">Prefer Separate Functions Over Parameters</a></h3>
<p>If a function has a <code>bool</code> or an <code>Option</code> parameter, and it is always called with <code>true</code>, <code>false</code>, <code>Some</code> and <code>None</code> literals, split the function in two.</p>
<pre><code class="language-rust">// GOOD
fn caller_a() {
    foo()
}

fn caller_b() {
    foo_with_bar(Bar::new())
}

fn foo() { ... }
fn foo_with_bar(bar: Bar) { ... }

// BAD
fn caller_a() {
    foo(None)
}

fn caller_b() {
    foo(Some(Bar::new()))
}

fn foo(bar: Option&lt;Bar&gt;) { ... }</code></pre>
<p><strong>Rationale:</strong> more often than not, such functions display "<code>false sharing</code>" -- they have additional <code>if</code> branching inside for two different cases.
Splitting the two different control flows into two functions simplifies each path, and remove cross-dependencies between the two paths.
If there is common code between <code>foo</code> and <code>foo_with_bar</code>, extract <em>that</em> into a common helper.</p>
<h3 id="appropriate-string-types"><a class="header" href="#appropriate-string-types">Appropriate String Types</a></h3>
<p>When interfacing with OS APIs, use <code>OsString</code>, even if the original source of data is utf-8 encoded.
<strong>Rationale:</strong> cleanly delineates the boundary when the data goes into the OS-land.</p>
<p>Use <code>AbsPathBuf</code> and <code>AbsPath</code> over <code>std::Path</code>.
<strong>Rationale:</strong> <code>wgsl-analyzer</code> is a long-lived process which handles several projects at the same time.
It is important not to leak cwd by accident.</p>
<h2 id="premature-pessimization"><a class="header" href="#premature-pessimization">Premature Pessimization</a></h2>
<h3 id="avoid-allocations"><a class="header" href="#avoid-allocations">Avoid Allocations</a></h3>
<p>Avoid writing code which is slower than it needs to be.
Do not allocate a <code>Vec</code> where an iterator would do, do not allocate strings needlessly.</p>
<pre><code class="language-rust">// GOOD
use itertools::Itertools;

let (first_word, second_word) = match text.split_ascii_whitespace().collect_tuple() {
    Some(it) =&gt; it,
    None =&gt; return,
}

// BAD
let words = text.split_ascii_whitespace().collect::&lt;Vec&lt;_&gt;&gt;();
if words.len() != 2 {
    return
}</code></pre>
<p><strong>Rationale:</strong> not allocating is almost always faster.</p>
<h3 id="push-allocations-to-the-call-site"><a class="header" href="#push-allocations-to-the-call-site">Push Allocations to the Call Site</a></h3>
<p>If allocation is inevitable, let the caller allocate the resource:</p>
<pre><code class="language-rust">// GOOD
fn frobnicate(string: String) {
    ...
}

// BAD
fn frobnicate(string: &amp;str) {
    let string = string.to_string();
    ...
}</code></pre>
<p><strong>Rationale:</strong> reveals the costs.
It is also more efficient when the caller already owns the allocation.</p>
<h3 id="collection-types"><a class="header" href="#collection-types">Collection Types</a></h3>
<p>Prefer <code>rustc_hash::FxHashMap</code> and <code>rustc_hash::FxHashSet</code> instead of the ones in <code>std::collections</code>.</p>
<p><strong>Rationale:</strong> they use a hasher that is significantly faster and using them consistently will reduce code size by some small amount.</p>
<h3 id="avoid-intermediate-collections"><a class="header" href="#avoid-intermediate-collections">Avoid Intermediate Collections</a></h3>
<p>When writing a recursive function to compute a set of things, use an accumulator parameter instead of returning a fresh collection.
The accumulator goes first in the list of arguments.</p>
<pre><code class="language-rust">// GOOD
pub fn reachable_nodes(node: Node) -&gt; FxHashSet&lt;Node&gt; {
    let mut result = FxHashSet::default();
    go(&amp;mut result, node);
    result
}
fn go(acc: &amp;mut FxHashSet&lt;Node&gt;, node: Node) {
    acc.insert(node);
    for n in node.neighbors() {
        go(acc, n);
    }
}

// BAD
pub fn reachable_nodes(node: Node) -&gt; FxHashSet&lt;Node&gt; {
    let mut result = FxHashSet::default();
    result.insert(node);
    for n in node.neighbors() {
        result.extend(reachable_nodes(n));
    }
    result
}</code></pre>
<p><strong>Rationale:</strong> re-use allocations, accumulator style is more concise for complex cases.</p>
<h3 id="avoid-monomorphization"><a class="header" href="#avoid-monomorphization">Avoid Monomorphization</a></h3>
<p>Avoid making a lot of code type parametric, <em>especially</em> on the boundaries between crates.</p>
<pre><code class="language-rust">// GOOD
fn frobnicate(function: impl FnMut()) {
    frobnicate_impl(&amp;mut function)
}
fn frobnicate_impl(function: &amp;mut dyn FnMut()) {
    // lots of code
}

// BAD
fn frobnicate(function: impl FnMut()) {
    // lots of code
}</code></pre>
<p>Avoid <code>AsRef</code> polymorphism, it pays back only for widely used libraries:</p>
<pre><code class="language-rust">// GOOD
fn frobnicate(foo: &amp;Path) {
}

// BAD
fn frobnicate(foo: impl AsRef&lt;Path&gt;) {
}</code></pre>
<p><strong>Rationale:</strong> Rust uses monomorphization to compile generic code, meaning that for each instantiation of a generic function with concrete types, the function is compiled afresh, <em>per crate</em>.
This allows for exceptionally good performance, but leads to increased compile times.
Runtime performance obeys the 80%/20% rule -- only a small fraction of code is hot.
Compile time <strong>does not</strong> obey this rule -- all code has to be compiled.</p>
<h2 id="code-style"><a class="header" href="#code-style">Code Style</a></h2>
<h3 id="order-of-imports"><a class="header" href="#order-of-imports">Order of Imports</a></h3>
<p>Separate import groups with blank lines.
Use one <code>use</code> per crate.</p>
<p>Module declarations come before the imports.
Order them in "suggested reading order" for a person new to the code base.</p>
<pre><code class="language-rust">mod x;
mod y;

// First std.
use std::{ ... }

// Second, external crates (both crates.io crates and other wgsl-analyzer crates).
use crate_foo::{ ... }
use crate_bar::{ ... }

// Then current crate.
use crate::{}

// Finally, parent and child modules, but prefer `use crate::`.
use super::{}

// Re-exports are treated as item definitions rather than imports, so they go
// after imports and modules. Use them sparingly.
pub use crate::x::Z;</code></pre>
<p><strong>Rationale:</strong> consistency.
Reading order is important for new contributors.
Grouping by crate allows spotting unwanted dependencies easier.</p>
<h3 id="import-style"><a class="header" href="#import-style">Import Style</a></h3>
<p>Qualify items from <code>hir</code> and <code>ast</code>.</p>
<pre><code class="language-rust">// GOOD
use syntax::ast;

fn frobnicate(func: hir::Function, r#struct: ast::Struct) {}

// BAD
use hir::Function;
use syntax::ast::Struct;

fn frobnicate(func: Function, r#struct: Struct) {}</code></pre>
<p><strong>Rationale:</strong> avoids name clashes, makes the layer clear at a glance.</p>
<p>When implementing traits from <code>std::fmt</code> or <code>std::ops</code>, import the module:</p>
<pre><code class="language-rust">// GOOD
use std::fmt;

impl fmt::Display for RenameError {
    fn fmt(&amp;self, formatter: &amp;mut fmt::Formatter&lt;'_&gt;) -&gt; fmt::Result { .. }
}

// BAD
impl std::fmt::Display for RenameError {
    fn fmt(&amp;self, formatter: &amp;mut std::fmt::Formatter&lt;'_&gt;) -&gt; std::fmt::Result { .. }
}

// BAD
use std::ops::Deref;

impl Deref for Widget {
    type Target = str;
    fn deref(&amp;self) -&gt; &amp;str { .. }
}</code></pre>
<p><strong>Rationale:</strong> overall, less typing.
Makes it clear that a trait is implemented, rather than used.</p>
<p>Avoid local <code>use MyEnum::*</code> imports.
<strong>Rationale:</strong> consistency.</p>
<p>Prefer <code>use crate::foo::bar</code> to <code>use super::bar</code> or <code>use self::bar::baz</code>.
<strong>Rationale:</strong> consistency, this is the style which works in all cases.</p>
<p>By default, avoid re-exports.
<strong>Rationale:</strong> for non-library code, re-exports introduce two ways to use something and allow for inconsistency.</p>
<h3 id="order-of-items"><a class="header" href="#order-of-items">Order of Items</a></h3>
<p>Optimize for the reader who sees the file for the first time, and wants to get a general idea about what is going on.
People read things from top to bottom, so place most important things first.</p>
<p>Specifically, if all items except one are private, always put the non-private item on top.</p>
<pre><code class="language-rust">// GOOD
pub(crate) fn frobnicate() {
    Helper::act()
}

#[derive(Default)]
struct Helper { stuff: i32 }

impl Helper {
    fn act(&amp;self) {

    }
}

// BAD
#[derive(Default)]
struct Helper { stuff: i32 }

pub(crate) fn frobnicate() {
    Helper::act()
}

impl Helper {
    fn act(&amp;self) {

    }
}</code></pre>
<p>If there is a mixture of private and public items, put public items first.</p>
<p>Put <code>struct</code>s and <code>enum</code>s first, functions and impls last. Order type declarations in top-down manner.</p>
<pre><code class="language-rust">// GOOD
struct Parent {
    children: Vec&lt;Child&gt;
}

struct Child;

impl Parent {
}

impl Child {
}

// BAD
struct Child;

impl Child {
}

struct Parent {
    children: Vec&lt;Child&gt;
}

impl Parent {
}</code></pre>
<p><strong>Rationale:</strong> easier to get the sense of the API by visually scanning the file.
If function bodies are folded in the editor, the source code should read as documentation for the public API.</p>
<h3 id="context-parameters"><a class="header" href="#context-parameters">Context Parameters</a></h3>
<p>Some parameters are threaded unchanged through many function calls.
They determine the "context" of the operation.
Pass such parameters first, not last.
If there are several context parameters, consider packing them into a <code>struct Ctx</code> and passing it as <code>&amp;self</code>.</p>
<pre><code class="language-rust">// GOOD
fn dfs(graph: &amp;Graph, v: Vertex) -&gt; usize {
    let mut visited = FxHashSet::default();
    return go(graph, &amp;mut visited, v);

    fn go(graph: &amp;Graph, visited: &amp;mut FxHashSet&lt;Vertex&gt;, v: usize) -&gt; usize {
        ...
    }
}

// BAD
fn dfs(v: Vertex, graph: &amp;Graph) -&gt; usize {
    fn go(v: usize, graph: &amp;Graph, visited: &amp;mut FxHashSet&lt;Vertex&gt;) -&gt; usize {
        ...
    }

    let mut visited = FxHashSet::default();
    go(v, graph, &amp;mut visited)
}</code></pre>
<p><strong>Rationale:</strong> consistency.
Context-first works better when non-context parameter is a lambda.</p>
<h3 id="variable-naming"><a class="header" href="#variable-naming">Variable Naming</a></h3>
<p><a href="https://www.youtube.com/watch?v=-J3wNP6u5YU">https://www.youtube.com/watch?v=-J3wNP6u5YU</a></p>
<p>Use boring and long names for local variables (yay code completion).
The default name is a lowercased name of the type: <code>global_state: GlobalState</code>.
Avoid all acronyms and contractions unless it is overwhelmingly appropriate.
Use American spelling (color, behavior).</p>
<p>Many names in <code>wgsl-analyzer</code> conflict with keywords.
We use <code>r#ident</code> syntax where necessary.</p>
<pre><code class="language-text">crate  -&gt; r#crate
enum   -&gt; r#enum
fn     -&gt; r#fn
impl   -&gt; r#impl
mod    -&gt; r#mod
struct -&gt; r#struct
trait  -&gt; r#trait
type   -&gt; r#type
</code></pre>
<p><strong>Rationale:</strong> idiomatic, clarity.</p>
<h3 id="error-handling-trivia"><a class="header" href="#error-handling-trivia">Error Handling Trivia</a></h3>
<p>Prefer <code>anyhow::Result</code> over <code>Result</code>.</p>
<p><strong>Rationale:</strong> makes it immediately clear what result that is.</p>
<p>Prefer <code>anyhow::format_err!</code> over <code>anyhow::anyhow</code>.</p>
<p><strong>Rationale:</strong> consistent, boring, avoids stuttering.</p>
<p><a href="https://doc.rust-lang.org/stable/std/error/trait.Error.html">Error messages are typically concise lowercase sentences without trailing punctuation.</a></p>
<h3 id="early-returns"><a class="header" href="#early-returns">Early Returns</a></h3>
<p><em>Do</em> use early returns</p>
<pre><code class="language-rust">// GOOD
fn foo() -&gt; Option&lt;Bar&gt; {
    if !condition() {
        return None;
    }

    Some(...)
}

// BAD
fn foo() -&gt; Option&lt;Bar&gt; {
    if condition() {
        Some(...)
    } else {
        None
    }
}</code></pre>
<p><strong>Rationale:</strong> reduce cognitive stack usage.</p>
<p>Use <code>return Err(error)</code> to "throw" an error:</p>
<pre><code class="language-rust">// GOOD
fn foo() -&gt; Result&lt;(), ()&gt; {
    if condition {
        return Err(());
    }
    Ok(())
}

// BAD
fn foo() -&gt; Result&lt;(), ()&gt; {
    if condition {
        Err(())?;
    }
    Ok(())
}</code></pre>
<p><strong>Rationale:</strong> <code>return</code> has type <code>!</code>, which allows the compiler to flag dead
code (<code>Err(...)?</code> is of unconstrained generic type <code>T</code>).</p>
<h3 id="comparisons"><a class="header" href="#comparisons">Comparisons</a></h3>
<p>When doing multiple comparisons use <code>&lt;</code>/<code>&lt;=</code>, avoid <code>&gt;</code>/<code>&gt;=</code>.</p>
<pre><code class="language-rust">// GOOD
assert!(lo &lt;= x &amp;&amp; x &lt;= hi);
assert!(r1 &lt; l2 || r2 &lt; l1);
assert!(x &lt; y);
assert!(0 &lt; x);

// BAD
assert!(x &gt;= lo &amp;&amp; x &lt;= hi);
assert!(r1 &lt; l2 || l1 &gt; r2);
assert!(y &gt; x);
assert!(x &gt; 0);</code></pre>
<p><strong>Rationale:</strong> Less-then comparisons are more intuitive; they correspond spatially to <a href="https://en.wikipedia.org/wiki/Real_line">real line</a>.</p>
<h3 id="if-let"><a class="header" href="#if-let">if-let</a></h3>
<p>Avoid <code>if let ... { } else { }</code> construct; prefer <code>match</code>.</p>
<pre><code class="language-rust">// GOOD
match context.expected_type.as_ref() {
    Some(expected_type) =&gt; completion_ty == expected_type &amp;&amp; !expected_type.is_unit(),
    None =&gt; false,
}

// BAD
if let Some(expected_type) = context.expected_type.as_ref() {
    completion_ty == expected_type &amp;&amp; !expected_type.is_unit()
} else {
    false
}</code></pre>
<p><strong>Rationale:</strong> <code>match</code> is almost always more compact.
The <code>else</code> branch can get a more precise pattern: <code>None</code> or <code>Err(_)</code> instead of <code>_</code>.</p>
<h3 id="match-ergonomics"><a class="header" href="#match-ergonomics">Match Ergonomics</a></h3>
<p>Do not use the <code>ref</code> keyword.</p>
<p><strong>Rationale:</strong> consistency &amp; simplicity.
<code>ref</code> was required before <a href="https://github.com/rust-lang/rfcs/blob/master/text/2005-match-ergonomics.md">match ergonomics</a>.
Today, it is redundant.
Between <code>ref</code> and mach ergonomics, the latter is more ergonomic in most cases, and is simpler (does not require a keyword).</p>
<h3 id="empty-match-arms"><a class="header" href="#empty-match-arms">Empty Match Arms</a></h3>
<p>Use <code>=&gt; (),</code> when a match arm is intentionally empty:</p>
<pre><code class="language-rust">// GOOD
match result {
    Ok(_) =&gt; (),
    Err(error) =&gt; error!("{}", error),
}

// BAD
match result {
    Ok(_) =&gt; {}
    Err(error) =&gt; error!("{}", error),
}</code></pre>
<p><strong>Rationale:</strong> consistency.</p>
<h3 id="functional-combinators"><a class="header" href="#functional-combinators">Functional Combinators</a></h3>
<p>Use high order monadic combinators like <code>map</code>, <code>then</code> when they are a natural choice; do not bend the code to fit into some combinator.
If writing a chain of combinators creates friction, replace them with control flow constructs: <code>for</code>, <code>if</code>, <code>match</code>.
Mostly avoid <code>bool::then</code> and <code>Option::filter</code>.</p>
<pre><code class="language-rust">// GOOD
if !x.cond() {
    return None;
}
Some(x)

// BAD
Some(x).filter(|it| it.cond())</code></pre>
<p>This rule is more "soft" then others, and boils down mostly to taste.
The guiding principle behind this rule is that code should be dense in computation, and sparse in the number of expressions per line.
The second example contains <em>less</em> computation -- the <code>filter</code> function is an indirection for <code>if</code>, it does not do any useful work by itself.
At the same time, it is more crowded -- it takes more time to visually scan it.</p>
<p><strong>Rationale:</strong> consistency, playing to languages' strengths.
Rust has first-class support for imperative control flow constructs
like <code>for</code> and <code>if</code>, while functions are less first-class due to lack
of universal function type, currying, and non-first-class effects (<code>?</code>, <code>.await</code>).</p>
<h3 id="turbofish"><a class="header" href="#turbofish">Turbofish</a></h3>
<p>Prefer type ascription over the turbofish.
When ascribing types, avoid <code>_</code></p>
<pre><code class="language-rust">// GOOD
let mutable: Vec&lt;T&gt; = old.into_iter().map(|it| builder.make_mut(it)).collect();

// BAD
let mutable: Vec&lt;_&gt; = old.into_iter().map(|it| builder.make_mut(it)).collect();

// BAD
let mutable = old.into_iter().map(|it| builder.make_mut(it)).collect::&lt;Vec&lt;_&gt;&gt;();</code></pre>
<p><strong>Rationale:</strong> consistency, readability.
If compiler struggles to infer the type, the human would as well.
Having the result type specified up-front helps with understanding what the chain of iterator methods is doing.</p>
<h3 id="helper-functions"><a class="header" href="#helper-functions">Helper Functions</a></h3>
<p>Avoid creating single-use helper functions:</p>
<pre><code class="language-rust">// GOOD
let buf = {
    let mut buf = get_empty_buf(&amp;mut arena);
    buf.add_item(item);
    buf
};

// BAD
let buf = prepare_buf(&amp;mut arena, item);

...

fn prepare_buf(arena: &amp;mut Arena, item: Item) -&gt; ItemBuf {
    let mut result = get_empty_buf(&amp;mut arena);
    result.add_item(item);
    result
}</code></pre>
<p>Exception: if you want to make use of <code>return</code> or <code>?</code>.</p>
<p><strong>Rationale:</strong> single-use functions change frequently, adding or removing parameters adds churn.
A block serves just as well to delineate a bit of logic, but has access to all the context.
Re-using originally single-purpose function often leads to bad coupling.</p>
<h3 id="local-helper-functions"><a class="header" href="#local-helper-functions">Local Helper Functions</a></h3>
<p>Put nested helper functions at the end of the enclosing functions
(this requires using return statement).
Do not nest more than one level deep.</p>
<pre><code class="language-rust">// GOOD
fn dfs(graph: &amp;Graph, v: Vertex) -&gt; usize {
    let mut visited = FxHashSet::default();
    return go(graph, &amp;mut visited, v);

    fn go(graph: &amp;Graph, visited: &amp;mut FxHashSet&lt;Vertex&gt;, v: usize) -&gt; usize {
        ...
    }
}

// BAD
fn dfs(graph: &amp;Graph, v: Vertex) -&gt; usize {
    fn go(graph: &amp;Graph, visited: &amp;mut FxHashSet&lt;Vertex&gt;, v: usize) -&gt; usize {
        ...
    }

    let mut visited = FxHashSet::default();
    go(graph, &amp;mut visited, v)
}</code></pre>
<p><strong>Rationale:</strong> consistency, improved top-down readability.</p>
<h3 id="helper-variables"><a class="header" href="#helper-variables">Helper Variables</a></h3>
<p>Introduce helper variables freely, especially for multiline conditions:</p>
<pre><code class="language-rust">// GOOD
let wgslfmt_not_installed =
    captured_stderr.contains("not installed") || captured_stderr.contains("not available");

match output.status.code() {
    Some(1) if !wgslfmt_not_installed =&gt; Ok(None),
    _ =&gt; Err(format_err!("wgslfmt failed:\n{}", captured_stderr)),
};

// BAD
match output.status.code() {
    Some(1)
        if !captured_stderr.contains("not installed")
           &amp;&amp; !captured_stderr.contains("not available") =&gt; Ok(None),
    _ =&gt; Err(format_err!("wgslfmt failed:\n{}", captured_stderr)),
};</code></pre>
<p><strong>Rationale:</strong> Like blocks, single-use variables are a cognitively cheap abstraction, as they have access to all the context.
Extra variables help during debugging, they make it easy to print/view important intermediate results.
Giving a name to a condition inside an <code>if</code> expression often improves clarity and leads to nicely formatted code.</p>
<h3 id="token-names"><a class="header" href="#token-names">Token names</a></h3>
<p>Use <code>T![foo]</code> instead of <code>SyntaxKind::FOO_KW</code>.</p>
<pre><code class="language-rust">// GOOD
match p.current() {
    T![true] | T![false] =&gt; true,
    _ =&gt; false,
}

// BAD
match p.current() {
    SyntaxKind::TRUE_KW | SyntaxKind::FALSE_KW =&gt; true,
    _ =&gt; false,
}</code></pre>
<p><strong>Rationale:</strong> The macro uses the familiar Rust syntax, avoiding ambiguities like "is this a brace or bracket?".</p>
<h3 id="documentation"><a class="header" href="#documentation">Documentation</a></h3>
<p>Style inline code comments as proper sentences.
Start with a capital letter, end with a dot.</p>
<pre><code class="language-rust">// GOOD

// Only simple single segment paths are allowed.
MergeBehavior::Last =&gt; {
    tree.use_tree_list().is_none() &amp;&amp; tree.path().map(path_len) &lt;= Some(1)
}

// BAD

// only simple single segment paths are allowed
MergeBehavior::Last =&gt; {
    tree.use_tree_list().is_none() &amp;&amp; tree.path().map(path_len) &lt;= Some(1)
}</code></pre>
<p><strong>Rationale:</strong> writing a sentence (or maybe even a paragraph) rather just "a comment" creates a more appropriate frame of mind.
It tricks you into writing down more of the context you keep in your head while coding.</p>
<p>For <code>.md</code> files, prefer a sentence-per-line format, do not wrap lines.
If the line is too long, you might want to split the sentence in two.</p>
<p><strong>Rationale:</strong> much easier to edit the text and read the diff, see <a href="https://asciidoctor.org/docs/asciidoc-recommended-practices/#one-sentence-per-line">this link</a>.</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="syntax-in-wgsl-analyzer"><a class="header" href="#syntax-in-wgsl-analyzer">Syntax in wgsl-analyzer</a></h1>
<h2 id="about-the-guide-1"><a class="header" href="#about-the-guide-1">About the guide</a></h2>
<p>This guide describes the current state of syntax trees and parsing in wgsl-analyzer as of 2020-01-09 (<a href="https://github.com/rust-lang/wgsl-analyzer/tree/cf5bdf464cad7ceb9a67e07985a3f4d3799ec0b6">link to commit</a>).</p>
<h2 id="source-code"><a class="header" href="#source-code">Source Code</a></h2>
<p>The things described are implemented in three places:</p>
<ul>
<li><a href="https://github.com/rust-analyzer/rowan/tree/v0.15.10">rowan</a> -- a generic library for rowan syntax trees.</li>
<li><a href="https://github.com/rust-lang/rust-analyzer/tree/36a70b7435c48837018c71576d7bb4e8f763f501/crates/syntax">syntax</a> crate inside rust-analyzer which wraps <code>rowan</code> into rust-analyzer specific API. Nothing in rust-analyzer except this crate knows about <code>rowan</code>.</li>
<li><a href="https://github.com/rust-lang/rust-analyzer/tree/36a70b7435c48837018c71576d7bb4e8f763f501/crates/parser">parser</a> crate parses input tokens into a <code>syntax</code> tree.</li>
</ul>
<h2 id="design-goals"><a class="header" href="#design-goals">Design Goals</a></h2>
<ul>
<li>Syntax trees are lossless, or full fidelity. All comments and whitespace get preserved.</li>
<li>Syntax trees are semantic-less. They describe <em>strictly</em> the structure of a sequence of characters, they do not have hygiene, name resolution, or type information attached.</li>
<li>Syntax trees are simple value types. It is possible to create trees for a syntax without any external context.</li>
<li>Syntax trees have intuitive traversal API (parent, children, siblings, etc).</li>
<li>Parsing is lossless (even if the input is invalid, the tree produced by the parser represents it exactly).</li>
<li>Parsing is resilient (even if the input is invalid, the parser tries to see as many syntax tree fragments in the input as it can).</li>
<li>Performance is important, it is OK to use <code>unsafe</code> if it means better memory/cpu usage.</li>
<li>Keep the parser and the syntax tree isolated from each other, such that they can vary independently.</li>
</ul>
<h2 id="trees"><a class="header" href="#trees">Trees</a></h2>
<h3 id="overview"><a class="header" href="#overview">Overview</a></h3>
<p>The syntax tree consists of three layers:</p>
<ul>
<li>GreenNodes</li>
<li>SyntaxNodes (aka RedNode)</li>
<li>AST</li>
</ul>
<p>Of these, only GreenNodes store the actual data, the other two layers are (non-trivial) views into the green tree.
Red-green terminology comes from <a href="https://ericlippert.com/2012/06/08/red-green-trees">Roslyn</a> and gives the name to the <code>rowan</code> library.
Green and syntax nodes are defined in rowan, ast is defined in wgsl-analyzer.</p>
<p>Syntax trees are a semi-transient data structure.
In general, the frontend does not keep syntax trees for all files in memory.
Instead, it <em>lowers</em> syntax trees to a more compact and rigid representation, which is not full-fidelity, but which can be mapped back to a syntax tree if so desired.</p>
<h3 id="greennode"><a class="header" href="#greennode">GreenNode</a></h3>
<p>GreenNode is a purely-functional tree with arbitrary arity. Conceptually, it is equivalent to the following run-of-the-mill struct:</p>
<pre><code class="language-rust">#[derive(PartialEq, Eq, Clone, Copy)]
struct SyntaxKind(u16);

#[derive(PartialEq, Eq, Clone)]
struct Node {
    kind: SyntaxKind,
    text_len: usize,
    children: Vec&lt;Arc&lt;Either&lt;Node, Token&gt;&gt;&gt;,
}

#[derive(PartialEq, Eq, Clone)]
struct Token {
    kind: SyntaxKind,
    text: String,
}</code></pre>
<p>All the differences between the above sketch and the real implementation are strictly due to optimizations.</p>
<p>Points of note:</p>
<ul>
<li>The tree is untyped. Each node has a "type tag", <code>SyntaxKind</code>.</li>
<li>Interior and leaf nodes are distinguished on the type level.</li>
<li>Trivia and non-trivia tokens are not distinguished on the type level.</li>
<li>Each token carries its full text.</li>
<li>The original text can be recovered by concatenating the texts of all tokens in order.</li>
<li>Accessing a child of a particular type (for example, the parameter list of a function) generally involves linearly traversing the children, looking for a specific <code>kind</code>.</li>
<li>Modifying the tree is roughly <code>O(depth)</code>.
We do not make special efforts to guarantee that the depth is not linear, but, in practice, syntax trees are branchy and shallow.</li>
<li>If a mandatory (grammar-wise) node is missing from the input, it is just missing from the tree.</li>
<li>If extra erroneous input is present, it is wrapped into a node with <code>ERROR</code> kind and treated just like any other node.</li>
<li>Parser errors are not a part of the syntax tree.</li>
</ul>
<p>An input like <code>fn foo() -&gt; i32 { return 90 + 2; }</code> might be parsed as:</p>
<pre><code class="language-text">Function@0..34
  Fn@0..2 "fn"
  Blankspace@2..3 " "
  Name@3..6
    Identifier@3..6 "foo"
  ParameterList@6..9
    ParenthesisLeft@6..7 "("
    ParenthesisRight@7..8 ")"
    Blankspace@8..9 " "
  ReturnType@9..16
    Arrow@9..11 "-&gt;"
    Blankspace@11..12 " "
    Int32@12..16
      Int32@12..15 "i32"
      Blankspace@15..16 " "
  CompoundStatement@16..34
    BraceLeft@16..17 "{"
    Blankspace@17..18 " "
    ReturnStatement@18..31
      Return@18..24 "return"
      Blankspace@24..25 " "
      InfixExpression@25..31
        Literal@25..28
          DecimalIntLiteral@25..27 "90"
          Blankspace@27..28 " "
        Plus@28..29 "+"
        Blankspace@29..30 " "
        Literal@30..31
          DecimalIntLiteral@30..31 "2"
    Semicolon@31..32 ";"
    Blankspace@32..33 " "
    BraceRight@33..34 "}"
</code></pre>
<h4 id="optimizations"><a class="header" href="#optimizations">Optimizations</a></h4>
<p>A significant amount of implementation work here was done by <a href="https://github.com/cad97">CAD97</a>.</p>
<p>To reduce the number of allocations, the GreenNode is a <a href="https://doc.rust-lang.org/reference/dynamically-sized-types.html">DST</a>, which uses a single allocation for the header and children.
Thus, it is only usable behind a pointer.</p>
<pre><code class="language-text">*-----------+------+----------+------------+--------+--------+-----+--------*
| ref_count | kind | text_len | n_children | child1 | child2 | ... | childn |
*-----------+------+----------+------------+--------+--------+-----+--------*
</code></pre>
<p>To more compactly store the children, we box <em>both</em> interior nodes and tokens, and represent <code>Either&lt;Arc&lt;Node&gt;, Arc&lt;Token&gt;&gt;</code> as a single pointer with a tag in the last bit.</p>
<p>To avoid allocating EVERY SINGLE TOKEN on the heap, syntax trees use interning.
Because the tree is fully immutable, it is valid to structurally share subtrees.
For example, in <code>1 + 1</code>, there will be a <em>single</em> token for <code>1</code> with ref count 2; the same goes for the whitespace token.
Interior nodes are shared as well (for example, in <code>(1 + 1) * (1 + 1)</code>).</p>
<p>Note that the result of the interning is an <code>Arc&lt;Node&gt;</code>.
That is, it is not an index into the interning table, so you do not have to have the table around to do anything with the tree.
Each tree is fully self-contained (although different trees might share parts).
Currently, the interner is created per-file, but it will be easy to use a per-thread or per-some-context one.</p>
<p>We use a <code>TextSize</code>, a newtyped <code>u32</code>, to store the length of the text.</p>
<p>We currently use <code>SmolStr</code>, a small object optimized string to store text.
This was mostly relevant <em>before</em> we implemented tree interning, to avoid allocating common keywords and identifiers. We should switch to storing text data alongside the interned tokens.</p>
<h4 id="greennode-alternative-designs"><a class="header" href="#greennode-alternative-designs">GreenNode Alternative designs</a></h4>
<h5 id="dealing-with-trivia"><a class="header" href="#dealing-with-trivia">Dealing with trivia</a></h5>
<p>In the above model, whitespace is not treated specially.
Another alternative (used by Swift and Roslyn) is to explicitly divide the set of tokens into trivia and non-trivia tokens, and represent non-trivia tokens as:</p>
<pre><code class="language-rust">struct Token {
  kind: NonTriviaTokenKind,
  text: String,
  leading_trivia: Vec&lt;TriviaToken&gt;,
  trailing_trivia: Vec&lt;TriviaToken&gt;,
}</code></pre>
<p>The tree then contains only non-trivia tokens.</p>
<p>Another approach (from Dart) is to, in addition to a syntax tree, link all the tokens into a bidirectional linked list.
That way, the tree again contains only non-trivia tokens.</p>
<p>Explicit trivia nodes, like in <code>rowan</code>, are used by IntelliJ.</p>
<h5 id="accessing-children"><a class="header" href="#accessing-children">Accessing Children</a></h5>
<p>As noted before, accessing a specific child in the node requires a linear traversal of the children (though we can skip tokens, because the tag is encoded in the pointer itself).
It is possible to recover O(1) access with another representation.
We explicitly store optional and missing (required by the grammar, but not present) nodes.
That is, we use <code>Option&lt;Node&gt;</code> for children.
We also remove trivia tokens from the tree.
This way, each child kind generally occupies a fixed position in a parent, and we can use index access to fetch it.
The cost is that we now need to allocate space for all not-present optional nodes.
So, <code>fn foo() {}</code> will have slots for visibility, unsafeness, attributes, abi, and return type.</p>
<p>IntelliJ uses linear traversal.
Roslyn and Swift do <code>O(1)</code> access.</p>
<h5 id="mutable-trees"><a class="header" href="#mutable-trees">Mutable Trees</a></h5>
<p>IntelliJ uses mutable trees.
Overall, it creates a lot of additional complexity.
However, the API for <em>editing</em> syntax trees is nice.</p>
<p>For example, the assist to move generic bounds to the where clause has this code:</p>
<pre><code class="language-kotlin">for typeBound in typeBounds {
  typeBound.typeParamBounds?.delete()
}
</code></pre>
<p>Modeling this with immutable trees is possible, but annoying.</p>
<h3 id="syntax-nodes"><a class="header" href="#syntax-nodes">Syntax Nodes</a></h3>
<p>A function green tree is not super-convenient to use.
The biggest problem is accessing parents (there are no parent pointers!).
But there are also "identity" issues.
Let us say you want to write code that builds a list of expressions in a file: <code>fn collect_expressions(file: GreenNode) -&gt; HashSet&lt;GreenNode&gt;</code>.
For input like:</p>
<pre><code class="language-rust">fn main() {
  let x = 90i8;
  let x = x + 2;
  let x = 90i64;
  let x = x + 2;
}</code></pre>
<p>both copies of the <code>x + 2</code> expression are represented by equal (and, with interning in mind, actually the same) green nodes.
Green trees just cannot differentiate between the two.</p>
<p><code>SyntaxNode</code> adds parent pointers and identity semantics to green nodes.
They can be called cursors or <a href="https://en.wikipedia.org/wiki/Zipper_(data_structure)">zippers</a> (fun fact: a zipper is a derivative (as in ′) of a data structure).</p>
<p>Conceptually, a <code>SyntaxNode</code> looks like this:</p>
<pre><code class="language-rust">type SyntaxNode = Arc&lt;SyntaxData&gt;;

struct SyntaxData {
  offset: usize,
  parent: Option&lt;SyntaxNode&gt;,
  green: Arc&lt;GreenNode&gt;,
}

impl SyntaxNode {
  fn new_root(root: Arc&lt;GreenNode&gt;) -&gt; SyntaxNode {
    Arc::new(SyntaxData {
      offset: 0,
      parent: None,
      green: root,
    })
  }
  fn parent(&amp;self) -&gt; Option&lt;SyntaxNode&gt; {
    self.parent.clone()
  }
  fn children(&amp;self) -&gt; impl Iterator&lt;Item = SyntaxNode&gt; {
    let mut offset = self.offset;
    self.green.children().map(|green_child| {
      let child_offset = offset;
      offset += green_child.text_len;
      Arc::new(SyntaxData {
        offset: child_offset,
        parent: Some(Arc::clone(self)),
        green: Arc::clone(green_child),
      })
    })
  }
}

impl PartialEq for SyntaxNode {
  fn eq(&amp;self, other: &amp;SyntaxNode) -&gt; bool {
    self.offset == other.offset
      &amp;&amp; Arc::ptr_eq(&amp;self.green, &amp;other.green)
  }
}</code></pre>
<p>Points of note:</p>
<ul>
<li>SyntaxNode remembers its parent node (and, transitively, the path to the root of the tree).</li>
<li>SyntaxNode knows its <em>absolute</em> text offset in the whole file.</li>
<li>Equality is based on identity. Comparing nodes from different trees does not make sense.</li>
</ul>
<h4 id="optimization"><a class="header" href="#optimization">Optimization</a></h4>
<p>The reality is different though.
Traversal of trees is a common operation, and it makes sense to optimize it.
In particular, the above code allocates and does atomic operations during a traversal.</p>
<p>To get rid of atomics, <code>rowan</code> uses non-thread-safe <code>Rc</code>.
This is OK because tree traversals mostly (always, in the case of wgsl-analyzer) run on a single thread.
If you need to send a <code>SyntaxNode</code> to another thread, you can send a pair of <strong>root</strong><code>GreenNode</code> (which is thread-safe) and a <code>Range&lt;usize&gt;</code>.
The other thread can restore the <code>SyntaxNode</code> by traversing from the root green node and looking for a node with the specified range.
You can also use a similar trick to store a <code>SyntaxNode</code>.
That is, a data structure that holds a <code>(GreenNode, Range&lt;usize&gt;)</code> will be <code>Sync</code>.
However, wgsl-analyzer goes even further.
It treats trees as semi-transient and instead of storing a <code>GreenNode</code>, it generally stores just the id of the file from which the tree originated: <code>(FileId, Range&lt;usize&gt;)</code>.
The <code>SyntaxNode</code> is restored by reparsing the file and traversing it from the root.
With this trick, wgsl-analyzer holds only a small number of trees in memory at the same time, which reduces memory usage.</p>
<p>Additionally, only the root <code>SyntaxNode</code> owns an <code>Arc</code> to the (root) <code>GreenNode</code>.
All other <code>SyntaxNode</code>s point to corresponding <code>GreenNode</code>s with a raw pointer.
They also point to the parent (and, consequently, to the root) with an owning <code>Rc</code>, so this is sound.
In other words, one needs <em>one</em> arc bump when initiating a traversal.</p>
<p>To get rid of allocations, <code>rowan</code> takes advantage of <code>SyntaxNode: !Sync</code> and uses a thread-local free list of <code>SyntaxNode</code>s.
In a typical traversal, you only directly hold a few <code>SyntaxNode</code>s at a time (and their ancestors indirectly).
A free list proportional to the depth of the tree removes all allocations in a typical case.</p>
<p>So, while traversal is not exactly incrementing a pointer, it is still pretty cheap: TLS + rc bump!</p>
<p>Traversal also yields (cheap) owned nodes, which improves ergonomics quite a bit.</p>
<h4 id="syntax-nodes-alternative-designs"><a class="header" href="#syntax-nodes-alternative-designs">Syntax Nodes Alternative Designs</a></h4>
<h5 id="memoized-rednodes"><a class="header" href="#memoized-rednodes">Memoized RedNodes</a></h5>
<p>C# and Swift follow the design where the red nodes are memoized, which would look roughly like this in Rust:</p>
<pre><code class="language-rust">type SyntaxNode = Arc&lt;SyntaxData&gt;;

struct SyntaxData {
  offset: usize,
  parent: Option&lt;SyntaxNode&gt;,
  green: Arc&lt;GreenNode&gt;,
  children: Vec&lt;OnceCell&lt;SyntaxNode&gt;&gt;,
}</code></pre>
<p>This allows using true pointer equality for comparison of identities of <code>SyntaxNodes</code>.
wgsl-analyzer used to have this design as well, but we have since switched to cursors.
The main problem with memoizing the red nodes is that it more than doubles the memory requirements for fully realized syntax trees.
In contrast, cursors generally retain only a path to the root.
C# combats increased memory usage by using weak references.</p>
<h3 id="ast"><a class="header" href="#ast">AST</a></h3>
<p><code>GreenTree</code>s are untyped and homogeneous, because it makes accommodating error nodes, arbitrary whitespace, and comments natural, and because it makes it possible to write generic tree traversals.
However, when working with a specific node, like a function definition, one would want a strongly typed API.</p>
<p>This is what is provided by the AST layer. AST nodes are transparent wrappers over untyped syntax nodes:</p>
<pre><code class="language-rust">pub trait AstNode {
  fn cast(syntax: SyntaxNode) -&gt; Option&lt;Self&gt;
  where
    Self: Sized;

  fn syntax(&amp;self) -&gt; &amp;SyntaxNode;
}</code></pre>
<p>Concrete nodes are generated (there are 117 of them), and look roughly like this:</p>
<pre><code class="language-rust">#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub struct FnDef {
  syntax: SyntaxNode,
}

impl AstNode for FnDef {
  fn cast(syntax: SyntaxNode) -&gt; Option&lt;Self&gt; {
    match kind {
      FN =&gt; Some(FnDef { syntax }),
      _ =&gt; None,
    }
  }
  fn syntax(&amp;self) -&gt; &amp;SyntaxNode {
    &amp;self.syntax
  }
}

impl FnDef {
  pub fn param_list(&amp;self) -&gt; Option&lt;ParamList&gt; {
    self.syntax.children().find_map(ParamList::cast)
  }
  pub fn ret_type(&amp;self) -&gt; Option&lt;RetType&gt; {
    self.syntax.children().find_map(RetType::cast)
  }
  pub fn body(&amp;self) -&gt; Option&lt;BlockExpr&gt; {
    self.syntax.children().find_map(BlockExpr::cast)
  }
  // ...
}</code></pre>
<p>Variants like expressions, patterns, or items are modeled with <code>enum</code>s, which also implement <code>AstNode</code>:</p>
<pre><code class="language-rust">#[derive(Debug, Clone, PartialEq, Eq, Hash)]
pub enum AssocItem {
  FnDef(FnDef),
  TypeAliasDef(TypeAliasDef),
  ConstDef(ConstDef),
}

impl AstNode for AssocItem {
  ...
}</code></pre>
<p>Shared AST substructures are modeled via (dynamically compatible) traits:</p>
<pre><code class="language-rust">trait HasVisibility: AstNode {
  fn visibility(&amp;self) -&gt; Option&lt;Visibility&gt;;
}

impl HasVisibility for FnDef {
  fn visibility(&amp;self) -&gt; Option&lt;Visibility&gt; {
    self.syntax.children().find_map(Visibility::cast)
  }
}</code></pre>
<p>Points of note:</p>
<ul>
<li>Like <code>SyntaxNode</code>s, AST nodes are cheap to clone pointer-sized owned values.</li>
<li>All "fields" are optional, to accommodate incomplete and/or erroneous source code.</li>
<li>It is always possible to go from an ast node to an untyped <code>SyntaxNode</code>.</li>
<li>It is possible to go in the opposite direction with a checked cast.</li>
<li><code>enum</code>s allow modeling of arbitrary intersecting subsets of AST types.</li>
<li>Most of wgsl-analyzer works with the ast layer, with notable exceptions of:
<ul>
<li>macro expansion, which needs access to raw tokens and works with <code>SyntaxNode</code>s</li>
<li>some IDE-specific features like syntax highlighting are more conveniently implemented over a homogeneous <code>SyntaxNode</code> tree</li>
</ul>
</li>
</ul>
<h4 id="ast-alternative-designs"><a class="header" href="#ast-alternative-designs">AST Alternative Designs</a></h4>
<h5 id="semantic-full-ast"><a class="header" href="#semantic-full-ast">Semantic Full AST</a></h5>
<p>In IntelliJ, the AST layer (dubbed <strong>P</strong>rogram <strong>S</strong>tructure <strong>I</strong>nterface) can have semantics attached, and is usually backed by either a syntax tree, indices, or metadata from compiled libraries.
The backend for PSI can change dynamically.</p>
<h3 id="syntax-tree-recap"><a class="header" href="#syntax-tree-recap">Syntax Tree Recap</a></h3>
<p>At its core, the syntax tree is a purely functional n-ary tree, which stores text at the leaf nodes and node "kinds" at all nodes.
A cursor layer is added on top, which gives owned, cheap to clone nodes with identity semantics, parent links, and absolute offsets.
An AST layer is added on top, which reifies each node <code>Kind</code> as a separate Rust type with the corresponding API.</p>
<h2 id="parsing"><a class="header" href="#parsing">Parsing</a></h2>
<p>The (green) tree is constructed by a DFS "traversal" of the desired tree structure:</p>
<pre><code class="language-rust">pub struct GreenNodeBuilder { ... }

impl GreenNodeBuilder {
    pub fn new() -&gt; GreenNodeBuilder { ... }

    pub fn token(&amp;mut self, kind: SyntaxKind, text: &amp;str) { ... }

    pub fn start_node(&amp;mut self, kind: SyntaxKind) { ... }
    pub fn finish_node(&amp;mut self) { ... }

    pub fn finish(self) -&gt; GreenNode { ... }
}</code></pre>
<p>The parser, ultimately, needs to invoke the <code>GreenNodeBuilder</code>.
There are two principal sources of inputs for the parser:</p>
<ul>
<li>source text, which contains trivia tokens (whitespace and comments)</li>
<li>token trees from macros, which lack trivia</li>
</ul>
<p>Additionally, input tokens do not correspond 1-to-1 with output tokens.
For example, two consecutive <code>&gt;</code> tokens might be glued, by the parser, into a single <code>&gt;&gt;</code>.</p>
<p>For these reasons, the parser crate defines a callback interfaces for both input tokens and output trees.
The explicit glue layer then bridges various gaps.</p>
<p>The parser interface looks like this:</p>
<pre><code class="language-rust">pub struct Token {
    pub kind: SyntaxKind,
    pub is_joined_to_next: bool,
}

pub trait TokenSource {
    fn current(&amp;self) -&gt; Token;
    fn lookahead_nth(&amp;self, n: usize) -&gt; Token;
    fn is_keyword(&amp;self, kw: &amp;str) -&gt; bool;

    fn bump(&amp;mut self);
}

pub trait TreeSink {
    fn token(&amp;mut self, kind: SyntaxKind, n_tokens: u8);

    fn start_node(&amp;mut self, kind: SyntaxKind);
    fn finish_node(&amp;mut self);

    fn error(&amp;mut self, error: ParseError);
}

pub fn parse(
    token_source: &amp;mut dyn TokenSource,
    tree_sink: &amp;mut dyn TreeSink,
) { ... }</code></pre>
<p>Points of note:</p>
<ul>
<li>The parser and the syntax tree are independent, they live in different crates neither of which depends on the other.</li>
<li>The parser does not know anything about textual contents of the tokens, with an isolated hack for checking contextual keywords.</li>
<li>For gluing tokens, the <code>TreeSink::token</code> might advance further than one atomic token ahead.</li>
</ul>
<h3 id="reporting-syntax-errors"><a class="header" href="#reporting-syntax-errors">Reporting Syntax Errors</a></h3>
<p>Syntax errors are not stored directly in the tree.
The primary motivation for this is that syntax tree is not necessary produced by the parser, it may also be assembled manually from pieces (which happens all the time in refactorings).
Instead, parser reports errors to an error sink, which stores them in a <code>Vec</code>.
If possible, errors are not reported during parsing and are postponed for a separate validation step.
For example, parser accepts visibility modifiers on trait methods, but then a separate tree traversal flags all such visibilities as erroneous.</p>
<h3 id="macros"><a class="header" href="#macros">Macros</a></h3>
<p>The primary difficulty with macros is that individual tokens have identities, which need to be preserved in the syntax tree for hygiene purposes.
This is handled by the <code>TreeSink</code> layer.
Specifically, <code>TreeSink</code> constructs the tree in lockstep with draining the original token stream.
In the process, it records which tokens of the tree correspond to which tokens of the input, by using text ranges to identify syntax tokens.
The end result is that parsing an expanded code yields a syntax tree and a mapping of text-ranges of the tree to original tokens.</p>
<p>To deal with precedence in cases like <code>$expression * 1</code>, we use special invisible parenthesis, which are explicitly handled by the parser.</p>
<h3 id="whitespace--comments"><a class="header" href="#whitespace--comments">Whitespace &amp; Comments</a></h3>
<p>Parser does not see whitespace nodes.
Instead, they are attached to the tree in the <code>TreeSink</code> layer.</p>
<p>For example, in</p>
<pre><code class="language-rust">// non doc comment
fn foo() {}</code></pre>
<p>the comment will be (heuristically) made a child of function node.</p>
<h3 id="incremental-reparse"><a class="header" href="#incremental-reparse">Incremental Reparse</a></h3>
<p>Green trees are cheap to modify, so incremental reparse works by patching a previous tree, without maintaining any additional state.
The reparse is based on heuristic: we try to contain a change to a single <code>{}</code> block, and reparse only this block.
To do this, we maintain the invariant that, even for invalid code, curly braces are always paired correctly.</p>
<p>In practice, incremental reparsing does not actually matter much for IDE use-cases, parsing from scratch seems to be fast enough.</p>
<h3 id="parsing-algorithm"><a class="header" href="#parsing-algorithm">Parsing Algorithm</a></h3>
<p>We use a boring hand-crafted recursive descent + pratt combination, with a special effort of continuing the parsing if an error is detected.</p>
<h3 id="parser-recap"><a class="header" href="#parser-recap">Parser Recap</a></h3>
<p>Parser itself defines traits for token sequence input and syntax tree output.
It does not care about where the tokens come from, and how the resulting syntax tree looks like.</p>

                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>



        <script>
            window.playground_line_numbers = true;
        </script>

        <script>
            window.playground_copyable = true;
        </script>

        <script src="ace.js"></script>
        <script src="mode-rust.js"></script>
        <script src="editor.js"></script>
        <script src="theme-dawn.js"></script>
        <script src="theme-tomorrow_night.js"></script>

        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            MathJax.Hub.Register.StartupHook('End', function() {
                window.setTimeout(window.print, 100);
            });
        });
        </script>


    </div>
    </body>
</html>
